content,title
"Schwarze Löcher sind schwierig zu entdecken – denn keinerlei Licht kann von ihnen ausgehen und sie sind extrem kompakt. Nun gelang es einer Gruppe von Astronomen, ein Schwarzes Loch relativ nahe zur Erde zu entdecken. Mit gerade einmal 1560 Lichtjahren Entfernung ist es das nächstgelegene bekannte Schwarze Loch überhaupt. Es ist außerdem ein „stilles“ Schwarzes Loch, verrät sich also nicht durch heiße Materie in seiner Umgebung, sondern nur durch den Einfluss seiner Schwerkraft auf seinen Begleitstern, wie die Forscher in den Monthly Notices of the Royal Astronomical Society berichten.
Schwarze Löcher senden keinerlei Strahlung aus – sind also absolut dunkel – und vereinigen ihre gesamte Masse in einem kleinen Volumen. Neben den prominentesten Vertretern, den supermassereichen Schwarzen Löchern, wie etwa dem im Zentrum der Milchstraße, gibt es unter anderem auch stellare Schwarze Löcher. Solche Objekte gehen aus Supernovae, also Sternexplosionen, schwerer Sterne hervor und haben Durchmesser von nur wenigen Kilometern. Einige von ihnen sind sogenannte stille Schwarze Löcher. Sie haben im Gegensatz zu aktiven Schwarzen Löchern keine umgebende Akkretionsscheibe. Deshalb verraten sie sich nicht durch Röntgenstrahlung, die von der heißen Materie einer solchen rotierenden Gas- und Staubscheibe ausgeht. Stattdessen lassen sie sich nur dadurch nachweisen, dass ein Begleitstern sie umkreist. Dessen Bewegung muss allerdings genau bestimmt werden, um Rückschlüsse über den unsichtbaren Teil des Systems – das mögliche Schwarze Loch – zu ziehen.
Nun fanden Forscher unter der Leitung von Kareem El-Badry vom Max-Planck-Institut für Astronomie und vom Harvard-Smithsonian Center for Astrophysics in den Daten des Weltraumteleskops Gaia Hinweise auf insgesamt sechs mögliche stille Schwarze Löcher. Aus den Daten mussten El-Badry und seine Kollegen mehrere Kandidaten wieder aussortieren, weil die Datenlage keine klare Aussage zuließ. Ein System jedoch erwies sich als besonders erfolgversprechend. Dieses untersuchten sie zusätzlich mit den Magellan-Teleskopen, dem Gemini-Observatorium, dem Keck-I-Teleskop und mit dem MPG/ESO-2,2-m-Teleskop am La Silla-Observatorium der Europäischen Südsternwarte. So konnten sie aus den Spektren der ausgesandten Strahlung und der Bewegung des leuchtenden Sterns, der sich um das Schwarze Loch bewegt, wichtige Schlüsse ziehen.
Wie die Forscher zeigten, ist der Stern etwas kleiner als unsere Sonne und umkreist ein unsichtbares Objekt einmal in 185 Tagen. Die Bahn, auf der er sich bewegt, hat im Schnitt einen Durchmesser, der etwa der Entfernung der Erde zur Sonne entspricht. Das dunkle Objekt inmitten dieser Bahn hat hingegen knapp zehn Sonnenmassen. Laut den Forschern ist das einzig mögliche Szenario, das diesen Orbit erklärt, dass es sich bei dem Objekt um ein Schwarzes Loch handelt.
Allerdings wirft der Fund auch Fragen auf. Ein so schweres Schwarzes Loch sollte aus einem Stern mit rund zwanzig Sonnenmassen hervorgegangen sein. Dieser hätte sich nach einer sehr kurzen Lebensspanne in einen Überriesen verwandelt, der weit über die Bahn seines Begleitsterns hinausgereicht hätte. Es ist unklar, wie der kleinere Begleitstern dies hätte überstehen können, ohne irgendwelche Besonderheiten zu zeigen. Die Astronomen spekulieren deshalb, das entdeckte Objekt könne sogar selbst aus zwei Schwarzen Löchern bestehen, die sich eng umkreisen. Diese beiden wären dann aus zwei nicht ganz so schweren Sternen hervorgegangen, so dass kein so großer Überriese entstanden wäre. Ein anderes Szenario besagt, dass ursprünglich die zwei Sterne in einem Sternhaufen entstanden und viel weiter voneinander entfernt gewesen sein könnten. Erst nach der Supernova, bei der sich das stellare Schwarze Loch aus dem größeren Stern gebildet hat, hätten sie sich einander aufgrund von Schwerkrafteinflüssen durch andere Sterne im Sternhaufen angenähert, so die Erklärung.
In den Daten von Gaia lassen sich allerdings noch viele weitere Schwarze Löcher finden. Deren Analyse wird sicherlich viel zum Verständnis exotischer Systeme beitragen. Und vermutlich wird sich auch ein noch näher an der Erde gelegenes Schwarzes Loch finden.",Nächstgelegenes Schwarzes Loch entdeckt
"Solarzellen auf Basis von Perowskiten bieten eine vielversprechende Ergänzung zu etablierten Siliziumzellen. Besonders doppelte Perowskitschichten bieten sich für möglichst hohe Wirkungsgrade an, da sie jeweils einen anderen Bereich des Sonnenlichtspektrums in elektrischen Strom umwandeln. Allerdings leiden solche Solarzellen bisher an mangelnder Stabilität. Doch dieses Problem haben Forscher nun ansatzweise gelöst: Wie sie in der Fachzeitschrift „Science"" berichten, ließ sich durch die Wahl eines speziellen Lösungsmittels beim Herstellen der Solarzellen die Haltbarkeit deutlich verbessern.
Bei der Produktion von Perowskitsolarzellen werden die Perowskitkristalle in meist organischen Flüssigkeiten gelöst. Diese Flüssigkeiten lassen sich dann mit verschiedenen Verfahren in hauchdünnen Filmen auf einer Oberfläche verteilen. Trocknen diese feuchten Filme, bleibt die gewünschte kristalline Perowskitschicht übrig. So lässt sich eine extrem dünne, zweidimensionale Perowskitschicht auf dickere, dreidimensionale Perowskitkristalle aufbringen. „Doch wenn man eine zweidimensionale Schicht auf eine dreidimensionale aufbringt, zerstört das Lösungsmittel zwangsläufig die untere Schicht"", sagt Aditya Mohite von der Rice University in Houston. Genau diesen Effekt, der die Haltbarkeit der Solarzellen erheblich verringert, konnte der Forscher gemeinsam mit Kollegen von der französischen Universität Rennes über die richtige Wahl des Lösungsmittels deutlich reduzieren.












                Experimentaufbau
            



Der Schlüssel zum Erfolg lag im Austarieren von zwei Eigenschaften der Lösungsmittel. Die Forscher nutzten eine Flüssigkeit, bei der die Fähigkeit, Elektronen abzugeben, und die dielektrische Konstante – ein Maß für die elektrische Durchlässigkeit – im Gleichgewicht standen. Damit ließ sich die obere, hauchdünne Perowskitschicht als dünner Flüssigkeitsfilm auftragen, ohne dass die untere, etwas dickere Perowskitschicht beschädigt wurde. Das Ergebnis war eine Perowskitsolarzelle mit 24,5 Prozent Wirkungsgrad. Selbst nach zweitausend Stunden büßte diese Solarzelle weniger als ein Prozent ihrer Effizienz ein. Die obere Schicht wandelte dabei Sonnenlicht im sichtbaren Bereich, die untere im nahen Infrarotbereich in elektrischen Strom um.
„Das ist ein sehr gutes Ergebnis"", sagt Solarzellenexperte Kaining Ding vom Forschungszentrum Jülich, der nicht an dieser Studie beteiligt war. Dennoch sei der Weg zu einer höheren Stabilität der Solarmodule noch lang. Denn 2000 Stunden sind im Vergleich zu 100 000 Stunden, die heutige Siliziumsolarzellen bei einem Betrieb von 25 Jahren durchhalten, noch sehr wenig. Doch zeigt die Studie, dass sich mit weiterer Forschung die Haltbarkeit von Perowskitsolarzellen noch deutlich verbessern lässt. „Das aktuelle Ergebnis ist auf jeden Fall ein weiterer Schritt in die richtige Richtung"", sagt Ding.",Stabilere Perowskitsolarzellen
"Auf der Erde finden sich große Wassermengen nicht nur an der Oberfläche und in den Ozeanen. Auch im Erdmantel, in hunderten Kilometern Tiefe, vermuten Geologen insgesamt so viel Wasser wie in allen Ozeanen zusammen. Dies nachzuweisen, ist jedoch schwierig. Helfen können hier Diamanten, die aus diesen Tiefen im Laufe der Erdgeschichte in oberflächennahe Minen gelangten und den Geologen ein einzigartiges Fenster in die Tiefe der Erde öffnen. So berichtet eine Forschergruppe nun in der Fachzeitschrift „Nature Geoscience“ von einer speziellen Mineralmischung in einem Diamanten, die neue Erkenntnisse über den Wasserhaushalt im Erdmantel liefert.
Der nur 1,5 Karat, also 0,3 Gramm, schwere Diamant, den die Forscher um Fabrizio Nestola von der Universität Padua in Italien untersuchten, stammt aus der Karowe Mine in Botsuana. Vulkanische Prozesse, wenn etwa flüssiges Magma durch tiefreichende Schlote aufsteigt, hatten den Diamanten über Jahrmillionen von der Grenze zwischen dem oberen und unteren Erdmantel bis zur Erdkruste transportiert. Mit mehreren physikalischen Methoden – wie etwa der Ramanspektroskopie und der Streuung von Röntgenstrahlen – analysierten die Forscher, woraus sich die Einschlüsse im Diamanten zusammensetzen. Dabei entdeckten sie eine vielfältige Mischung besonderer Minerale in dessen Innern: Magnesiumsilikat, Bridgmanit oder auch das seltene Ringwoodit, ein Magnesiumsilikat mit zusätzlichen Eisenanteilen.












                Diamant mit eingeschlossenen Mineralen
            



Anhand dieser Ergebnisse schlossen Nestola und seine Kollegen auf die geologischen Bedingungen an dem Entstehungsort des Diamanten zurück: Denn eine solche Kombination entsteht meist in einer wasserreichen Umgebung. So liefert diese Analyse einen überzeugenden Hinweis, dass in der Mantelübergangszone zwischen dem oberen und unteren Erdmantel – 660 Kilometer unter der Erdoberfläche – tatsächlich große Wasservorkommen vorliegen sollten.
Bereits vor vier Jahren hatten Geologen von der University of Nevada in Las Vegas ebenfalls Hinweise auf Wasser im Erdmantel gefunden. Auch sie hatten die Einschlüsse von Diamanten mit Röntgenstrahlen untersucht und dabei eine spezielle Eisvariante – sogenanntes Eis-VII – mit einer unter hohem Druck stabilen kubischen Kristallstruktur gefunden. Dieses Wasser war vermutlich durch das Abtauchen der ozeanischen unter die kontinentale Erdkruste in den Erdmantel und schließlich in die Diamanten gelangt.
Mit der neuen Studie mehren sich also die Hinweise auf weit verbreitete Wasservorkommen im Erdmantel. Belegte der frühere Nachweis von Eis-VII zumindest an einer Stelle Wasservorkommen, legt die nun identifizierte Mineralienmischung nahe, dass Wasser über weiter ausgedehnte Zonen im Erdmantel vorkommt. Mit diesem Wissen können Geologen nun das Fließverhalten heißer Gesteine und die Entstehung von Magma besser verstehen. Damit lassen sich geologische Prozesse in der Erde, vor allem der Vulkanismus, genauer beschreiben als bisher.",Wasservorkommen im Erdmantel
"Alle uns bekannte Materie im Universum setzt sich aus winzigen Bausteinen – den Elementarteilchen – zusammen. Ihre Eigenschaften lassen sich mit Teilchenbeschleunigern wie dem Large Hadron Collider am Forschungszentrum CERN im Detail untersuchen. Dort haben Forscher mit den Detektoren des ALICE-Experiments nun ein Phänomen beobachtet, das bislang nur aus theoretischen Berechnungen bekannt war. Welche neuen Erkenntnisse diese Entdeckung bringt und was sie für die Teilchenphysik bedeutet, berichtet Christian Klein-Bösing von der Universität Münster im Interview mit Welt der Physik. 
Welt der Physik: Was wird mit den Experimenten am Forschungszentrum CERN untersucht? 












                Christian Klein-Bösing
            



Christian Klein-Bösing: Wir erforschen die fundamentalen Bausteine, aus denen sich die gesamte uns bekannte Materie zusammensetzt: Atome bestehen aus einer Atomhülle mit Elektronen und einem Atomkern, der Protonen und Neutronen enthält. Während Elektronen bereits elementare Teilchen sind, setzen sich die Protonen und Neutronen wiederum aus noch kleineren Teilchen, den sogenannten Quarks und Gluonen, zusammen. Da Quarks und Gluonen jedoch fest in den Protonen und Neutronen eingeschlossen sind, lassen sich ihre Eigenschaften nur schwer untersuchen.
Wie erforschen Sie die Quarks und Gluonen dann? 
Mit dem Large Hadron Collider – kurz LHC – beschleunigen wir Protonen auf sehr hohe Energien und lassen sie aufeinanderprallen. Bei diesen Kollisionen brechen die Protonen in ihre Bestandteile – die Quarks und Gluonen – auf. Mit großen Detektoren wie denen des ALICE-Experiments können wir dann die Eigenschaften dieser winzigen Teilchen und die sogenannte starke Kraft, die zwischen ihnen wirkt, genauer untersuchen.
Und wie machen Sie das? 
Tatsächlich bewegen sich die Quarks und Gluonen im ALICE-Experiment nur für Sekundenbruchteile frei umher. Unmittelbar nach der Kollision beginnen sie, ihre Energie abzustrahlen – und zwar in Form von neuen Gluonen, die aus der überschüssigen Energie entstehen. Auch diese Gluonen können ihre Energie in Form weiterer Gluonen abstrahlen. Außerdem können sie in Quarks zerfallen. So entsteht quasi direkt nach der Kollision im Teilchenbeschleuniger eine Kaskade aus immer energieärmeren Quarks und Gluonen, die sich daraufhin wieder zu gebundenen Teilchen zusammenschließen. Diese gebundenen Teilchen können wir schließlich mit den Detektoren des ALICE-Experiments nachweisen. Die große Herausforderung ist es dann, aus den zahlreichen Spuren im Detektor die Eigenschaften der ursprünglichen Quarks und Gluonen zu rekonstruieren. Daran arbeitet meine Forschungsgruppe.
Wie gelingt Ihnen das? 
Wir schauen uns sogenannte Jets an. Das sind kegelförmige Bereiche in der Flugrichtung der ursprünglichen Quarks und Gluonen. In diesen Bereichen entstehen nach der Kollision die meisten Teilchen. Nun können wir versuchen, die Struktur dieser Jets zu entschlüsseln: Wenn etwa zwei Teilchen sehr nah beieinander liegen, stammen sie wahrscheinlich vom selben Quark oder Gluon in der Kaskade ab. Außerdem können wir die Impulse der detektierten Teilchen in einem Bereich addieren und erhalten den Impuls des ursprünglichen Teilchens. So gehen wir die Kaskade quasi rückwärts entlang, bis wir bei den ursprünglichen Quarks und Gluonen angelangt sind.
Auf diese Art und Weise haben Forscher nun eine Eigenschaft der Quarks – ihre Masse – nachgewiesen. Wie macht sich die Masse der Quarks denn bemerkbar? 
Aus dem Standardmodell der Teilchenphysik – einer Theorie, die alle uns bekannten Elementarteilchen und Kräfte zwischen ihnen beschreibt – wissen wir, dass es sechs verschiedene Quarks gibt, die alle eine spezifische Masse haben. Diese Massen hat man bislang allerdings nur indirekt gemessen, indem man das Verhalten von gebundenen Teilchen wie etwa Protonen beobachtet hat. Doch die ALICE-Kollaboration hat nun erstmals eine direkte Auswirkung der Masse von freien Quarks nachgewiesen.












                Kaskade aus Quarks und Gluonen
            



Was genau haben die Forscher am ALICE-Experiment denn beobachtet? 
Die Masse eines Teilchens wirkt sich auf die Verteilung der Spuren aus, die die Detektoren nach der Kollision aufzeichnen: So sagen theoretische Berechnungen vorher, dass es für schwere Teilchen sehr unwahrscheinlich ist, in einem bestimmten kegelförmigen Bereich um ihre Flugrichtung herum Energie in Form von neuen Teilchen abzustrahlen. So einen Bereich, der frei von Teilchen blieb, ließ sich jetzt am ALICE-Experiment für eine bestimmte Art von Quarks – die sogenannten Charm-Quarks – beobachten.
Lässt sich daraus dann auch die Masse der Charm-Quarks berechnen? 
Die Beobachtungen decken sich ziemlich genau mit dem, was man anhand der indirekt bestimmten Masse der Quarks erwarten würde. Doch um die Masse aus den Kollisionen direkt zu erhalten, bräuchte man noch deutlich genauere Messungen. Ein nächster Schritt wäre erstmal, den Effekt auch für eine andere Art von Quarks – die sogenannten Beauty-Quarks – zu beobachten. Mit dem kürzlichen Update des LHC und seinen Detektoren könnte das nun gelingen. Denn zum einen wurde die Rate, mit der die Teilchenkollisionen aufgezeichnet werden, drastisch erhöht. Zum anderen wurden die Detektoren komplett erneuert, sodass man die erzeugten Teilchen viel genauer nachweisen und zurückverfolgen kann.",„Der erste direkte Nachweis“
"Von ihnen gibt es zahlreiche Vertreter im All: Doppelsysteme aus zwei nahezu identischen Sternen. Solche Sternenzwillinge sollten, so die Theorie, aus einer gemeinsamen Gaswolke entstanden sein. Doch viele von ihnen sind wesentlich weiter voneinander entfernt als die Gaswolken, in denen sie entstanden sein müssten, groß sind. Diesen Widerspruch scheint ein Forscherteam jetzt aufgelöst zu haben. Ihre Erkenntnis: Die beiden Sterne bewegen sich zumeist auf extrem langgestreckten Umlaufbahnen, müssen sich früher enger beieinander befunden und dann im Lauf der Zeit voneinander entfernt haben, so die Wissenschaftler im Fachblatt „Astrophysical Journal Letters“.
Zwillingspaare von Sternen zeichnen sich dadurch aus, dass die beiden Sterne in Masse und Größe übereinstimmen. Häufig sind sie über hundert Milliarden Kilometer voneinander entfernt. Die Orte, an denen Sterne entstehen – protostellare Scheiben –, haben hingegen Durchmesser von nur etwa 15 Milliarden Kilometer. „Sie sind also viel kleiner als die typischen Abstände solcher Zwillingspaare“, erläutern Hsiang-Chih Hwang vom Institute for Advanced Study in Princeton und seine Kollegen. Das Rätsel, wie die Zwillingssterne entstanden sind, obwohl sie solch immense Distanzen voneinander trennen, wollten die Forscher nun lüften. Vielleicht, so ihre Vermutung, sind die Sterne dicht beieinander entstanden und erst später durch Störungen ihrer Bahnen weiter auseinander gewandert. Solche Störungen führen aber stets zu stark elliptischen Umlaufbahnen.
Diese Vermutung ist jedoch nicht so einfach zu überprüfen. Denn bei solchen weiten Systemen dauert es mitunter viele hundert oder gar tausend Jahre, bis die Sterne sich einmal gegenseitig umrundet haben. Hwang und seine Kollegen griffen daher auf einen umfangreichen Datensatz zurück, der aus Beobachtungen des europäischen Weltraumsatelliten Gaia stammt. Die Daten enthalten fast eine Million Sternenzwillinge mit Abständen von 60 bis 150 Milliarden Kilometer, das entspricht dem 400- bis 1000-Fachen des Abstands der Erde zur Sonne.
Aus diesen Daten ermittelten Hwang und seine Kollegen einerseits die Verbindungslinien zwischen beiden Sternen und andererseits die Bewegungsrichtungen der beiden Sterne relativ zueinander. Bei nahezu kreisförmigen Bahnen sollten diese etwa einen rechten Winkel bilden. Stattdessen stellte sich heraus, dass in den meisten Fällen die Verbindungslinien und die Bewegungsrichtungen nahezu parallel waren – typisch für langgestreckte Ellipsenbahnen.
Ihre stark elliptische Form deute darauf hin, „dass die Bahnen durch dynamische Prozesse während der Entstehungsphase der Sterne stark gestört worden sind“, so Hwang und seine Kollegen. Beide Sterne aus solchen Zwillingssternsystemen scheinen also in einer einzigen Gaswolke entstanden zu sein. Die Frage, welcher Prozess die Bahnen gestört hat, bleibt jedoch bestehen. Infrage kommen sowohl Wechselwirkungen der jungen Sterne mit der rotierenden Gasscheibe, aus der sie entstanden sind, als auch weitere Sterne, die eng an den jungen Sternen vorbeigezogen sein könnten.
Wie die Forscher betonen, ist die Antwort auf diese Frage auch für Einzelsterne von Interesse: Die Störungen könnten nämlich mitunter so groß sein, dass sich die Sternenpaare auflösen – in zwei Einzelsterne, die in entgegengesetzten Richtungen davonfliegen. Weit entfernte Sternenzwillinge können uns also offenbaren, wie sich ganze Gruppen von Sternen entwickeln, so die Astrophysiker.",Wie Sternenzwillinge entstehen
"Vor etwa 100 bis 200 Millionen Jahren zerbrach ein Mond des Saturn und aus seinen Überresten bildeten sich die Saturnringe. Auch die große Neigung der Rotationsachse des Planeten lässt sich mit dem Prozess erklären. Zu diesem Ergebnis kamen Forscher nun mithilfe eines Modells von Saturn und seinen Monden, das sie anhand von Beobachtungsdaten der Raumsonde Cassini sowie mit Computersimulationen erstellten. Um das neue Modell zu bestätigen, müsse der innere Aufbau von Saturn allerdings noch genauer untersucht werden, so die Wissenschaftler im Fachblatt „Science“.
Vor knapp 4,5 Milliarden Jahren entstand der Planet Saturn aus einer Gasscheibe, die um die Sonne rotierte. Gemäß theoretischen Berechnungen sollte die Rotationsachse des Planeten nahezu senkrecht auf der Ebene seiner Umlaufbahn stehen. Doch Beobachtungen zeigen, dass die Rotationsachse von Saturn tatsächlich um etwa 26,7 Grad geneigt ist. Eine mögliche Erklärung dafür ist eine Wechselwirkung zwischen Saturn und Neptun aufgrund der zwischen ihnen wirkenden Gravitationskraft. Denn die Rotationsachse des Saturn taumelt – und zwar im gleichen Rhythmus wie die Umlaufbahn des Neptun. Dadurch könnten sich die leichten Schwankungen gegenseitig verstärken – man spricht von Resonanz – und über einen langen Zeitraum dazu führen, dass sich eine ursprünglich kleine Neigung der Achse bis auf den heutigen Wert vergrößert haben könnte.
Die Einwirkung Neptuns auf Saturn hängt sowohl von den Saturnmonden als auch vom inneren Aufbau Saturns ab. Um die Vorgänge besser zu verstehen, konstruierten Jack Wisdom vom Massachusetts Institute of Technology und seine Kollegen anhand von Daten der US-amerikanischen Raumsonde Cassini ein verbessertes Modell des Planeten. Und kamen zu einem unerwarteten Ergebnis: Saturn befindet sich laut dem neuen Modell nicht im Bereich einer Resonanz mit Neptun, sondern knapp außerhalb.
Daraufhin hatten die Forscher eine neue Idee:  Vielleicht befand sich Saturn zwar ursprünglich in Resonanz mit Neptun, wurde aber dann durch ein Ereignis daraus heraus befördert. Mithilfe von theoretischen Überlegungen und einer großen Zahl von Simulationen des Planeten und seiner Monde suchten Wisdom und sein Team nach einem solchen Szenario und fanden schließlich eine Erklärung: Saturn muss ursprünglich einen weiteren Mond besessen haben. Nur mit diesem zusätzlichen Mond – der etwa so groß wie der drittgrößte Saturnmond Japetus gewesen sein soll und den die Forscher „Chrysalis“ tauften – blieb Saturn über lange Zeit in Resonanz mit Neptun.
Die Simulation der Forscher zeigte außerdem, dass die Saturnmonde ihre Umlaufbahnen im Lauf der Zeit veränderten. So wanderte insbesondere der Orbit des größten Saturnmonds Titan langsam nach außen. Vor etwa 100 bis 200 Millionen Jahren veränderte sich dadurch auch die Bahn von Chrysalis und er näherte sich dem Saturn so sehr an, dass er durch die Gezeitenkräfte des Planeten zerrissen wurde. Ein großer Teil der Trümmer fiel auf den Planeten herab und aus den Überresten bildeten sich schließlich die heutigen Saturnringe. Ohne den Einfluss von Chrysalis befand sich Saturn schließlich nicht mehr in Resonanz mit Neptun. Ob sich dieses Szenario jedoch tatsächlich so ereignet hat, ist noch unklar und hängt vor allem vom inneren Aufbau des Saturn ab. Daher sind noch genauere Daten von zukünftigen Raumsonden nötig, um das Modell zu bestätigen.",Die Rätsel um Saturn
"Vom 19. bis zum 24. September 2022 findet in Regensburg das große Wissenschaftsfestival „Highlights der Physik“ statt. Dort geben Wissenschaftlerinnen und Wissenschaftler aus dem ganzen Land aus erster Hand und auf unterhaltsame Weise Einblicke in ihre zukunftsweisende Forschung. Mit einer spannenden Mischung aus einem interaktiven Programm vor Ort und einem umfangreichen Onlineangebot werden Physikerinnen und Physiker einen Blick in zahlreiche Bereiche der Wissenschaft ermöglichen. Die große Mitmachausstellung „Phänomikon“ bietet spannende Unterhaltung für Groß und Klein und Vorträge, beispielsweise über das Leben auf fernen Planeten oder zum Thema Energiewende, laden zum Zuhören und Mitdenken ein. Außerdem wird ein Workshop zur Programmierung von Quantencomputern angeboten sowie viele weitere interessante Themen präsentiert, bei denen Physik in unserem Leben eine wichtige Rolle spielt. Kostenlose Einlasskarten für die jewiligen Programmpunkte sind unter highlights-physik.de/tickets erhältlich.",Highlights der Physik 2022
"Schlägt ein Himmelskörper auf der Oberfläche eines Planeten ein, erschüttert dieser Aufprall den Boden des Planeten und es entsteht ein Krater. Nun ist es erstmals gelungen, diese beiden Effekte ein und desselben Einschlags auf einem fremden Planeten nachzuweisen. Mit ihren hochempfindlichen Messgeräten registrierte die Marssonde InSight die Beben gleich mehrerer Einschläge von Meteoriten. Die Krater der Einschläge ließen sich dann auf hochaufgelösten Aufnahmen des Mars Reconnaissance Orbiter wiederfinden, wie die Wissenschaftler in der Fachzeitschrift „Nature Geoscience“ berichten.
Die gleichen Meteoriteneinschläge auf einem anderen Himmelskörper mittels verschiedener Daten zu identifizieren, ist eine besondere Herausforderung. Die Möglichkeit dazu besteht insbesondere bei Mond und Mars – den beiden am besten untersuchten Himmelskörpern im Sonnensystem. Diese sind allerdings von zahllosen Kratern übersäht, was eine Analyse erschwert. Um Meteoriteneinschläge dennoch auf verschiedene Weisen zuzuordnen, griffen Raphaël Garcia von der Universität Toulouse und seine Kollegen nun auf zwei Marssonden der NASA zurück.
Der Lander InSight detektiert Einschläge von Meteoriten, indem er mit einem sogenannten Seismometer Erschütterungen des Marsbodens misst. Währenddessen spüren Drucksensoren atmosphärische Schwingungen auf – beides weist auf eingeschlagene Meteoriten hin. Im Zeitraum zwischen Mai 2020 und September 2021 hat InSight gleich vier solcher Ereignisse aufgezeichnet. Deren Ursprung, so die Forscher, waren nicht etwa Marsbeben, sondern rührten offensichtlich von Treffern aus dem All her.
Anschließend verglichen die Wissenschaftler, wann die seismischen Wellen im Untergrund und die atmosphärischen Druckwellen an den Messgeräten angekommen waren. Kommen beide innerhalb eines kurzen Zeitfensters an, liegt ein Meteoritentreffer nahe. Dabei ist zu berücksichtigen, dass akustische Signale in der dünnen Marsatmosphäre und seismische Wellen im Gestein sich unterschiedlich schnell ausbreiten. Teilweise waren die Meteoriten auch schon in der Luft explodiert. Anhand der Richtung und Laufzeitunterschiede der Signale ermittelten die Forscher dann die ungefähren Auftreffpunkte der Meteoriten: Diese lagen rund 80 bis knapp 300 Kilometern vom Lander entfernt.












                Mars Reconnaissance Orbiter
            



Diese Daten gaben die Forscher weiter an die Experten vom Mars Reconnaissance Orbiter – eine Raumsonde, die bereits seit 2006 den Roten Planeten umkreist und mit ihrer hochauflösenden Kamera seitdem Bilder von der Oberfläche des Roten Planeten schießt. Auf diesen Aufnahmen ließen sich tatsächlich die Krater identifizieren, die die vier Meteoriten geschlagen hatten: Die Krater hatten Durchmesser von rund vier bis zwölf Metern und die Meteoriten wogen beim Einschlag schätzungsweise zwischen 10 und 40 Kilogramm. Für den schwersten Meteoriten ließ sich außerdem bestimmen, dass er vor dem Eintritt in die Marsatmosphäre wohl knapp 200 Kilogramm schwer war. Durch die Reibung heizte er sich allerdings stark auf und explodierte schließlich, woraufhin sich ein Großteil des Gesteins über eine Fläche von mehreren Quadratkilometern verteilte.
Für die Marsforschung geben solche Meteoritentreffer wichtige Einblicke. Da die ausgelösten Beben durch unterschiedliches Gestein in der Marskruste laufen, lässt sich etwa aus den seismischen Messungen mehr über das Innere des Mars – insbesondere über die Struktur der Marskruste – erfahren.",Meteoriteneinschläge auf Mars gemessen
"Trotz aller Fortschritte gestaltet es sich noch immer schwierig, Vulkanausbrüche vorherzusagen. Das liegt daran, dass sich die Regionen der Erde geologisch teils stark unterscheiden und sich die Quelle des Magmas nur aufwändig untersuchen lässt. Zwei Forschungsteams haben nun den Ausbruch des Vulkans Fagradalsfjall in Island untersucht: Einerseits analysierten sie Signale vorangegangener Erdbeben und andererseits die ausgeflossene Lava. Dabei stellten sie fest, dass im Untergrund ein dynamisches Wechselspiel herrscht, bei dem Lava aus unterschiedlicher Tiefe die Eruption speisen kann.
Der Fagradalsfjall befindet sich rund 40 Kilometer von der isländischen Hauptstadt Reykjavík entfernt und ist ein sogenannter Tafelvulkan: Er besitzt eine flache, breite Erhebung im Zentrum, die von steil abfallenden, erodierten Hängen umgeben ist. Von früheren Untersuchungen war bereits bekannt, dass dieser Vulkan durchaus lange – meist um die 800 bis 1000 Jahre – ruht, und anschließend wieder – typischerweise für 200 bis 300 Jahre – aktiv wird. So war er auch nun schon seit rund 800 Jahren nicht mehr ausgebrochen – bis zum 19. März 2021. An jenem Tag öffnete sich in einem Tal im Süden des Fagradalsfjall eine Spalte, aus der über Monate hinweg Lava floss. Nach knapp einem Jahr Pause brach er dann im August 2022 erneut aus.
Dieser vulkanischen Aktivität gingen heftige Bewegungen im Untergrund voraus, die das Forschungsteam um Freysteinn Sigmundsson von der Universität Reykjavik analysierte. So hatte es schon in den vorangegangenen zwei Jahren wiederholt Erdbeben in der Region um den Fagradalsfjall gegeben – in den letzten drei Wochen vor dem Ausbruch waren es sogar über 40 000. Auch die Oberfläche des Vulkans habe sich verformt. Dies deute, so die Forscher, auf ansteigende Ströme des Magmas, also glühend heißen Gesteins, im Erdinnern hin.
In den Tagen direkt vor dem Ausbruch ab Mitte März 2021 haben diese seismischen Aktivitäten jedoch abgenommen. Dies lasse sich nach Ansicht der Wissenschaftler dadurch erklären, dass vor dem Ausbruch Spannungen im Boden aufgrund sich bewegender Erdplatten zunahmen. Dabei füllten sich kilometerlange unterirdische Hohlräume mit Magma. In der Ruhephase vor dem Vulkanausbruch sei dieser Prozess dann weitgehend abgeschlossen gewesen und die Spannungen ließen nach. Dabei stieg das Magma zur Oberfläche auf: Der Vulkan brach aus.
In einer weiteren Studie untersuchte Sæmundur Halldórsson, ebenfalls von der Universität Reykjavík, und seine Kollegen die Lava aus den ersten 50 Tagen des Ausbruchs. Anhand verschiedener Laboranalysen des Lavagesteins stellten sie fest, dass die Lava – wie Magma genannt wird, wenn es an die Erdoberfläche tritt – zu Beginn des Ausbruchs von der Grenze zwischen der unteren Erdkruste und dem Erdmantel kam – aus etwa 15 Kilometer Tiefe. In den darauffolgenden Wochen trat aber Material aus noch größerer Tiefe von teils über 20 Kilometer Tiefe an die Oberfläche. Das Ursprungsgebiet des Magmas, an der Grenze von Erdkruste und Erdmantel sowie darunter, ist also eine sehr dynamische Zone. Dort kann sich Magma aus verschiedenen Tiefen innerhalb von Tagen oder Wochen miteinander vermischen. Diese Ergebnisse geben einen seltenen Einblick, wie sich Vulkanausbrüche in Echtzeit abspielen, inklusive der vorhergehenden Entwicklung im Untergrund und der ablaufenden Magmaströme.",Was Vulkane zum Ausbruch bringt
"Knochen bestehen aus weichem, elastischem Kollagen und festem Mineral. Die Kombination dieser beiden Materialien macht Knochen besonders belastbar und bruchfest. Welche Prozesse auf der Nanoebene für diese besonderen Eigenschaften verantwortlich sind und wie sie sich mit Röntgenstrahlen untersuchen lassen, berichtet Wolfgang Wagermaier vom Max-Planck-Institut für Kolloid- und Grenzflächenforschung in Potsdam im Interview mit Welt der Physik.
Welt der Physik: Woraus bestehen Knochen?












                Wolfgang Wagermaier
            



Wolfgang Wagermaier: Knochen bestehen unter anderem aus Kollagen. Dieses Protein bildet eine faserige Struktur, die man an vielen Stellen im menschlichen Körper findet – etwa in der Haut oder in den Muskeln, und auch in Sehnen. Doch im Vergleich zu Sehnen kommt in Knochen noch ein weiteres Material hinzu: Das Mineral Kalziumphosphat, auch Hydroxylapatit genannt, ist in die Kollagenstruktur eingelagert. Die festen Mineralpartikel sind sehr klein und wechselwirken mit dem weichen Kollagen, sodass ein sogenanntes Hybridmaterial entsteht. Dieses ist zwar fest, aber durch den Kollagenanteil nicht so spröde wie etwa Keramik, sondern zäher.
Also erhält man gewissermaßen Knochen, wenn man Sehnen Minerale hinzufügt?
Das ist natürlich etwas vereinfacht dargestellt, aber aus Sicht der Materialwissenschaften nicht falsch. Tatsächlich haben wir genau das in unseren Experimenten versucht: Wir haben die Kollagenstrukturen von Sehnen künstlich mineralisiert, um diesen Prozess und die sich verändernden Materialeigenschaften zu untersuchen.
Wie genau liefen diese Experimente ab?
Wir haben wenige Zentimeter lange Probenstücke von Truthahnsehnen genommen und in verschiedene Lösungen aus Kalzium- und Strontium-Verbindungen gelegt. Die gelösten Minerale durchdrangen die Kollagenstruktur und kristallisierten dann innerhalb der Struktur aus. Das hat also funktioniert. Aber als wir die Probe unter dem Mikroskop anschauen wollten, tauchte ein Problem auf: Während der Mineralisation hat sich das Probenstück, das auf einem Glasträger klebte, immer unter dem Klebeband herausgezogen. Das war ein interessanter Effekt! Denn durch die Mineralisation schien eine unerwartete Kraft aufzutreten.
Wie haben Sie diesen Effekt dann weiter untersucht?
Die mechanische Belastung haben wir mit einer maßgeschneiderten Zugprüfanlage gemessen und so die makroskopische Kraft bestimmt, mit der sich das Sehnenstück zusammenzieht. Um allerdings zu verstehen, was auf der mikroskopischen Ebene und auch auf der Nanoebene passiert, haben wir unser Experiment mit der Röntgenlichtquelle BESSY II am Helmholtz-Zentrum Berlin untersucht.
Können Sie die Versuche an BESSY II genauer beschreiben?
Mit den brillanten Röntgenstrahlen von BESSY II kann man sich Strukturen mit sehr kurzen Belichtungszeiten anschauen. Das war für uns wichtig, denn wir hatten es ja mit einem sehr dynamischen Prozess zu tun. So haben wir Röntgenlicht in relativ kurzen Pulsen auf die Probe geschickt, die in unserem Fall aus einem Mineral mit hoher Elektronendichte und dem Kollagen mit einer geringeren Elektronendichte bestand. Und das ergibt bei unserer Methode, der Röntgenkleinwinkelstreuung, einen Kontrast, ein sogenanntes Streubild. Daraus lassen sich zum Beispiel die Größe der Mineralpartikel und der Kollagenmoleküle und die Abstände zwischen ihnen bestimmen. Wir haben also nicht nur gemessen, wie weit die Mineralisation fortgeschritten war, sondern auch, wie stark sich die Kollagenfasern zusammenzogen.













                Im Forschungslabor
            



Was haben Sie dabei beobachtet?
Man kann sich das so vorstellen: Die Sehne besteht aus Bündeln von Kollagen. Die einzelnen Moleküle sind immer gleich lang, etwa 300 Nanometer, und außerdem sind die Moleküle in Schichten leicht gegeneinander versetzt. Das heißt, man hat sehr regelmäßige Abstände, fast wie in einem Kristall. In die Lücken zwischen dem Kollagen fließt die Minerallösung hinein. Die Mineralisation beginnt und kleinste Partikel lagern sich ab. Dadurch beginnt die Kontraktion der Kollagenfasern, die sich dann auf größere Ebenen überträgt – bis sich das Probenstück sogar unter dem Klebestreifen herausziehen kann.
Welche Folge hat diese Kontraktion der Kollagenfasern?
Das gesamte Hybridmaterial steht dadurch unter Spannung, was vor allem seine mechanischen Eigenschaften verbessert. Man kann das mit Spannbeton vergleichen, den man zum Beispiel im Brückenbau verwendet, um die Festigkeit von Bauelementen zu erhöhen. Dazu nimmt man Beton, bettet darin Stahldrähte ein und zieht dann alles zusammen. Dabei entsteht eine mechanische Spannung im Material – die sogenannte Vorspannung –, wodurch die Bruchfestigkeit des Materials erhöht wird. Und das ist im Knochen ganz ähnlich: Die Kollagenfasern drücken sozusagen die Mineralpartikel zusammen.
Welche Bedeutung haben Ihre Erkenntnisse?
Zum einen lässt sich unser Experiment auch mit anderen Mineralen als Hydroxylapatit durchführen und könnte damit zu weiteren Erkenntnissen über Mineralisationsprozesse und die dabei entstehenden Kräfte führen. Außerdem könnten unsere Ergebnisse dabei helfen, knochenähnliche Materialien beispielsweise für Implantate herzustellen. Und auch für andere Bereiche der Medizin ist unsere Studie relevant. Denn es gibt Knochenkrankheiten, bei denen der Grad der Mineralisation des Knochens eine wichtige Rolle spielt – etwa die Osteoporose oder die Glasknochenkrankheit. Auch dafür ist es wichtig, die Mineralisationsprozesse genau zu verstehen.",„Ganz ähnlich wie Spannbeton“
"Schon seit geraumer Zeit rätseln Planetenforscher über die Zusammensetzung des Gesteins auf unserer Erde. Denn obwohl sich die Erde ursprünglich aus dem gleichen Material wie etwa Asteroiden gebildet hat, unterscheidet sich die atomare Zusammensetzung der Erdkruste von der anderer, kleinerer Himmelskörper. Ein Forscherteam hat nun die Häufigkeit bestimmter Atomsorten in verschiedenen Meteoriten untersucht und diese mit irdischen Proben verglichen. Dabei fanden sie eine mögliche Erklärung für die Unterschiede: Heftige Meteoritenhagel könnten die Kruste der Erde in ihrer Entstehungsphase immer wieder abgetragen und dadurch ihre Zusammensetzung verändert haben, wie die Forscher im Fachjournal „Science“ berichten.
Vor etwa 4,5 Milliarden Jahren entstand unser Sonnensystem aus einer Scheibe aus Gas und Staub. Zunächst bildeten sich unzählige kleinere Himmelskörper, die dann schrittweise miteinander kollidierten und zu Asteroiden, Monden und Planeten heranwuchsen. Damit sich ein Planet wie unsere Erde bilden konnte, waren hunderttausende Kollisionen kleinerer Körper notwendig. Paul Frossard von der Universität Clermont-Auvergne in Clermont-Ferrand und der ETH Zürich und seine Kollegen wollten nun die Prozesse, die dabei eine Rolle spielten, besser verstehen. Dazu untersuchten sie Meteoriten, in denen die ursprüngliche atomare Zusammensetzung des Materials noch genauso wie zur Entstehungszeit unseres Sonnensystems erhalten war.
Zunächst haben die Forscher bestimmt, wie häufig spezielle Isotope sowohl in irdischem Gestein als auch im Meteoritenmaterial vorkommen. Als Isotope bezeichnet man Varianten eines bestimmten Elements, die sich lediglich in der Anzahl an Neutronen in ihrem Atomkern unterscheiden. Frossard und seine Kollegen interessierten sich vor allem für das Verhältnis zweier Isotope des Seltenerdmetalls Neodym – Neodym-142 und Neodym-144. Die beiden Isotope kommen natürlicherweise vor, wobei das leichtere Neodym-142 auch beim radioaktiven Zerfall des Elements Samarium entstehen kann.
Wie die Analysen zeigten, besitzt irdisches Gestein verglichen mit den Meteoriten ein größeres Mengenverhältnis an Neodym-142 gegenüber Neodym-144. Nachdem die Forscher mit Hilfe weiterer Messungen andere Erklärungen ausschließen konnten, blieb vor allem eine stichhaltige Erklärung für dieses Verhältnis übrig: Im Gegensatz zu Asteroiden und Kometen hat die Erde – genau wie andere große Himmelskörper – geochemische Prozesse durchgemacht: Schwere Elemente wie Eisen und Nickel sind in den Erdkern gesunken, und im darüberliegenden Erdmantel und in der außen liegenden Erdkruste haben sich Silikate angelagert. Sowohl Neodym als auch Samarium verbinden sich bevorzugt mit Silikaten, auch wenn sie zu den schweren Elementen zählen – Neodym mit solchen aus der Erdkruste, Samarium mit solchen aus dem Erdmantel. Deshalb haben sich diese Elemente nach Ansicht der Forscher in der Urzeit unseres Planeten dort angereichert.
Durch die ständige Bombardierung mit kleineren Himmelskörpern wurde das Krustenmaterial dann häufig erodiert und entwich ins All. So reduzierte sich der Neodymanteil in der Erdkruste – bis der ständige Meteoritenhagel irgendwann nachließ. Währenddessen verblieb das Samarium tiefer in der Erde, wo es besser geschützt war. Im Lauf der Jahrmillionen zerfiel es zu dem Isotop Neodym-142 und gelangte dann bis zur Erdkruste – zu einer Zeit, als kaum mehr Meteoriten auf die Erde trafen. Dadurch ist in der Erdkruste heute ein erhöhter Anteil an Neodym-142, dem Zerfallsprodukt von Samarium, zu finden.
Aus dem Vergleich der Zahlenverhältnisse schlossen die Forscher, dass offensichtlich bis zu 20 Prozent des Materials der jungen Erde aufgrund solcher Kollisionen ins All entwichen ist – es fand also eine massive Erosion der irdischen Oberfläche statt. Dieser Prozess könnte auch erklären, warum in der Erdkruste andere Elemente wie Uran, Kalium und Thorium deutlich seltener vorkommen als erwartet.",Kollisionen zur Entstehungszeit der Erde
"Zu Beginn des 20. Jahrhunderts entdeckten Physiker die Quantenwelt. Die Gesetze, die in dieser mikroskopischen Welt der Teilchen und Atome gelten, entpuppten sich als grundlegend anders als in der uns bekannten makroskopischen Welt.
Vor rund hundert Jahren entdeckten Physiker, dass sich die Welt auf den kleinsten Skalen nicht mit den Gesetzen der klassischen Physik beschreiben lässt: Die Bausteine der Materie – also Atome und deren Bestandteile: Protonen, Neutronen und Elektronen – aber beispielsweise auch Lichtteilchen, zeigen ein gänzlich anderes Verhalten als wir es aus unserer Alltagswelt kennen.
Im Gegensatz zu makroskopischen Gegenständen besitzen Quantenteilchen beispielsweise neben ihren Teilcheneigenschaften auch Welleneigenschaften. So lassen sich etwa bei Elektronen, Atomen und sogar relativ großen Molekülen unter bestimmten Bedingungen typische Wellenphänomene wie Beugung oder Interferenz beobachten. Dieses Verhalten hat weitreichende Konsequenzen: Solange man nicht nachsieht, lässt sich beispielsweise nicht exakt angeben, wo sich ein Atom oder Elektron gerade befindet. Nicht, weil man es nicht weiß, sondern weil das Quantenteilchen gleichzeitig an verschiedenen Orten zu sein scheint.
Erst die Theorie der Quantenmechanik – entwickelt von Werner Heisenberg, Erwin Schrödinger und anderen Physikern – lieferte Konzepte, um die Vorgänge in der Quantenwelt zu beschreiben. An die Stelle der klassischen Bewegungsgleichungen tritt dabei eine Wellenfunktion: Sie umfasst etwa alle möglichen Aufenthaltsorte – oder allgemeiner: Zustände – eines Quantenteilchens und erlaubt Aussagen darüber, wie wahrscheinlich die verschiedenen Zustände sind. Erst durch eine Messung endet die wellenartige Überlagerung aller möglichen Zustände und das Quantensystem legt sich zufällig auf einen eindeutigen Wert fest. Die Beobachtung verändert also das Quantensystem – in der klassischen Physik undenkbar.
Eine weitere Eigenheit der Quantenwelt zeigt sich, wenn man bestimmte physikalische Größen – wie Ort und Geschwindigkeit – eines Teilchens gleichzeitig misst: Egal wie genau die Messmethode auch sein mag, die beiden Eigenschaften lassen sich nicht gleichzeitig exakt bestimmen. Ist die Position eines Teilchens sehr genau bekannt, ist seine Geschwindigkeit weitestgehend unbestimmbar. Umgekehrt wissen wir kaum etwas über seinen Aufenthaltsort, wenn wir seine Geschwindigkeit sehr genau kennen. Der Physiker Werner Heisenberg beschrieb diese für Quantenteilchen charakteristische Gesetzmäßigkeit mit seiner berühmten Unschärferelation.
Obwohl diese und weitere Quanteneffekte nicht nur den Prinzipien der klassischen Physik widersprechen, sondern auch unseren Erfahrungen – bisher stimmen alle Experimente mit den Vorhersagen der Quantenphysik überein. Und die neuen Konzepte haben längst nicht nur die Physik revolutioniert, sondern auch viele Technologien ermöglicht, die für uns heute selbstverständlich sind: Laserdrucker, Transistoren und damit Computerchips oder bildgebende Verfahren wie die Magnetresonanztomografie würde es ohne das Wissen um die Physik der Quanten nicht geben.",Quanteneffekte
"Es klingt wie Science-Fiction: Am frühen Morgen des 27. September soll zum ersten Mal in der Geschichte der Raumfahrt eine Raumsonde der NASA absichtlich mit hoher Geschwindigkeit in den Asteroiden Dimorphos einschlagen und ihn so von seinem Kurs ablenken. Zwar stellt Dimorphos keine Gefahr für die Erde dar, aber sollte ein Asteroid zukünftig die Erde bedrohen, ließen sich die Erkenntnisse aus diesem Test für eine Abwehr nutzen. Um den kontrollierten Einschlag mit etwa 24 000 Kilometern pro Stunde vorzubereiten, simulierten Sabina Raducan von der Universität Bern und ihre Kollegen im Vorhinein die Kollision der Sonde mit dem Asteroiden. Im Interview berichtet die Astrophysikerin, was genau am 27. September geschehen könnte und welche Erkenntnisse sie sich davon erhofft.












                Sabina Raducan
            



Welt der Physik: Wie wahrscheinlich ist es, dass die Erde in naher Zukunft durch Asteroideneinschläge bedroht ist?
Sabina Raducan: Ein Asteroideneinschlag ist zwar ein unwahrscheinliches, aber potenziell sehr folgenschweres bis katastrophales Naturereignis. Wir wissen, dass es immer wieder zu Einschlägen kommt – wie etwa zuletzt 2013 in Tscheljabinsk. In den letzten Jahrhunderten hatten wir das Glück, dass die Einschläge entweder klein waren oder vorwiegend unbewohnte Gebiete trafen. Mittlerweile würden wir einen bevorstehenden großen Einschlag lange – vielleicht sogar Jahrzehnte – im Voraus erkennen und könnten etwas dagegen unternehmen. Deswegen versuchen wir jetzt einen Plan zu entwickeln, um vorbereitet zu sein.
Und wie sieht dieser Plan aus?
Der einfachste Weg der Asteroidenabwehr ist es, ein Geschoss ins Weltall zu schicken, das mit hoher kinetischer Energie in den Asteroiden einschlägt und ihn von seiner Bahn ablenkt. Zumindest in der Theorie funktioniert das. Die DART-Mission der NASA – kurz für Double Asteroid Redirection Test – wird dieses Vorgehen jetzt erstmals praktisch proben. Die DART-Sonde wurde letzten November ins Weltall geschossen und wird am frühen Morgen des 27. September ihr Ziel erreichen und auf dem Asteroiden Dimorphos einschlagen. Dimorphos ist mit 160 Metern Durchmesser relativ klein und bildet gemeinsam mit Didymos ein Doppel-Asteroiden-System. DART ist in erster Linie eine planetare Verteidigungsmission und ein Technologietest. Deswegen soll die Hera-Mission der europäischen Weltraumorganisation ESA im Jahr 2024 folgen und Dimorphos zwei Jahre später erreichen. Mit Hera wollen wir dann die Bahnveränderung des Asteroiden genau vermessen und Daten zur Struktur und Zusammensetzung von Dimorphos sammeln.
Wie wird der Asteroid durch den Einschlag abgelenkt?
Durch den Einschlag der DART-Sonde entsteht zunächst ein Krater. Die dabei ausgeworfene Materie wird dem Asteroiden – ähnlich wie beim Ausstoß einer Rakete – einen zusätzlichen Impuls geben und ihn dadurch aus seiner Bahn ablenken. Die große Frage ist, wie stark Dimorphos abgelenkt wird. Das werden wir mithilfe von verschiedenen Bildern untersuchen: Vor dem Einschlag wird die Raumsonde den LICIACube, einen Kleinstsatelliten der italienischen Raumfahrtagentur ASI, abwerfen, der im Vorbeiflug erste Bilder vom Auswurf aus dem Krater machen wird. Großteleskope auf der Erde und Weltraumteleskope wie Hubble und das James-Webb-Teleskop werden zusätzliche Bilder liefern. Für genauere Messungen müssen wir aber noch etwas warten, bis Hera in vier Jahren mit weiteren Instrumenten vor Ort ist.
Was denken Sie, wie stark der Asteroid abgelenkt werden wird?
Wie sehr wir den Asteroiden ablenken können, hängt stark davon ab, woraus der Asteroid besteht. Allein in unserem Sonnensystem gibt es unzählige Asteroiden und jeder hat eine andere Zusammensetzung und Struktur. Lange dachten wir, dass Asteroiden sehr harte Oberflächen haben. Neuere Beobachtungen haben aber Objekte gezeigt, die eine extrem lockere innere Struktur besitzen und gerade noch so durch ihre eigene Schwerkraft zusammengehalten werden. Gemeinsam mit meinem Team habe ich deswegen den Aufprall der Sonde auf den Asteroiden im Vorhinein für verschiedene mögliche Zusammensetzungen und Oberflächenstrukturen des Asteroiden modelliert.













                DART-Mission
            



Wie gehen Sie bei Ihren Simulationen vor?
Wir modellieren die Kraterbildung nach dem Einschlag der Sonde mithilfe von Computersimulationen. Dafür verwenden wir als Grundlage beispielsweise Erkenntnisse aus Laborexperimenten. Bislang waren solche Simulationen allerdings auf feste Objekte ausgerichtet, auf denen sich innerhalb von wenigen Sekunden ein Krater durch einen Einschlag bildet. Denn bis vor kurzem war es nicht möglich, Systeme bei sehr niedriger Schwerkraft und von sehr geringer Festigkeit zu untersuchen: In typischen Laborexperimenten können wir diese Situation schon aufgrund der Schwerkraft der Erde nicht nachstellen und theoretische Simulationen von solchen Objekten hätten viel zu lange gedauert. Wir mussten also erst einige neue numerische Tricks einführen, um den Einschlagsprozess auch für losere Objekte zu simulieren. Das war wichtig, denn wir glauben, dass das Innere von Dimorphos tatsächlich eine sehr viel losere Struktur besitzt als zuvor angenommen. Dann würde durch den Aufprall der Sonde nicht nur ein kleiner Krater entstehen, wie wir ursprünglich dachten, sondern der Asteroid würde komplett deformiert werden.
Was haben Sie mit Ihren Simulationen noch über den Einschlag auf Dimorphos herausgefunden?
Die Geschwindigkeit von Dimorphos kann sich durch den Einschlag um wenige Millimeter pro Sekunde bis zu mehreren Metern pro Sekunde ändern – wir erwarten aber zurzeit, dass sich die Geschwindigkeit um wenige Zentimeter pro Sekunde ändern wird. Der Unterschied ist wichtig zu verstehen, denn im Ernstfall möchte man einen für uns gefährlichen Asteroiden ja stark genug ablenken, damit er die Erde verfehlt. Wenn der Asteroid noch weit genug entfernt ist, genügt dafür zwar eine minimale Bahnänderung. Aber wir müssen trotzdem wissen: Reicht es aus, eine einzelne Sonde zu senden? Brauchen wir mehrere Sonden? Wie viele sind zu viele und würden den Asteroiden sprengen?
Erwarten Sie von den Missionen neue Erkenntnisse über Asteroiden?
Da noch nie ein Einschlag auf dieser Größenskala getestet wurde, bietet sich damit auch die Möglichkeit unsere theoretischen Modelle zu überprüfen. Und falls unsere theoretischen Vorhersagen nicht zutreffen sollten, müssen wir herausfinden, warum das so ist. Mit Hera wollen wir auch besser verstehen, woraus Dimorphos im Einzelnen besteht. Da er der kleinste Asteroid ist, dem wir in nächster Zeit derart nahekommen, werden uns die Daten dabei helfen, Asteroiden besser zu verstehen. Asteroiden sind schließlich die Überbleibsel aus der Entstehung unseres Sonnensystems – wir werden also auch neue Erkenntnisse über die Entstehung der Planeten in unserem Sonnensystem gewinnen.
Wo werden Sie selbst den 27. September, den „DART Impact Day“, verbringen?
Am Johns Hopkins University Applied Physics Laboratory APL in den USA, wo ein Großteil der Raumsonde gebaut wurde, und an diesem Tag fast das gesamte Team aus Forschern und Ingenieuren vor Ort sein wird. Auf diesen Moment haben wir viele Jahre hingearbeitet und fiebern dem Ereignis natürlich alle entgegen. Wenn wir Erfolg haben, wird das eine große Feier. Bevor die DART-Sonde durch den Aufprall zerstört wird, wird sie beim Annähern Bilder machen und live an uns auf die Erde funken. Alle können das über eine Live-Übertragung direkt mitverfolgen.",„Wir wollen einen Asteroiden ablenken“
"In Computerchips oder beispielsweise bei bildgebenden Verfahren wie der Magnetresonanztomografie macht man sich bereits die besonderen Eigenschaften der kleinsten Teilchen, die den Gesetzen der Quantenphysik gehorchen, zunutze. Momentan arbeiten Physiker daran, die Quanteneffekte gezielt zu kontrollieren und so ganz neue Anwendungen zu ermöglichen.
In unserem Alltag lassen sich zahlreiche Beispiele für Technologien finden, die es ohne das Wissen um die Physik der Quanten nicht geben würde. Da sich diese Phänomene auf winzigen Größenskalen abspielen, blieben sie allerdings lange verborgen. Erst zu Beginn des 20. Jahrhunderts entdeckten Physiker, dass sich die Welt auf den kleinsten Skalen nicht mit den Gesetzen der klassischen Physik beschreiben lässt: Die Bausteine der Materie – also Atome und deren Bestandteile – aber beispielsweise auch Lichtteilchen, zeigten ein gänzlich anderes Verhalten, als wir es aus unserer Alltagswelt kennen. Erst die Theorie der Quantenmechanik – entwickelt von Werner Heisenberg, Erwin Schrödinger und anderen Physikern – lieferte Konzepte, um die Vorgänge in der Quantenwelt zu beschreiben.
Und diese neuen Konzepte haben längst nicht nur die Physik revolutioniert, sondern auch viele Technologien ermöglicht, die für uns heute selbstverständlich sind. So fußt beispielsweise der Transistor, der sich auf jedem Computerchip findet, auf dem Verständnis von quantenmechanischen Prozessen in Halbleitern. Doch Physiker wollen noch einen Schritt weitergehen und sich die Quanteneffekte nicht nur zunutze machen, sondern sie gezielt kontrollieren und so ganz neue Anwendungen ermöglichen.
Ein Beispiel sind Quantencomputer: Als Informationsträger nutzen diese neuartigen Rechner keine klassischen Bits, sondern sogenannte Qubits. Wie die Bits in herkömmlichen Computern können auch Qubits die beiden Zustände „Null“ und „Eins“ darstellen. Dank den Gesetzen der Quantenphysik können sie sich darüber hinaus allerdings auch in allen möglichen Überlagerungszuständen von „Null“ und „Eins“ – Superposition genannt – befinden. Zudem können sich die unterschiedlichen Qubits eines Quantencomputers in einem sogenannten verschränkten Quantenzustand befinden. Dadurch erreicht die Rechenleistung eines Quantencomputers einen sehr hohen Grad an Parallelität, sodass sie die von herkömmlichen Computern weit übertrifft.
Inzwischen existieren bereits erste Quantencomputer und liefern sogar erste Ergebnisse: indem sie etwa als sogenannte Quantensimulatoren andere, komplexere Quantensysteme nachahmen, die sich selbst weder im Labor untersuchen lassen, noch mit Supercomputern simulieren lassen. Durch geschickte Manipulation des künstlichen Systems können Wissenschaftler dann das Verhalten des realen Systems nachstellen und systematisch untersuchen.
Andererseits ermöglichen Quanteneffekte aber auch Verschlüsselungsverfahren, die prinzipiell unknackbar sind: die sogenannte Quantenkryptografie. Hier werden einzelne Quanten – üblicherweise Lichtteilchen – durch Glasfasern oder in der Luft mithilfe von Teleskopen übertragen und vom Empfänger gemessen. Fängt ein Spion währenddessen die Lichtquanten ab, würde er eindeutige Spuren hinterlassen. Empfänger und Sender merken dadurch sofort, dass jemand die Leitung anzapft. Das erlaubt im Prinzip eine absolut abhörsichere Kommunikation.
Auch auf dem Gebiet der Quantensensorik kommen Quanteneffekte gezielt zum Einsatz, um beispielsweise Längen oder elektrische und magnetische Felder viel präziser zu messen als mit klassischen Techniken. Mit den neuartigen Sensoren ließen sich beispielsweise die Auflösung von Radarsystemen oder die Genauigkeit von Navigationssatelliten verbessern.",Quantentechnik
"Der Nobelpreis für Physik wird in diesem Jahr an Alain Aspect von der Universität Paris-Saclay und der École Polytechnique in Frankreich, John F. Clauser von J.F. Clauser & Assoc. in Walnut Creek in den USA und Anton Zeilinger von der Universität Wien „für Experimente mit verschränkten Photonen, Nachweise der Verletzung der Bell‘schen Ungleichung und Pionierarbeiten auf dem Gebiet der Quanteninformation” verliehen.
Zu Beginn des 20. Jahrhunderts entdeckten Physiker, dass sich in der mikroskopischen Welt der Teilchen und Atome ein gänzlich anderes Verhalten als in der klassischen Welt zeigt. Erst die Theorie der Quantenmechanik – entwickelt von Werner Heisenberg, Erwin Schrödinger und anderen Physikern – lieferte Konzepte, um die Vorgänge in der Quantenwelt zu beschreiben. Gemäß dieser Theorie können Teilchen beispielsweise miteinander verschränkt sein, wodurch bestimmte Eigenschaften miteinander verknüpft sind und sie sich nicht mehr unabhängig voneinander beschreiben lassen. Zudem werden die Eigenschaften eines Quantenteilchens erst durch eine Messung festgelegt. Dadurch verändert die Messung an einem Teilchen unmittelbar den Zustand eines anderen mit ihm verschränkten Teilchens – egal wie weit beide voneinander entfernt sind. Damit wären gleich zwei Prinzipien der klassischen Physik nicht eingehalten, zum einen Lokalität – es gibt keine „spukhafte Fernwirkung“ – und zum anderen Realismus – die Eigenschaften der Gegenstände existieren unabhängig davon, ob wir sie messen.
Albert Einstein, der die quantenmechanische Verschränkung nur als „spukhafte Fernwirkung“ bezeichnete, lehnte diese nichtlokalen Eigenschaften der Quantentheorie ab. Gemeinsam mit Boris Podolsky und Nathan Rose veröffentlichte er im Jahr 1935 das als EPR-Paradoxon bekannte Gedankenexperiment, mit dem sie zeigen wollten, dass die Quantenmechanik keine vollständige Beschreibung der Realität darstellt. Eine bessere Theorie sollte diesen nichtlokalen Charakter laut der Physiker nicht mehr aufweisen, sondern die Messergebnisse von quantenmechanischen Zuständen auf bislang verborgene Variablen zurückführen.
Diese Vorstellung einer Theorie mit verborgenen Variablen lässt sich beispielsweise mit einer Maschine veranschaulichen, die zwei gegenüberstehenden Personen je einen roten beziehungsweise einen blauen Ball zuwirft. Fängt die eine Person einen roten Ball, weiß sie sofort, dass die andere Person einen blauen Ball gefangen hat. Die Bälle haben ihre Farbe gemäß Einstein damit schon vor dem Wurf, wodurch keine „spukhafte Fernwirkung“ zwischen den Bällen wirken würde. Im Gegensatz dazu verhält es sich laut den Gesetzen der Quantenmechanik vollkommen anders: Die Ballfarbe ist zunächst unbestimmt und wird erst zufällig festgelegt, sobald ein Ball gefangen wurde. Da die Bälle miteinander verschränkt sind, ist auch automatisch die Farbe des anderen Balls festgelegt.












                Verschränkung in der Quantenmechanik
            



Im Jahr 1964 gelang es dem irischen Theoretiker John Stewart Bell, diese philosophischen Fragen des EPR-Paradoxons in eine mathematische Gleichung zu übersetzen. Mit dieser nach ihm benannten Bell‘schen Ungleichung kann man Aussagen darüber treffen, wie stark die Messergebnisse an verschränkten Teilchen im Rahmen einer lokal realistischen Theorie miteinander korreliert sind. Damit lässt sich überprüfen, ob sich Quantenteilchen wie Objekte im Sinne der Quantentheorie verhalten oder ob sie doch lokale Zusatzeigenschaften im Sinne Einsteins aufweisen. Dank dieser bahnbrechenden Arbeit von Bell wurden die grundlegenden naturphilosophischen Fragen zur Quantenphysik experimentell unterscheidbar. Doch es dauerte noch einige Jahre, bis die Ideen von Bell experimentell untersucht wurden – und dabei spielten die diesjährigen Preisträger eine entscheidende Rolle.
Im Jahr 1974 führte John Francis Clauser, damals an der University of California in Berkeley, gemeinsam mit seinen Kollegen ein Experiment durch, das erstmals eindeutig auf eine Verletzung der Bell‘schen Ungleichung hinwies. Dafür untersuchten die Physiker die Polarisation – die Schwingungsrichtung – von verschränkten Photonenpaaren. Die Polarisation zeigte dabei aufgrund der Verschränkung in dieselbe Richtung, war zunächst aber unbekannt. Erst durch das unabhängige Messen der Polarisation der beiden Photonen – jeweils mit einem Polarisationsfilter – wurde diese bestimmt. Das Besondere an dem Versuchsaufbau war, dass Clauser und seine Kollegen den Winkel zwischen den beiden Polarisationsfiltern variierten. So stellten sie fest, dass die Wahrscheinlichkeit, dass ein Photon durch den Filter hindurchging, von der Messung abhing, die an dem anderen Photon durchgeführt wurde. Damit wurde der Zufallscharakter der Quantenmechanik bestätigt und Albert Einsteins Vorstellung von einem lokalen Realismus widerlegt.
Auch Alain Aspect von der Universität Paris-Saclay beschäftigte sich in seinen Experimenten mit der Bell’schen Ungleichung. Gemeinsam mit seinem Team entwickelte er die experimentelle Methode von Clauser nochmals weiter und konnte 1982 abermals die von Einstein, Podolsky und Rosen postulierten verborgenen Parameter ausschließen. Der neue Aufbau des Experiments ermöglichte es, sogenannte „Schlupflöcher“ zu stopfen, um etwa externe Einflussfaktoren oder unbekannte Kräfte auszuschließen. Und seither werden derartige Experimente immer wieder weiterentwickelt, um noch mehr Schlupflöcher zu schließen – so auch im Jahr 1999 von Anton Zeilinger von der Universität Wien und seinen Kollegen Daniel Greenberger und Michael Horne.
Und auch mit weiteren Experimenten sorgten die Nobelpreisträger für Aufsehen. Beispielsweise gelang es Anton Zeilinger gemeinsam mit seinen Kollegen im Jahr 1997 erstmals, einen Quantenzustand zu teleportieren – eine Idee, die Physiker zu Beginn der 1990er-Jahre begannen zu diskutierten. In ihren Experimenten übertrugen Zeilinger und sein Team einen bestimmten Quantenzustand von einem Teilchen an einem Ort auf ein Teilchen an einem anderen Ort. Bei einer solchen Quantenteleportation geht der Zustand des Ursprungsteilchens verloren und das andere Teilchen nimmt diesen Zustand an.
Die drei Physiker werden somit für bahnbrechende Experimente ausgezeichnet, mit denen sie in den vergangenen Jahrzehnten die Gesetzmäßigkeiten der Quantenmechanik immer wieder überprüften und bestätigten. Ihre Ergebnisse revolutionierten nicht nur die Physik, sondern bereiteten auch den Weg für neue Technologien, mit denen sich Quantenzustände kontrollieren lassen und ganz neue Anwendungen ermöglicht werden – von der Quantensensorik, über Quantencomputer bis hin zur Quantenkryptographie.",Nobelpreis für Physik 2022
"Nach mehrjähriger Wartungs- und Umbaupause geht der weltweit größte und leistungsfähigste Teilchenbeschleuniger, der Large Hadron Collider am Forschungszentrum CERN – kurz LHC –, Ende März wieder in Betrieb. Mehrere Jahre lang soll er in dieser Run 3 genannten Betriebsphase neue Daten sammeln. Im Interview mit Welt der Physik erzählt Andreas Hoecker vom Forschungszentrum CERN, welche Teile des Beschleunigers und der Detektoren während der Betriebspause erneuert wurden und was sich die Forscher von dem dritten Run erhoffen.
Welt der Physik: Was ist während der etwa dreijährigen Betriebspause am LHC passiert?












                Andreas Hoecker
            



Andreas Hoecker: Es wurde an allen Ecken und Enden gearbeitet. Sowohl am Beschleuniger selbst als auch an den Detektoren wurden viele Komponenten ausgetauscht und durch bessere Systeme ersetzt. Eine besonders wichtige Neuerung ist der Linearbeschleuniger Linac 4 – die erste Beschleunigungsstufe der Protonen.
Welche Funktion hat dieser Linearbeschleuniger?
Im LHC werden Protonen in bis zu 2800 Paketen zur Kollision gebracht. Um die besten Ergebnisse zu erzielen, sollten sich möglichst viele Protonen mit genau definierter Energie in jedem Paket befinden. Deswegen werden die Protonen, bevor sie den LHC erreichen, durch eine Kette von Vorbeschleunigern von null auf 450 Gigaelektronenvolt beschleunigt und in Pakete gepackt. Der Linac 4 ist die erste Beschleunigungsstufe in der Kette. Er liefert eine mehr als dreimal höhere Endenergie der Protonen als sein Vorgänger, der Linac 2, bevor diese dann in den nächsten Beschleuniger geleitet werden. Durch einen Trick – es werden negativ geladene Wasserstoffionen an Stelle der Protonen beschleunigt – kann er außerdem mehr Protonen beschleunigen. Das ist eine wichtige Voraussetzung für die Weiterentwicklung des LHC zum „High-Luminosity LHC“, für den dann in einigen Jahren die Kollisionsraten nochmals deutlich erhöht werden sollen.
Welche Energie erreicht der Teilchenstrahl nun insgesamt?
Wir erwarten beim Run 3 durchgehend eine Kollisionsenergie von 13,6 Teraelektronenvolt – oder kurz TeV. Das ist etwas mehr als die Kollisionsenergie von 13 TeV, die während des Run 2 von 2015 bis 2018 erreicht wurde. Die höhere Energie erhöht die Rate, mit der schwere Teilchen – wie beispielsweise das Higgs-Boson – entstehen. Außerdem wird die Suche nach neuen Teilchen in einem zusätzlichen Massenbereich ermöglicht, der bislang nicht zugänglich war. Die neue Energie ist allerdings etwas niedriger als die ursprünglich geplanten 14 TeV, die dann hoffentlich bei Run 4 erreicht werden. Dafür müssten alle Magnete im LHC eigentlich ihre volle Leistung bringen. Wir hätten allerdings monatelange Verzögerungen riskiert, wenn sich nun beim Hochfahren auf die maximale Magnetfeldstärke auch nur bei einem einzigen der 1232 Magnete Probleme entwickelt hätten. Auch angesichts der Schwierigkeiten und Verzögerungen aufgrund der Coronapandemie wurde entschieden, dieses Risiko nicht einzugehen. Die ersten Kollisionen mit der erwarteten Energie von 13,6 TeV sollten Anfang bis Mitte Juni dieses Jahres stattfinden.












                Linac 4
            



Wie gut sind die Umbaumaßnahmen insgesamt vorangekommen?
Alle beteiligten Kollaborationen mussten in den letzten Jahren trotz der coronabedingten Einschränkungen mit vollem Einsatz arbeiten, um die geplanten Umbaumaßnahmen durchführen zu können. Damit haben wir den LHC mitsamt allen Detektoren nochmals deutlich leistungsfähiger gemacht. Alle Kollaborationen sehen dem Run 3 mit hohen Erwartungen entgegen. In den kommenden vier Jahren werden wir rund doppelt so viele Daten sammeln wie bei der vorherigen Betriebsphase.
Welche Veränderungen gibt es bei den großen Detektoren?
Am LHC selbst gibt es vier große Experimente: ATLAS und CMS sind die beiden großen Universaldetektoren, während ALICE und LHCb auf bestimmte Prozesse spezialisiert sind. Bei allen Experimenten gab es erhebliche Veränderungen, die deren Leistungsfähigkeit verbessert haben. Beim CMS-Detektor wurde unter anderem der innerste Teil des Spurdetektors erneuert. Bei ALICE wurden etwa die inneren Spurkammern ersetzt. Zudem wurde ein neues Auslesesystem entwickelt, mit dem rund fünfzigmal mehr Daten erfasst werden können, als bislang möglich war. Beim LHCb-Experiment wurde sogar fast der komplette Detektor erneuert, so dass dort ebenfalls ein mehrfacher Anstieg der Datenrate zu verzeichnen sein wird.
Und am ATLAS-Detektor?
Wir haben im Wesentlichen zwei Komponenten ersetzt, die für eine bessere Online-Selektion wichtig sind. Mit dieser Online-Selektion lassen sich in zwei Stufen die im Detektor gemessenen Kollisionsprozesse, die sogenannten Ereignisse, von 30 Millionen pro Sekunde auf ungefähr 1500 pro Sekunde reduzieren. Es werden dann nur diese interessantesten 1500 Ereignisse pro Sekunde für die spätere Analyse gespeichert. Zur Auswahl dieser physikalisch besonders aufschlussreichen Ereignisse dienen einerseits die neuen kleinen Myon-Räder, mit denen die besonders durchdringenden Myonen mit besserer Genauigkeit nachgewiesen werden können. Außerdem haben wir die Elektronik für die Online-Selektion des sogenannten Kalorimeters deutlich verbessert, sodass wir Elementarteilen, wie Elektronen, Photonen, Tau-Leptonen sowie bestimmte Teilchenjets genauer identifizieren können. Das alles ermöglicht es uns, die interessantesten Ereignisse mit größerer Effizienz online zu selektieren und dadurch nur die physikalisch relevantesten Daten zu speichern.
Gibt es auch neue Experimente am LHC?
Ungefähr 480 Meter vor und hinter dem ATLAS-Detektor entlang des LHC-Strahlrohrs gibt es zwei kleine neue Detektoren, die nach seltenen, hypothetischen Teilchen suchen und Neutrinos untersuchen können. Sie heißen FASER, das steht für Forward Search Experiment, und SND, kurz für Scattering and Neutrino Detector. Die großen Detektoren wie ATLAS untersuchen alle Prozesse, die rund um die Kollision stattfinden, sind allerdings weitgehend blind für Ereignisse, deren Produkte entlang des Strahlrohrs fliegen. Solche könnten bislang unbekannte, leichte und nur schwach wechselwirkende Teilchen enthalten, wie sie von einigen Theorien zur Dunklen Materie vorhergesagt werden. Der Zerfall solcher Teilchen könnte in FASER und SND entdeckt werden. Mit den beiden Experimenten sollen auch Reaktionen von Neutrinos gemessen werden, die bei den Kollisionen in ATLAS in großen Mengen produziert werden.













                FASER
            



Mit welchen Ergebnissen rechnen Sie bei Run 3?
Wir werden unter anderem das Higgs-Teilchen noch genauer unter die Lupe nehmen. Um den grundlegenden Prozess der Massenerzeugung durch den Higgs-Mechanismus zu testen, wollen wir nach sogenannten Di-Higgs-Prozessen Ausschau halten. Das sind Ereignisse, bei denen gleich zwei Higgs-Teilchen auf einmal produziert werden. Schon die Produktion einzelner Higgs-Teilchen ist aber sehr selten. Higgs-Doppelerzeugungen sind noch einmal rund tausendmal seltener. Wir gehen zwar derzeit davon aus, dass wir den High-Luminosity-LHC brauchen, um Di-Higgs-Ereignisse statistisch signifikant nachzuweisen. Doch vielleicht können wir erste Anzeichen schon im Run 3 sehen. Denn am spannendsten ist natürlich immer das, womit man nicht rechnet.","„Am spannendsten ist das, womit man nicht rechnet“"
"Vor zwei Jahren meldeten Astronomen die Entdeckung eines Schwarzen Lochs in der Nähe des etwa tausend Lichtjahre entfernten Sterns HR 6819. Es wäre damit das bislang der Erde nächstgelegene bekannte Schwarze Loch – und sorgte weltweit für Schlagzeilen. Genauere Beobachtungen zeigen nun jedoch, dass es sich zwar um einen ungewöhnlichen Stern handelt, sich dieser allerdings nicht um ein Schwarzes Loch dreht. Zu der Fehlinterpretation sei es gekommen, weil sich der Stern in einer seltenen Entwicklungsphase befinde, so die Wissenschaftler im Fachblatt „Astronomy & Astrophysics“.
Der Stern HR 6819 ist unter günstigen Bedingungen sogar mit bloßen Augen am Südhimmel zu erkennen. Thomas Rivinius von der Europäischen Südsternwarte ESO und seine Kollegen hatten das Sternenlicht von HR 6819 mit einem Teleskop genau untersucht und analysiert, wie viel Licht welcher Wellenlänge auf ihr Teleskop trifft. Da sich dieses Spektrum sowohl mit der Temperatur als auch mit der Bewegung eines Sterns verändert, erhalten Astronomen auf diese Weise Informationen über ihn. Doch das Spektrum von HR 6819 war ungewöhnlich und konnte nicht von einem einzigen Stern stammen.
Zunächst erklärten die Forscher um Rivinius ihre Entdeckung mit einem Modell, nach dem es sich bei HR 6819 um ein System aus insgesamt drei Objekten handle, von denen eines ein Schwarzes Loch mit etwa der vierfachen Masse der Sonne sei. Um dieses Schwarze Loch wiederum kreise auf einer engen Umlaufbahn ein Stern mit etwa fünf Sonnenmassen. In deutlich größerer Entfernung ziehe laut dem Modell ein weiterer Stern seine Bahn um dieses enge Doppelsystem. Über dessen Masse konnten die Forscher jedoch keine Aussage machen.
Bereits kurz darauf zeigten Wissenschaftler um Julia Bodensteiner, damals an der Universität Löwen in Belgien, jedoch, dass sich das Spektrum von HR 6819 auch anders erklären ließe – und zwar ohne ein Schwarzes Loch. Stattdessen könne das System aus lediglich zwei Sternen bestehen, von denen einer dem anderen einen Teil seiner Materie entrissen habe.
Um zu entscheiden, um was für ein System es sich bei HR 6819 nun tatsächlich handelt, führten beide Forscherteams gemeinsam weitere Beobachtungen mit dem Very Large Telescope der Europäischen Südsternwarte durch. „Wir waren uns einig, dass es zwei leuchtende Sterne in dem System gibt“, so Rivinius. „Die Frage war also, ob sie weit voneinander entfernt sind, wie im Szenario des Schwarzen Lochs, oder einander eng umkreisen, wie im alternativen Szenario.“ Das Ergebnis war eindeutig: Die Bilder zeigten zwei eng beieinander liegende Sterne. „Unsere beste Erklärung ist jetzt, dass wir diesen engen Doppelstern kurz nach dem Moment sehen, in dem ein Stern seinem Begleiter die Atmosphäre entrissen hat“, erläutert Bodensteiner.
Die Korrektur eines Ergebnisses sei für die Wissenschaft nicht nur normal, sondern „genau so muss es sein“, erläutert Rivinius, „Ergebnisse müssen von Kollegen kritisch unter die Lupe genommen werden“. Damit enthalte HR 6819 zwar kein Schwarzes Loch, biete den Astronomen aber die seltene Gelegenheit, einen Einblick in eine spannende Phase der Entwicklung eines Doppelsterns zu erhalten.",Doch kein Schwarzes Loch
"Der Amazonasregenwald ist nicht nur einer der wichtigsten Spender von Sauerstoff, auch für zahlreiche Arten bildet er ein einzigartiges Refugium und spielt für die Biodiversität eine entscheidende Rolle. Doch das rund sechs Millionen Quadratkilometer große Ökosystem leidet inzwischen immer stärker – nicht nur unter der intensiven Nutzung durch den Menschen, sondern auch unter dem Klimawandel. Klimaforscher warnen nun in der Fachzeitschrift „Nature Climate Change“, dass die Widerstandskraft des tropischen Regenwaldes, auch Resilienz genannt, in den vergangenen beiden Jahrzehnten sehr gelitten habe und etwa drei Viertel der Waldfläche davon betroffen seien.
In den Grünpflanzen im Amazonasregenwald sind gigantische Mengen von Kohlenstoff als Biomasse gespeichert, da die Pflanzen für die Photosynthese tagsüber Kohlendioxid aufnehmen. Nachts erzeugen die Pflanzen dann ihre Energie, indem sie Kohlenhydrate und Sauerstoff verbrauchen und wieder Kohlendioxid freisetzen. Wenn sich allerdings bestimmte äußere Faktoren ändern, kann die Effizienz der Photosynthese sinken – im Extremfall kann dadurch von den Pflanzen sogar mehr CO2 abgegeben als aufgenommen werden. Das passierte bereits in den Dürrejahren zwischen 2005 und 2010, in denen sich die Tropenregion kurzfristig von einer CO2-Senke in eine CO2-Quelle verwandelte.
Diese schwindende Resilienz haben nun Chris Boulton von der University of Exeter in England und zwei weitere Klimaforscher genauer untersucht. Da die Resilienz des Amazonasregenwaldes bislang nur mit Modellberechnungen abgeschätzt wurde, analysierten die Forscher stattdessen Vegetations- und Niederschlagsdaten des Regenwalds. Dafür verwendeten sie Daten, die zwischen den Jahren 1991 und 2016 von verschiedenen Erdbeobachtungssatelliten gesammelt wurden. Die Analyse der Datensätze ergab, dass sich die Vegetation nach Flächenbränden oder Dürrephasen immer langsamer und schlechter regeneriert. Von dieser Destabilisierung des Ökosystems sind vor allem relativ trockene Regionen und Flächen in der Nähe menschlicher Infrastruktur besonders stark betroffen.
„Das bestätigt, dass eine Begrenzung der Abholzung und auch eine Reduktion der Treibhausgasemissionen zum Schutz des Amazonas nötig sind“, so der beteiligte Klimaforscher Tim Lenton. Zusätzlich ist eine schrumpfende Resilienz ein wichtiger Indikator für einen drohenden Verlust des Regenwaldes. „Jedoch können wir nicht sagen, wann dieser mögliche Wandel vom Regenwald zur Savanne eintreten wird"", sagt Niklas Boers vom Potsdam Institut für Klimafolgenforschung. „Doch wenn sich das beobachten lässt, wird es wahrscheinlich zu spät sein, um es noch zu verhindern.“ So liefert diese Studie Hinweise, dass aufgrund des Klimawandels auch im Amazonasregenwald ein Kipppunkt erreicht werden könne – also eine nicht mehr umkehrbare Entwicklung des Ökosystems wie beispielsweise das vollständige Abschmelzen der Arktis oder das Auftauen der Permafrostböden.",Regenwald verliert seine Selbstheilungskräfte
"Die Idee eines Quantencomputers ist schon einige Jahrzehnte alt. Doch in den vergangenen Jahren haben Wissenschaftler große Fortschritte dabei gemacht, die Idee zu verwirklichen. Auf welchen Phänomenen der Quantenphysik ein Quantencomputer beruht, wie sich ein solcher Rechner technisch realisieren lässt und wo er gegenüber einem klassischen Computer überhaupt Vorteile bietet, berichtet Rainer Blatt von der Universität Innsbruck in dieser Folge des Podcasts.












                Rainer Blatt
            



Erst zu Beginn des 20. Jahrhunderts entdeckten Physiker, dass sich die Welt auf den kleinsten Skalen nicht mit den Gesetzen der klassischen Physik beschreiben lässt: Für die Bausteine der Materie – also Atome und deren Bestandteile –, aber beispielsweise auch für Lichtteilchen, versagten die etablierten Theorien.Rainer Blatt: „Das Verhalten dieser Quanten war zunächst rätselhaft, konnte aber dann – Mitte der 1920er-Jahre – vor allem durch Werner Heisenberg und Erwin Schrödinger beschrieben werden.“ Dieser Schritt gelang den beiden Physikern allerdings nur, indem sie die klassische Physik hinter sich ließen und völlig neue Konzepte einführten. Mehr dazu in der 338. Folge.",Quantencomputer
"Der 444 Lichtjahre entfernte Stern IRS 48 ist von einer Scheibe aus Gas und Staub umgeben, in der vermutlich Planeten entstehen. Forscherinnen haben in dieser Scheibe nun erstmals das aus neun Atomen bestehende Molekül Dimethylether nachgewiesen – ein möglicher Baustein für die Entstehung von Leben. Vermutlich gebe es dort auch noch größere und komplexere Moleküle, so die Wissenschaftlerinnen im Fachblatt „Astronomy & Astrophysics“.
Seit langem wissen Astronomen, dass sich bereits in dichten Gaswolken – aus denen Sterne entstehen – komplexe organische Moleküle bilden können. Diese Verbindungen auf der Basis von Kohlenstoff gelten als die Bausteine des Lebens. Forscher vermuteten, dass die Moleküle von dort aus auch in protoplanetare Scheiben vordringen, in denen sich aus Gas und Staub Planeten bilden. Unter geeigneten Bedingungen könnten die komplexen Moleküle auf den Planeten dann die Entstehung von Leben anstoßen. Doch bislang haben Wissenschaftler in protoplanetaren Scheiben nur sehr einfache Verbindungen wie Methanol gefunden.
Auf der Suche nach komplexeren Molekülen haben Nashanty Brunken von der Sternwarte Leiden und ihre Kolleginnen nun den Stern IRS 48 mit der Radioteleskopanlage ALMA in Chile genau untersucht. Denn in der protoplanetaren Scheibe des Sterns befindet sich eine Region, in der sich – vermutlich durch die Schwerkraft eines entstehenden Planeten – viele Staubkörner ansammeln. Die Staubkörnchen sind wiederum von Eisschichten umhüllt, die auch größere organische Moleküle enthalten können. Verdampft das Eis durch die Hitze des jungen Sterns, werden auch diese Moleküle freigesetzt. Dabei geben sie eine charakteristische Strahlung ab, die sich, so die Hoffnung der Forscherinnen, nachweisen ließe.
Tatsächlich gelang es den Astronominnen, die Strahlung von verdampfendem Dimethylether einzufangen. Außerdem stießen sie bei ihren Beobachtungen auch auf Strahlung, die auf Ameisensäuremethylester hinweist – ein organisches Molekül aus acht Atomen. Aus solchen Molekülen können sich wichtige Grundbausteine für die Entstehung von Leben wie etwa Aminosäuren und Zucker bilden. „Wir können diese Moleküle jetzt von den Sternentstehungsregionen über protoplanetare Scheiben bis hin zu Kometen verfolgen“, fasst Nienke van der Marel von der Sternwarte Leiden die Ergebnisse zusammen. „Damit lernen wir auch etwas über die Entstehung des Lebens auf unserem Planeten“, erklärt Brunken, „und wir erhalten eine genauere Vorstellung davon, welches Potenzial es für die Entstehung von Leben in anderen Planetensystemen gibt.“",Moleküle in protoplanetarer Scheibe
"Wie sich Energie speichern lässt und warum das für eine klimaneutrale Strom- und Wärmeversorgung wichtig ist, erklärt Michael Sterner von der Ostbayerischen Technischen Hochschule Regensburg in dieser Folge des Podcasts.












                Michael Sterner
            



Ohne Energie in Form von elektrischem Strom und Wärme ist unser Leben kaum vorstellbar. Wir sind es gewohnt, dass wir immer dann auf Energie zugreifen können, wenn wir sie brauchen. Eine zentrale Rolle spielen dabei Energiespeicher.Michael Sterner: „Wenn Menschen das Wort ‚Energiespeicher‘ hören, denken sie zuerst an Batterien. Warum? Weil sie uns im Alltag einfach am häufigsten begegnen. Ich habe die kleine Zellenbatterie, die ich von Kindesbeinen an aus meinem Spielzeug kenne, und ich habe jeden Tag mein Handy in der Hand und muss es aufladen.“Tatsächlich gibt es neben elektrochemischen Energiespeichern wie den Batterien noch eine ganze Reihe anderer Energiespeichersysteme. Und längst nicht alle sind eine Erfindung des Menschen. Mehr dazu in der 331. Folge.",Energiespeicher
"In sonnigen Wüstenregionen lässt sich besonders viel Strom mit Solarkraftwerken erzeugen. Doch in solchen trockenen Gebieten mangelt es an Wasser, um die Module regelmäßig von abschattenden Staubschichten zu befreien. Dieses Problem wollen Wissenschaftler nun mit einer elektrostatischen Reinigungsmethode lösen. Wie sie in der Fachzeitschrift „Science Advances"" berichten, ließen sich in ersten Versuchen die meisten Staubpartikel mithilfe von elektrischen Spannungsfeldern schonend beseitigen.
In Nordafrika, auf der arabischen Halbinsel oder im US-Bundesstaat Arizona wird bereits mit riesigen Solarkraftwerken elektrischer Strom erzeugt. Doch binnen eines Monats können sich so viele Staubpartikel auf den Modulen ablagern, dass deren Leistung halbiert wird. Ein einfaches Abfegen der verdreckten Solarmodule ist nicht zu empfehlen, da die Oberfläche leicht verkratzt und dadurch die Stromausbeute dauerhaft verringert werden kann. Bislang werden deswegen große Wassermengen für die Reinigung verwendet, die aber vor allem in solchen trockenen Regionen nur schwierig aufzubringen sind. Um Wasser einzusparen, entwickelten Sreedath Panat und Kripa Varanasi vom Massachusetts Institute of Technology in Cambridge nun eine alternative Reinigungsmethode.
Zunächst betrachteten die beiden Forscher die Staubpartikel aus einer Wüste in Arizona genauer. Es zeigte sich, dass die Partikel hauptsächlich aus Sand bestehen – genauer aus Siliziumdioxid. Eigentlich lassen sich diese Kristalle nicht elektrostatisch aufladen, doch bei einer Luftfeuchtigkeit zwischen 30 und 95 Prozent absorbieren die Sandkörnchen so viel Wasser, dass sich ihre elektrische Leitfähigkeit erhöht. In ersten Laborversuchen verteilten Panat und Varansi leicht feuchte Sandkörnchen verschiedener Größen auf einem kleinen Solarmodul, das mit einem durchsichtigen und elektrisch leitfähigen Material beschichtet war. An das Solarmodul schlossen sie eine Elektrode an, mit der sich Spannungen von bis zu 12 000 Volt erzeugen ließen. Aufgrund dieser elektrischen Spannungen luden sich die Sandkörnchen positiv auf und wurden von der negativ geladenen Elektrode angezogen.
Die Laborversuche von Panat und Varansi zeigen, dass sich verdreckte Solarmodule mithilfe von tragbaren Elektroden fast vollständig reinigen lassen: Nahezu alle Sandkörnchen mit einem Durchmesser von über 30 Mikrometer wurden bereits ab Spannungen von 6000 Volt von der Elektrode angezogen. Nur kleinere Körnchen blieben wegen größerer Haftkräfte auf der Oberfläche liegen. Ein aufwendiges Säubern mit Wasser ist damit nicht mehr nötig, sofern die Solarmodule mit dem speziellen Material beschichtet sind. Auch lasse sich die Methode, so die Forscher, in vielen Wüstenregionen anwenden, da die Luft zumindest in den Nachtstunden eine ausreichende Feuchtigkeit aufweise.",Saubere Solarmodule ohne Wasser
"Etwa 350 Lichtjahre von uns entfernt umkreist ein seltsamer Planet seinen Stern: HIP 41378f, so seine Katalogbezeichnung, hat eine extrem geringe mittlere Dichte, die im Bereich irdischer Schaumstoffe liegt. Auf der Suche nach einer Ursache für diese extrem geringe Dichte haben Astronomen nun mit dem Weltraumteleskop Hubble einen Blick in die Atmosphäre des Planeten geworfen. Bislang allerdings ohne Erfolg, denn die Daten ließen immer noch höchst unterschiedliche Erklärungen für die Eigenschaften des Planeten zu, so die Wissenschaftler im Fachblatt „Astrophysical Journal Letters“.
Der bereits im Jahr 2016 entdeckte Planet HIP 41378f besitzt etwa die zwölffache Masse der Erde, hat dabei aber mehr als den neunfachen Durchmesser. Aus diesen Daten ergibt sich eine mittlere Dichte von 0,09 Gramm pro Kubikzentimeter – die der Erde beträgt im Vergleich 5,5 Gramm pro Kubikzentimeter. Eigentlich können Astronomen aus der mittleren Dichte eines Planeten folgern, ob es sich um einen Gesteinsplaneten wie unsere Erde oder eher um einen Gasplaneten wie Jupiter mit 1,33 Gramm pro Kubikzentimeter handelt. Doch die extrem geringe Dichte von HIP 41378f macht die Himmelsforscher bislang ratlos.
Es wurden bereits verschiedene Modelle zur Erklärung entwickelt, wobei zwei derzeit von Astronomen favorisiert werden. So könnte es sich einerseits um einen „extrem aufgeblähten“ Planeten handeln, der im Verhältnis zu seiner Masse wesentlich mehr Gas enthält als bislang angenommen. Andererseits wurden die Daten – da der Planet mit der Transit-Methode entdeckt wurde – möglicherweise falsch interpretiert. Mit der Transit-Methode wird die Abdunklung von Sternen untersucht, die auftritt, wenn ein Planet von der Erde aus gesehen vor seinem Stern vorbeizieht. Aus der Stärke der Verdunklung lässt sich dann auf die Größe des Planeten schließen. Demnach könnte es sich auch um einen kleineren Planeten mit einem Ringsystem handeln, das nur einen größeren Planeten vortäuscht.
Munazza Alam vom Harvard Smithsonian Center for Astrophysics in Cambridge und ihre Kollegen wollten dieses Rätsel nun mit dem Weltraumteleskop Hubble lösen. Denn wenn der Planet vor seinem Stern vorüberzieht, geht ein Teil des Sternenlichts durch die Atmosphäre des Planeten hindurch. Die Beobachtung dieses von der Atmosphäre gefilterten Lichts sollte, so die Überlegung der Forscher, Rückschlüsse auf die Zusammensetzung der Atmosphäre und damit auch auf den Aufbau des Planeten liefern. Doch sie wurden enttäuscht: Das mit dem Hubble-Teleskop beobachtete gefilterte Sternenlicht zeigte keinerlei Anzeichen auf Moleküle, die solche Schlussfolgerungen erlaubt hätten. Alam und ihre Kollegen können mithilfe der Daten nur vermuten, dass es sich nicht um eine durchsichtige Atmosphäre handelt, die hauptsächlich aus Wasserstoff und Helium besteht.
Als mögliche Erklärung für den rätselhaften Planten bleiben damit nur die bereits bekannten Modelle: eine extrem ausgedehnte Atmosphäre, die allerdings mit schwereren Elementen angereichert sein müsste, oder ein ausgedehntes Ringsystem. Im letzteren Fall könnte der Planet um 60 Prozent kleiner sein als bislang angenommen, so die Wissenschaftler. Damit hätte er dann eine Dichte von 1,2 Gramm pro Kubikzentimeter und läge im normalen Bereich von Gasplaneten. Zukünftige Beobachtungen mit dem neuen James-Webb-Weltraumteleskop könnten aber womöglich zwischen diesen Modellen entscheiden und damit endlich eine Ursache für die geringe Dichte von HIP 41378f liefern.",Seltsamer Planet bleibt rätselhaft
"Auf dem Zwergplaneten Pluto gibt es bis zu sieben Kilometer hohe Eisvulkane – die vermutlich bis in die jüngste Zeit hinein aktiv waren oder noch immer sind. Zu diesem Schluss kommt ein Forscherteam, das Bilder der Raumsonde „New Horizons“ neu analysiert hat und die Ergebnisse im Fachblatt „Nature Communications“ vorstellt.
Pluto ist mit einem Durchmesser von 2377 Kilometern der größte Himmelskörper am Rand des Sonnensystems. Jenseits des Planeten Neptun umkreist er im Kuipergürtel, in dem sich auch viele weitere überwiegend eisbedeckte Objekte aus der Entstehungszeit unseres Sonnensystems befinden, die Sonne. Im Juli 2015 flog die Raumsonde „New Horizons“ nach über neunjähriger Flugzeit an dem Zwergplaneten vorüber und lieferte erstmals hochaufgelöste Bilder und Daten von Pluto und seinen Monden zur Erde.
Auf den Bildern sind neben alten, kraterübersäten Regionen auch außergewöhnlich junge Regionen auf Pluto – fast ohne Krater – zu sehen. In manchen Regionen wiederum hat sich die Oberfläche durch Erosion, Verdampfung und Ablagerung offenbar stetig erneuert. Besonders fällt dabei die Sputnik Planitia auf, eine 1000 Kilometer große Tiefebene, die vermutlich in der Frühzeit des Zwergplaneten durch den Einschlag eines großen Himmelskörpers entstanden ist. Sie ist von einer kilometerdicken Schicht aus gefrorenem Stickstoff bedeckt und nahezu frei von Kratern.
Für Rätsel sorgten bislang jedoch kuppelförmige Strukturen am südwestlichen Rand der Ebene. Und diese Strukturen untersuchten nun Kelsi Singer vom Southwest Research Institute in den USA und ihre Kollegen. Dabei stellten sie fest: Es handelt sich wohl um Vulkandome, also Gipfel von Vulkanen, an denen sich das vom Vulkan ausgestoßene und erkaltete Material sammelt. Bei den Vulkanen auf Pluto drang jedoch keine heiße Gesteinsschmelze, das Magma, aus dem Inneren des Zwergplaneten an die Oberfläche, sondern ein zähflüssiges Gemisch aus Wasser und Eis. Dieses als Kryovulkanismus bezeichnete Phänomen kennen die Forscher bereits von Monden der Planeten Jupiter, Saturn und Neptun. Doch die Größe und Form der Eisvulkane auf Pluto sei einzigartig, so Singer und ihre Kollegen.
Die Vulkandome auf Pluto sind bis zu sieben Kilometer hoch und haben Durchmesser von 10 bis 150 Kilometern. Teilweise überlagern sich die Dome gegenseitig und bilden so noch größere Strukturen. Insgesamt müssen dort nach den Berechnungen des Teams mehr als 10 000 Kubikkilometer der eisigen Masse aus dem Inneren des Zwergplaneten an die Oberfläche gefördert worden sein. Und das bis in die jüngste Zeit hinein, denn aufgrund ihrer Lage am Rand der Sputnik Planitia sollten einige der Eisvulkane nur wenige hundert Millionen Jahre alt sein.
Die Vulkane konnten jedoch nur aktiv sein, wenn Wärme im Inneren Plutos das Gemisch aus Wasser und Eis nach außen trieb. Das wirft die Frage auf, welche Wärmequelle die eisigen Vulkane antreibt. Eine Antwort darauf haben Singer und ihre Kollegen noch nicht. „Entweder hat der innere Aufbau Plutos und seine Entwicklung eine längere Speicherung von Wärme möglich gemacht“, stellen die Wissenschaftler jedoch fest, „oder mehr Wärme erzeugt, als wir bislang angenommen haben.“",Eisvulkane auf Pluto
"In 15 bis 30 Kilometer Höhe schirmt die Ozonschicht in der Stratosphäre energiereiche ultraviolette Strahlung ab und schützt so Pflanzen, Menschen und Tiere vor Strahlungsschäden wie etwa Hautkrebs. Doch durch industrielle Schadstoffe wie etwa Fluorkohlenwasserstoffe – kurz FCKW – dünnte sich die Ozonschicht in den 1980er-Jahren stark aus. Damit sie sich langsam wieder erholen kann, wurde die Nutzung von FCKW im Jahr 1987 verboten. Allerdings beobachteten Forscher nun, dass große Buschbrände eine vollständige Regeneration der Ozonschicht, die eigentlich für Mitte dieses Jahrhunderts erwartet wurde, verzögern könnten. In der Fachzeitschrift „Science“ präsentieren sie ihre Analyse von ozonschädigenden Substanzen, die nach den ausgedehnten Buschbränden in Australien von April bis Dezember 2020 in die Stratosphäre gelangten.
Für ihre Analyse nutzten Peter Bernath von der Old Dominion University in Norfolk und seine Kollegen die Daten, die vom Atmospheric Chemisty Experiment – ACE gesammelt wurden. Das Instrument befindet sich an Bord des kanadischen Erdbeobachtungssatelliten SCISAT, der seit dem Jahr 2003 in 650 Kilometer Höhe die Erde umkreist. Ausgestattet mit einem sogenannten Infrarotspektrometer lässt sich mit ACE von mehr als 44 verschiedenen Molekülen die Konzentration in der Stratosphäre bestimmen. Die Messungen nach den australischen Buschbränden von April bis Dezember 2020 zeigten nun, dass die Konzentration einiger Moleküle deutlich höher war als in den Jahren zuvor. Signifikante Änderungen machten die Forscher beispielsweise für Formaldehyd und die chlorhaltigen Verbindungen Chlornitrat, Chlormonoxid und hypochlorige Säure aus. Andere Verbindungen – Stickstoffdioxid und Salzsäure – kamen dagegen in geringeren Konzentrationen vor als gewöhnlich.
Die veränderte Zusammensetzung der Moleküle in der Stratosphäre führte, so Bernath und seine Kollegen, über verschiedene chemische Reaktionswege insgesamt zu einem verstärkten Abbau von Ozon. Bereits in den Messdaten kurz nach den Buschbränden wiesen sie in einer Höhe von etwa 20 Kilometern eine im Vergleich zu den Vorjahren verringerte Ozonkonzentration nach. In diesem Rückgang sehen sie eine Bestätigung, dass die Buschbrände tatsächlich zu einem längeren relevanten Ozonverlust geführt haben. Auf dieser Grundlage warnen die Atmosphärenforscher nun davor, dass mit der Erderwärmung häufiger auftretende Buschbrände die Regeneration der Ozonschicht über der südlichen Hemisphäre verzögern könnte.
Dass Wald- und Buschbrände durchaus einen Einfluss auf die chemischen Prozesse in der Stratosphäre haben, ist schon länger bekannt – doch die neue Studie zeigt nun ein unerwartetes Ausmaß. „In der Summe wirken sich diese Prozesse negativ auf die stratosphärische Ozonschicht in mittleren Breiten aus, wobei die zusätzliche Ozonzerstörung im Vergleich zum sogenannten Ozonloch eher moderat ist“, sagt Johannes Laube vom Forschungszentrum Jülich. Der Klimaforscher vermutet, dass die Entstehung eines solchen Ozonlochs in mittleren Breiten eher unwahrscheinlich ist. Dennoch betont er, dass diese Prozesse und ihre Auswirkungen mit zunehmenden Waldbränden genauer untersucht werden sollten.",Buschbrände schädigen Ozonschicht
"Elektrisch aktive Fasern verwandeln Textilien bereits in kleine Kraftwerke, Kühlelemente oder Stromspeicher – zumindest im Labor. Nun funktioniert ein Stoff aus solchen Fasern wie ein Mikrofon. Der Prototyp besteht aus piezoelektrischen Fasern, die sich in Stoffe einweben lassen und dort ähnlich wie ein Mikrofon auf Schallwellen reagieren. Wie die Wissenschaftler in der Zeitschrift „Nature“ berichten, könnten solche akustischen Stoffe zukünftig Hörhilfen unterstützen oder auch für die Echtzeitanalyse des Herzschlags genutzt werden.












                Stoff mit Akustikfasern
            



Yoel Fink vom Massachusetts Institute of Technology in Cambridge und sein Team nutzten piezoelektrische Kunststofffasern, mit denen sich mechanische Bewegungen in elektrische Spannungspulse umwandeln lassen. Doch die kleinen Druckunterschiede, die durch Schallwellen in der Luft entstehen, reichten bislang nicht aus, um in den Fasern einen ausreichenden piezoelektrischen Effekt auszulösen. Deswegen verwendeten die Forscher nicht nur den Piezokunststoff, sondern auch piezoelektrische Nanokristalle aus Bariumtitanat. Zudem fügten sie den flexiblen Fasern in einem speziellen Verfahren filigrane Kupferdrähte als Elektroden hinzu. So entstand ein 70 Meter langer und knapp einen Millimeter dünner Faden, den Fink und seine Kollegen direkt in ein Textilstoff einwebten.
Eine einzige Faser genügte bereits, um ein ein Quadratmeter großes Stoffstück in ein Mikrofon zu verwandeln: Trafen Schallwellen in einem Frequenzbereich zwischen 200 und 1000 Hertz auf den Stoff, entstanden – abhängig von der Lautstärke – kleine Spannungspulse. Beispielsweise erzeugte ein Händeklatschen eine Spannung von knapp einem Volt. Mithilfe eines Verstärkers ließen sich die von den Fasern erzeugten Spannungspulse dann wieder in hörbare Schallwellen umwandeln. Auch wenn der akustische Stoff nicht die hohe Empfindlichkeit eines Mikrofons erreichte, ließ sich mit dem Prototyp trotzdem eine Schallquelle orten. „Der Stoff konnte die Richtung des Schalls in drei Meter Abstand bis auf einen Grad genau erkennen“, sagt die an den Experimenten beteiligte Forscherin Grace Noel.
So könnte ein T-Shirt mit Akustikfasern beispielsweise Menschen mit eingeschränktem Hörvermögen helfen, sich optimal zu einer Schallquelle hin auszurichten. Zusätzlich ließ sich mit dem Akustikstoff – getragen im Brustbereich einer Testperson – der Herzschlag wahrnehmen und ebenfalls in elektrische Signale überführen. In weiteren Versuchen könnte nun die Empfindlichkeit der piezoelektrischen Fasern weiter erhöht werden. Da die Faser tausendfachen Verbiegungen und selbst mehreren Wäschen mühelos standhielt, wäre sie auch für einen Dauereinsatz geeignet. Und ergänzt mit weiteren speziellen Fasern – etwa um Strom zu erzeugen und zu speichern – ließen sich zukünftig womöglich funktionelle Kleidungsstücke für Hörgeschädigte oder zur Kontrolle der Herzfunktion entwickeln.",T-Shirt als Mikrofon
"Um die Entwicklungsgeschichte unserer Milchstraße noch besser zu verstehen, bestimmen Astronomen unter anderem das Alter und die Herkunft vieler Sterne. Nun sind zwei Astronomen die bislang genauesten Datierungen der Geschichte unserer Heimatgalaxie gelungen. Dafür verwendeten sie insgesamt 247 104 spezielle Sterne – Unterriesen genannt – als kosmische Uhren. Die Scheibe der Milchstraße ist demnach bereits vor 13 Milliarden Jahren – nur 800 Millionen Jahre nach dem Urknall – entstanden, berichten die Astronomen im Fachblatt „Nature"".
Unsere Milchstraße ist eine Spiralgalaxie: Die meisten Sterne befinden sich in einer Scheibe sowie einer zentralen Verdickung – Bulge genannt. Eingehüllt ist diese Scheibe in einen inneren und einen äußeren Halo aus Sternen. Die Entstehung der inneren Hülle fällt mit der Verschmelzung der jungen Milchstraße mit der kleineren Satellitengalaxie Gaia-Enceladus zusammen. Dieses Ereignis löste offenbar auch die Entstehung vieler Sterne in der Scheibe aus. Doch die genaue Entwicklungsgeschichte der Milchstraße und ihrer komplexen Struktur nachzuvollziehen, ist für Astronomen keine leichte Aufgabe. Denn dazu müssen sie das Alter und die Herkunft möglichst vieler Sterne bestimmen.












                Aufbau unserer Heimatgalaxie
            



Besonders geeignet für die Altersbestimmung von Sternen sind sogenannte Unterriesen – eine Entwicklungsstufe von Sternen. Zunächst erzeugen Sterne wie unsere Sonne ihre Energie durch die Kernfusion von Wasserstoff zu Helium. Doch wenn der Vorrat an Wasserstoff im Kern verbraucht ist, dehnt sich der Stern zu einem roten Riesen aus. Davor geht der Stern allerdings noch in die kurze Phase des Unterriesen über, während der Energie durch die Kontraktion des Kerns entsteht. Da diese Phase astronomisch gesehen kurz ist – sie dauert wenige Millionen Jahre – lässt sich das Alter eines Sterns in dieser Phase sehr genau bestimmen.
Doch zugleich ist es für Astronomen, aufgrund der kurzen Dauer, sehr schwierig, überhaupt Unterriesen aufzuspüren. Auf der Suche nach diesen speziellen Sternen nutzten Maosheng Xiang und Hans-Walter Rix vom Max-Planck-Institut für Astronomie in Heidelberg deswegen nun zwei große Himmelsdurchmusterungen: Sie analysierten einerseits die Daten des europäischen Satellitenteleskops Gaia, mit dem die Entfernung und die Bewegung von über einer Milliarde Sternen in der Milchstraße gemessen wurden. Andererseits verwendeten sie die Spektren, die das optische Teleskop LAMOST vom Observatorium Xinglong Station in China von mehreren hunderttausend Sternen sammelte.
Durch die Kombination der Daten beider Teleskope identifizierten Xiang und Rix nicht nur insgesamt 247 104 Unterriesen, sondern bestimmten auch ihr genaues Alter und ordneten den Sternen ihren Ursprung zu. Anhand der von LAMOST gemessenen Spektren – also der Verteilung der verschiedenen Wellenlängen im Sternenlicht – ließ sich etwa die chemische Zusammensetzung der Sterne und daraus deren Alter bestimmen. Je mehr schwere Elemente die Atmosphäre eines Sterns enthält, umso später in der galaktischen Geschichte ist er entstanden. Denn die schweren Elemente entstehen erst durch Kernfusion im Inneren von Sternen vorhergehender Generationen.
Insgesamt überdeckt das Alter der untersuchten Sterne einen Bereich von 1,5 bis 13,8 Milliarden Jahren, also nahezu die gesamte Geschichte unserer Milchstraße. Die meisten untersuchten Sterne ließen sich der Entstehungsphase der Scheibe und des inneren Halos der Galaxis zuordnen. „Die Unterriesen sind damit die besten Indikatoren für die galaktische Archäologie, die wir haben"", so Xiang und Rix. Für die Zukunft erhoffen sich die beiden Forscher, weitere Unterriesen zu identifizieren. Denn je mehr dieser Sterne die Astronomen untersuchen, desto detailliertere Aussagen lassen sich über die Entwicklungsgeschichte der Milchstraße treffen.",Blick in die Vergangenheit der Milchstraße
"Astronomen haben eine klare Vorstellung davon, wie die großen Gasplaneten Jupiter und Saturn in unserem Sonnensystem entstanden sind: Zunächst prallen große Gesteinskörper – Planetesimale genannt – zusammen und bilden den Kern des künftigen Planeten. Und dieser zieht dann aufgrund seiner Schwerkraft große Mengen von Gas aus seiner Umgebung an. Allerdings lässt sich mit diesem Modell nicht erklären, wie Riesenplaneten, die sehr weit von ihrem Stern entfernt sind, entstehen. Astronomen konnten nun erstmals den Entstehungsprozess eines solchen Planeten direkt beobachten. Offenbar bildet sich der Planet durch den direkten Kollaps von dichtem, kühlen Gas in der protoplanetaren Scheibe um seinen Stern, wie die Forscher im Fachblatt „Nature Astronomy"" schreiben.
„Bisherige Untersuchungen zur Planetenentstehung nutzten Daten bereits entwickelter Planeten"", erläutern Thayne Currie von der japanischen Sternwarte auf Hawaii und seine Kollegen. „Doch die spätere Position eines Planeten muss nicht mit seinem Entstehungsort übereinstimmen."" Deshalb sei es wichtig, den Entstehungsprozess selbst zu beobachten. „Nur die direkte Abbildung von Protoplaneten, die noch in Scheiben aus Gas und Staub um junge Sterne eingebettet sind, können uns den Schlüssel zum Verständnis der Entstehung von großen Gasplaneten liefern.""
Deshalb richteten Currie und seine Kollegen das Subaru-Teleskop auf Hawaii sowie das Weltraumteleskop Hubble nun auf den 520 Lichtjahre entfernten jungen Stern AB Aurigae, der noch von einer protoplanetaren Scheibe umgeben ist. Mit Erfolg: Die von den Instrumenten gelieferten hochaufgelösten Bilder zeigten zum einen mehrere spiralförmige Strukturen, sowie mehrere Verdichtungen in der Gasscheibe. Bei dem auffälligsten Klumpen handelt es sich, wie Currie und seine Kollegen berichten, um einen Riesenplaneten mit etwa der neunfachen Masse des Jupiters – jedoch mit zehnmal größerem Abstand zu seinem Stern.
Die Teleskopaufnahmen von AB Aurigae zeigten außerdem noch zwei weitere Verdichtungen weiter außen in der Scheibe, im 90-fachen und 120-fachen Abstand vom Jupiter zur Sonne. „Damit haben wir erstmals einen direkten Nachweis dafür, dass solche Riesenplaneten tatsächlich in großer Entfernung von ihrem Stern entstehen"", betonen Currie und seine Kollegen. Doch das widerspricht dem bisherigen Modell zur Entstehung solcher Planeten. Denn so weit von einem jungen Stern entfernt gibt es nicht ausreichend Planetesimale, um einen Planetenkern zu bilden.
Wie also entstehen solche Riesenplaneten in derart großem Abstand von ihrem Stern? Auch diese Frage konnten die Forscher nun beantworten: Die in der Scheibe beobachteten Spiralen deuten darauf hin, dass sich das Gas in der Scheibe zufällig verdichtet und dann aufgrund der eigenen Schwerkraft zu einem Planeten kollabiert. In Computersimulationen dieses Prozesses bilden sich dabei exakt solche spiralförmigen Strukturen, wie Currie und seine Kollegen sie jetzt nachgewiesen haben. Damit liefern die Forscher erstmals einen direkten Nachweis für dieses alternative Szenario einer Planetenentstehung.",Wie Riesenplaneten entstehen
"Reiner Kohlenstoff – ob in Graphit oder Diamanten – reagiert nicht auf Magnetfelder. Nun fanden Wissenschaftler allerdings einen Weg, mikroskopisch kleine Diamanten dennoch in einem Magnetfeld auszurichten. Verantwortlich dafür sind winzige Verunreinigungen, wie die Forscher in der Fachzeitschrift „Physical Review Letters“ berichten. So könnten die kleinen Diamanten in Zukunft möglicherweise als Magnetsensoren in der Medizin oder für bessere Kernspinaufnahmen dienen.
In ihren Experimenten nutzten Gabriel Hétet von der Sorbonne Universität in Paris und seine Kollegen winzige Diamanten, die neben reinen Kohlenstoffatomen auch sogenannte Fehlstellen – also vereinzelte Stickstoffatome sowie Lücken in der sonst regelmäßigen Kristallstruktur – enthielten. Diese künstlich hergestellten Nanodiamanten hielten die Forscher zunächst mit einem elektrischen Feld in der Schwebe und setzten sie dann Laserimpulsen und einem Magnetfeld aus. Dabei zeigte sich, dass sich die verunreinigten Diamanten, anders als Diamanten aus reinem Kohlenstoff, im magnetischen Feld ausrichteten.
Bei Magnetfeldstärken von bis zu 100 Millitesla – das entspricht etwa dem 2500-Fachen des Erdmagnetfelds – wurden die Nanodiamanten etwas angezogen und richteten sich grob an den Magnetfeldlinien aus. Ab 103 Millitesla jedoch änderten sich die magnetischen Eigenschaften. Nun richteten sich die Nanodiamanten so aus, dass die Fehlstellen exakt den Magnetfeldlinien wie eine Kompassnadel folgten – man spricht von einem diamagnetischen Verhalten.
Laut den Wissenschaftlern liegt die Ursache für dieses Verhalten in den Elektronen der Fehlstelle. Denn sowohl die Laserpulse als auch die Magnetfelder veränderten den Eigendrehimpuls der Elektronen. Dieser sogenannte Spin wirkt wie eine winzige Magnetnadel, die sich daraufhin entlang des Magnetfelds ausrichtete und damit auch die Position des gesamten Nanodiamanten beeinflusste.
Dank dieser kontrollierten Veränderung der Spineigenschaften könnten Nanodiamanten mit Fehlstellen sogar einige Anwendungen nach sich ziehen. Die Forscher schlagen etwa hochempfindliche Magnetsensoren in der Medizintechnik vor. Auch der Kontrast von Kernspinaufnahmen könnte sich mit solchen Nanodiamanten steigern lassen.",Magnetisierte Diamanten
"Wer am Äquator 70 Kilogramm auf die Waage bringt, bekäme am Nordpol 350 Gramm mehr angezeigt. Der Grund dafür: Die Schwerkraft wirkt nicht überall auf dem Planeten gleich stark. Wie diese Variationen im Erdschwerefeld zustande kommen und was sie über unseren Planeten verraten, erklärt Frank Flechtner vom Deutschen Geoforschungszentrum in Potsdam in dieser Folge des Podcasts.












                Frank Flechtner
            



In einer der bekanntesten Anekdoten der Wissenschaftsgeschichte fällt ein Apfel auf das Haupt des Physikers Isaac Newton. Er soll sich daraufhin gefragt haben, warum Gegenstände stets nach unten fallen, und nicht etwa seitwärts oder aufwärts. Seine Überlegungen führten schließlich zum Newtonschen Gravitationsgesetz.Frank Flechtner: „Die Schwerkraft ist demnach proportional zum Produkt von zwei Massen und umgekehrt proportional zum Quadrat ihres Abstands. Sie nimmt also mit wachsendem Abstand der beiden Körper deutlich ab.Dank dieses Zusammenhangs lässt sich die Schwerkraft, die auf einen Apfel oder einen Menschen wirkt, an jedem Punkt der Erde berechnen. Da unser Planet keine perfekte Kugel ist, variiert der Abstand zwischen Erdmittelpunkt und Erdoberfläche allerdings von Ort zu Ort – und so fällt das Ergebnis je nach Standort ein bisschen anders aus. Am Nord- oder Südpol wäre Newton beispielsweise etwas härter vom Apfel getroffen worden als in seinem Garten in England. Mehr dazu in der 332. Folge.",Schwerefeld der Erde
"Theoretische Modelle, mit denen sich physikalische Phänomene beschreiben lassen, werden von Wissenschaftlern immer wieder überprüft. Weicht eine Messung von der Vorhersage ab, müssen die Theorien nachgebessert oder sogar völlig neu aufgestellt werden. Vor allem das Standardmodell der Teilchenphysik, das alle bekannten Bausteine unseres Universums und die zwischen ihnen wirkenden Kräfte beschreibt, steht auf dem Prüfstand. Nun stellt eine neue hochpräzise Messung der Masse des sogenannten W-Bosons das Standardmodell erneut auf die Probe. Das Elementarteilchen ist offenbar etwas schwerer als Forscher eigentlich erwartet haben. Wie sie nun in der Fachzeitschrift „Science"" berichten, könnte das Ergebnis sogar ein Hinweis auf bisher unbekannte und unentdeckte Teilchen sein.
In den 1960er-Jahren wurden W-Bosonen erstmals theoretisch vorhergesagt und bereits vor knapp vierzig Jahren am Forschungszentrum CERN experimentell nachgewiesen. Seither werden die Eigenschaften des Elementarteilchens, das die schwache Wechselwirkung vermittelt, an Teilchenbeschleunigern immer wieder untersucht. Riesige Datenmengen aus Teilchenkollisionen am Forschungszentrum Fermilab nahe Chicago legen nun die Basis für eine neue Massenbestimmung des W-Bosons. Dafür nutzten die Forscher der CDF-Kollaboration Daten, die zwischen den Jahren 1985 und 2011 mit dem Collider Detector am Tevatron-Beschleuniger gesammelt wurden. In diesem Teilchenbeschleuniger prallten Protonen und Antiprotonen aufeinander, wodurch unter anderem W-Bosonen entstanden.
Insgesamt wurden 4,2 Millionen Ereignisse, an denen das Elementarteilchen beteiligt war, in dem Zeitraum detektiert. Aus der komplexen Auswertung dieser Ereignisse, an der 400 Wissenschaftler beteiligt waren, ergab sich nun ein überraschendes Ergebnis für die Masse des W-Bosons: Es ist schwerer als vom Standardmodell der Teilchenphysik vorhergesagt. „Dieses Ergebnis könnte ein Hinweis auf neue Wechselwirkungen oder neue Teilchen sein"", kommentieren Claudia Campagnari von der University of California in Santa Barbara und Martijn Muldern vom Forschungszentrum CERN die Ergebnisse. Denn das Besondere an der neuen Massenbestimmung ist vor allem die hohe Genauigkeit von etwa einem hundertstel Prozent – keine Messung zuvor war so präzise.
„Dazu haben wir sowohl unser Wissen über den Detektor als auch theoretische wie experimentelle Erkenntnisse zur Wechselwirkung von W-Bosonen mit anderen Teilchen berücksichtigt"", sagt der beteiligte Physiker Ashutosh Kotwal von der Duke University. Doch trotz der hohen Genauigkeit muss die gemessene Abweichung vom Standardmodell zunächst noch von anderen Teilchenexperimenten bestätigt werden. „Wenn der Unterschied zwischen erwartetem und gemessenem Wert etwa durch neue Teilchen verursacht wird, dann besteht eine gute Chance, diese durch zukünftige Experimente zu entdecken"", sagt David Toback von der Texas A&M University. Die leistungsfähigeren Teilchenbeschleuniger, die derzeit etwa in Europa, Japan und China geplant werden, könnten das in Zukunft ermöglichen.",Standardmodell auf dem Prüfstand
"Die meisten Flugdrohnen haben vier Propeller und lassen sich so mühelos in alle Richtungen steuern. Für winzige, nur wenige Mikrometer große Objekte sind aber selbst die kleinsten Versionen dieser Motoren noch viel zu groß. Eine Arbeitsgruppe fand mit einem Lichtantrieb nun eine mögliche Alternative. In der Fachzeitschrift „Nature Nanotechnology“ berichten die Wissenschaftler, wie sie winzige Mikrodrohnen entwickelten, die sich mit Licht durch eine Flüssigkeit steuern ließen.
Die winzigen Drohnen stellte die Forschungsgruppe um Bert Hecht von der Universität Würzburg aus dünnen, nur wenige Mikrometer großen Kunststoffscheiben her. Diese Scheiben versahen sie mit hauchdünnen, maßgeschneiderten Goldflocken. Diese Goldstrukturen dienten als einzelne Motoren, die sich durch Lichtstrahlen ansteuern ließen und die Drohne – analog zu den Propellern einer Flugdrohne – antrieben.












                Modell einer Mikrodrohne
            



Trifft das Licht auf die Goldstrukturen, die als sogenannte optische Antennen dienen, wird es dort absorbiert oder gestreut. Daraufhin strahlen die Antennen wieder Licht ab und erzeugen dadurch einen kleinen Impuls. Dieser genügt, um die Kunststoffscheiben in Bewegung zu versetzen. Als Lichtquellen verwendeten die Forscher infrarot strahlende Laser mit Wellenlängen von 830 und 980 Nanometern. Zudem variierten die Forscher noch eine weitere Eigenschaft der Lichtstrahlen: ihre zirkulare Polarisation.
Für jede der vier möglichen Kombinationen aus Wellenlänge und zirkularer Polarisation der Laserstrahlen reagierten die Goldantennen der Drohnen unterschiedlich. Die Miniaturdrohne ließ sich damit entweder um die eigene Achse drehen oder in eine beliebige Richtung lenken. So gelang es den Forschern, die Drohne in einer wässrigen Flüssigkeit gezielt anzutreiben und zu steuern. Außerdem konnten die Antennen unabhängig von der Orientierung der Drohne Licht empfangen, was sie besonders praktikabel macht.
In weiteren Versuchen wollen die Forscher die Drohnen durch zusätzliche optische Antennen nicht nur in einer Ebene, sondern in allen Richtungen im Raum kontrolliert steuern. Dann wäre die Mikrodrohne ebenso manövrierfähig wie Flugdrohnen mit Propellerantrieb. Zwar ist der Einsatz auf Wasser oder ähnliche Flüssigkeiten beschränkt – doch auch in diesem Medium gibt es zahlreiche mögliche Anwendungen: So könnten sich mit den Mikrodrohnen etwa Oberflächen analysieren oder Nanostrukturen zusammenbauen lassen.",Minidrohnen mit Lichtantrieb
"Das Teleskopnetzwerk MeerKAT in Südafrika ist eines der weltweit größten Observatorien für Radiowellen. Es besteht aus insgesamt 64 Antennen, die Radiostrahlung aus dem Weltall empfangen und sich zu einem virtuellen Teleskop zusammenschalten lassen. So untersuchen Forscher zahlreiche Himmelsobjekte wie etwa schnell rotierende Neutronensterne oder auch ganze Galaxien. Wie das Radioteleskop genau funktioniert und wie MeerKAT nun ausgebaut werden soll, um in Zukunft noch genauere Himmelsbeobachtungen zu ermöglichen, berichtet Dominik Schwarz von der Universität Bielefeld im Interview mit Welt der Physik.
Welt der Physik: Wozu dient das Teleskop MeerKAT?












                Dominik Schwarz
            



Dominik Schwarz: MeerKAT ist ein Radioteleskop, das bislang aus 64 großen Parabolantennen von je 13,5 Metern Durchmesser besteht. Jedes dieser Teleskope besitzt im Augenblick zwei Empfänger, also Empfangseinheiten für unterschiedliche Wellenlängen im mittleren Radiobereich. In Kürze geht für jede Antenne ein dritter Empfänger für höhere Frequenzen in Betrieb. Die Besonderheit von MeerKAT ist, dass sich alle 64 Antennen gleichzeitig zusammenschalten lassen. Dadurch funktionieren sie wie eine riesige gemeinsame Antenne von rund acht Kilometern Durchmesser. Somit ist MeerKAT das weltweit empfindlichste Radioteleskop für den mittleren Radiowellenlängenbereich und bietet herausragende technische Möglichkeiten, um Astrophysik zu betreiben.
Was lässt sich mit dem Radioteleskop beobachten?
Wir machen damit vor allem große Kartierungsprogramme. Dazu gehört die Analyse von neutralem Wasserstoffgas im Universum, um kosmologische Fragen wie die Entstehung und Verteilung von Galaxien besser zu verstehen. Denn Galaxien haben sich ursprünglich aus Wasserstoffgas gebildet. Wir untersuchen auch Radioemissionen in Galaxien, die entstehen, wenn ein aktives supermassereiches Schwarzes Loch im Zentrum das heiße Gas in einer Galaxie zum Leuchten im Radiobereich angeregt hat. Dank der Empfindlichkeit von MeerKAT können wir außerdem Radiostrahlung von Galaxien mit intensiver Sternentstehung, aber ohne ein Schwarzes Loch, beobachten. Denn hier ist die Strahlung der jungen, intensiv leuchtenden Sterne stark genug, um im galaktischen Gas ein nachweisbares Signal zu erzeugen.
Lassen sich auch einzelne kleinere Himmelskörper mit MeerKAT beobachten?
Zu den Klassikern der Radioastronomie gehören Pulsare – das sind schnell rotierende Neutronensterne, die Radiostrahlung aussenden. Von der Südhalbkugel, auf der sich das Teleskop befindet, haben wir Blick auf unser galaktisches Zentrum. Interessant wäre es, einen Pulsar zu finden, der nahe am Schwarzen Loch im Zentrum unserer Milchstraße vorbeiläuft. Damit ließe sich die Relativitätstheorie extrem genau überprüfen. Wir suchen aber auch nach sogenannten Fast Radio Bursts, also Schnelle Radioblitze, deren Ursache noch immer ungeklärt ist. Manche wiederholen sich, andere nicht. Mit etwas Glück können wir zur Lösung dieses Rätsels mit MeerKAT beitragen.












                Galaxie PKS 2014-55
            



Bei solchen Beobachtungen fallen sicher große Datenmengen an. Wo werden diese Daten verarbeitet?
Hochempfindliche Radioteleskope baut man möglichst weit weg von der Zivilisation, um ein möglichst geringes Hintergrundrauschen zu haben. Zu den störenden Einflüssen gehören nicht nur Rundfunk und Mobilfunkmasten, sondern auch Rechenzentren, in denen die Beobachtungsdaten verarbeitet werden. Große Radioteleskope mit vielen Antennen wie MeerKAT erzeugen allerdings eine so riesige Menge an Daten, dass wir diese nicht alle über das Internet von Südafrika nach Europa schicken können. Jede Sekunde erhalten wir Daten in der Größenordnung von Terabyte, das entspricht rund 100 DVDs pro Sekunde. Wir benötigen also ein Rechenzentrum vor Ort, um das Datenvolumen mit modernsten Methoden zu reduzieren. Dieses befindet sich hinter einem Hügel, in einem speziell abgeschirmten, unterirdischen Gebäude, um möglichst jede Störstrahlung zu vermeiden.
Das Max-Planck-Institut für Radioastronomie baut nun zwanzig neue Teleskopschüsseln für MeerKAT. Wozu dienen die neuen Schüsseln?
Diese Schüsseln sind nochmals etwas größer als diejenigen, die bisher bei MeerKAT zum Einsatz gekommen sind. Sie werden rund 15 Meter Durchmesser haben und fünf statt drei Empfänger. Dank der neuen Schüsseln steigt auch die Leistungsfähigkeit von MeerKAT noch einmal deutlich: Wir werden dann ein virtuelles Radioteleskop von 17 Kilometern Durchmessern haben und damit nochmals eine deutlich höhere Winkelauflösung, Empfindlichkeit und Bildqualität. Außerdem werden solche Radioteleskope – neben anderen – auch beim kommenden internationalen Großprojekt Square Kilometer Array, kurz SKA, zum Einsatz kommen.",„Ein virtuelles Radioteleskop“
"Bei der Suche nach Planeten außerhalb unseres Sonnensystems ist ein Forscherteam nun zufällig auf eine neue Art von Sternexplosionen gestoßen und taufte sie „Mikronova“. Von dem Namen sollte man sich allerdings nicht täuschen lassen. Denn auch wenn Mikronovae nur etwa ein Tausendstel der Energie wie gewöhnliche Sternexplosionen ausstoßen, sind sie nach irdischen Maßstäben dennoch gewaltig: Die dabei verbrannte Menge an Wasserstoff entspricht etwa der 40 000-fachen Masse des Mount Everest. Wie die Forscher im Fachblatt „Nature“ berichten, sind thermonukleare Explosionen an den Polen von Weißen Zwergen die Ursache der Mikronovae.
Gewöhnliche Sternexplosionen – auch Novae genannt – kennen Astronomen bereits seit Jahrhunderten: Manche Sterne leuchten immer wieder für mehrere Wochen hell auf. Dabei handelt es sich um Weiße Zwerge, also die Überreste von einst sonnenähnlichen Sternen, die mit einem zweiten Stern ein Doppelsternsystem bilden. Während sich die beiden Sterne umkreisen, entreißt der Weiße Zwerg seinem größeren Partnerstern Materie – vor allem Wasserstoffgas. Hat sich genug Wasserstoffgas auf der Oberfläche des Weißen Zwergs angesammelt, fusioniert der Wasserstoff zu Helium. Diese thermonukleare Reaktion breitet sich dann explosionsartig über die gesamte Oberfläche des Weißen Zwergs aus und der Stern leuchtet hell auf.
Doch die von Simone Scaringi von der Durham University in Großbritannien und seinen Kollegen entdeckten Explosionen lassen sich nicht mit diesem Modell erklären. Auf der Suche nach Planeten außerhalb unseres Sonnensystems stießen die Forscher auf Sterne, deren Helligkeit ähnlich wie bei gewöhnlichen Novae ansteigt – allerdings nicht so stark und lediglich für mehrere Stunden. „Diese Ereignisse stellen unser Verständnis davon infrage, wie thermonukleare Explosionen auf Sternen ablaufen“, stellt Scaringi fest. „Bisher dachten wir, wir wüssten das, aber diese Entdeckung zeigt einen völlig neuen Mechanismus auf.“
Insgesamt identifizierten die Forscher drei derartige Ereignisse in den Daten des Weltraumteleskops TESS der NASA. Zwei der betroffenen Sterne waren bereits als Weiße Zwerge bekannt. Und Beobachtungen mit dem Very Large Telescope der Europäischen Südsternwarte ESO in Chile zeigten, dass es sich auch bei dem dritten Objekt um einen Weißen Zwerg handelt. Daher vermuteten die Astronomen, dass das beobachtete Phänomen den helleren und längeren Novaexplosionen ähnelt.
Wie die Forscher feststellten, finden die Mikronovae auf Weißen Zwergen mit starken Magnetfeldern statt. „Diese Beobachtung war entscheidend für die Interpretation und die Entdeckung der Mikronovae“, so Scaringi. Denn offenbar lenken diese Magnetfelder die dem Partnerstern entrissene Materie bevorzugt zu den magnetischen Polen des Weißen Zwergs und halten sie dort fest. „Dadurch findet die Kernfusion dann nur an diesen Polen statt. Wir haben also das erste Mal gesehen, dass die Wasserstofffusion auch lokal begrenzt stattfinden kann“, erläutert Paul Groote von der Radboud Universität in den Niederlanden. Das Team will nun gezielt nach weiteren Beispielen für diese neue Art von Sternexplosionen suchen.",Eine neue Art von Sternexplosionen
"Nicht nur Zugvögel, sondern auch Schildkröten oder Lachse finden über Tausende von Kilometern sicher zu ihrem Zielort. Diese Fähigkeit verdanken die Tiere unter anderem ihrem Magnetsinn, mit dem sie sich am irdischen Magnetfeld orientieren können. Eine interdisziplinäre Kollaboration aus Biologen und Physikern hat nun gezeigt, dass ein Mineral in den Zellen der Tiere dafür verantwortlich ist. Wie die Forscher das herausgefunden haben und wie sich der Orientierungssinn der Tiere unterscheidet, berichtet Uwe Hartmann von der Universität des Saarlandes im Interview mit Welt der Physik.
Welt der Physik: Wie ist es möglich, dass sich Tiere am Magnetfeld der Erde orientieren?












                Uwe Hartmann
            



Uwe Hartmann: Es wurde schon lange gemutmaßt, dass magnetische Partikel in den Zellen der Tiere für den Orientierungssinn verantwortlich sind. Vor allem das Mineral Magnetit gilt schon seit Jahrzehnten als interessanter Kandidat, der den Magnetsinn der Tiere ermöglicht. Nanokristalle aus Magnetit finden sich in unterschiedlichsten Lebensformen, von einfachen Bakterien über Insekten und Fische bis hin zu Säugetieren. Wie genau dieses Mineral innerhalb von Sinneszellen schließlich zu einem Nervenimpuls führt, der dem Tier mitteilt: „Hier ist Norden“, ist allerdings noch nicht geklärt. Dazu wird derzeit intensiv geforscht. Meine Kollegen und ich haben aber jetzt gezeigt, dass in der Tat Magnetit die entscheidende Rolle spielt.
Wie haben Sie das herausgefunden?
Wir haben mit einem interdisziplinären Team gearbeitet, dem ebenso Forscher aus der Sinnesphysiologie, der Genetik und der Evolutionsbiologie angehören wie Geo- und Nanophysiker. Schon seit Längerem erforschen wir den Magnetsinn von Lachsen, der im Riechorgan der Tiere lokalisiert ist. Es ist uns gelungen, zunächst diejenigen Zellen im Gewebe mit besonders hohem Anteil an Magnetit von den anderen zu separieren. Die Magnetitkristalle in diesen Zellen haben wir dann gezielt mit hochempfindlichen physikalischen Verfahren analysiert – unter anderem mithilfe von ferromagnetischer Resonanz, Rasterkraft- und Magnetkraftmikroskopie.
Was hat diese Analyse ergeben?
Wir haben herausgefunden, dass Magnetit in Form von Nanokristallen vorliegt, die bei Lachsen rund 30 Nanometer groß sind. Bei anderen Tierarten können sie auch etwas kleiner sein und bei manchen Bakterien sind sie nur rund zehn Nanometer groß. Unsere Analyse hat gezeigt, dass die Kristalle bei Lachsen interessanterweise in anderer Form vorliegen als etwa bei sogenannten magnetotaktischen Bakterien. Bei Letzteren sind sie entlang einer Kette angeordnet und ermöglichen es den Bakterien dadurch, sich etwa in trübem Wasser zu orientieren und Schutz im Schlamm zu suchen, wenn sie aufgewirbelt werden. Dagegen sind die Magnetitkristalle bei Lachsen in weintraubenförmigen Clustern angeordnet. Warum das so ist, wissen wir noch nicht. Es handelt sich auch nicht um gewöhnliches Magnetit, zu dessen Erzeugung hohe Temperaturen von einigen hundert Grad Celsius notwendig sind. Stattdessen sind diese Partikel mit Proteinen durchsetzt. Diese Kristalle entstehen also durch einen besonderen Typ von Biomineralisation, der bei Umgebungstemperatur abläuft.
Gibt es eine genetische Beziehung zwischen den verschiedenen Spezies, die sich am Magnetfeld der Erde orientieren?
Die genetische Analyse hat überraschende evolutionäre Zusammenhänge zwischen den Organismen aufdecken können. Denn alle Lebewesen, die solche Magnetit-Nanokristalle nutzen – von Archaeen und Bakterien bis hin zu Säugetieren –, besitzen dieselben Gene. Nun zählen Archaeen und Bakterien zu den Prokaryoten, haben also keinen Zellkern. Alle höheren Lebewesen sind allerdings Eukaryoten und haben einen Zellkern. Vermutlich haben frühe Eukaryoten magnetisch empfindliche Prokaryoten oder zumindest deren Gene im Lauf der Evolution in sich aufgenommen. Der Magnetsinn von Lachsen und anderen höheren Tieren hat sich also nicht von selbst genetisch unabhängig entwickelt, sondern die Gene zur Erzeugung von Magnetit-Nanokristallen sind irgendwann vor mehr als einer Milliarde Jahren in Prokaryoten entstanden und wurden dann von Eukaryoten aufgenommen.
Und wie nutzen die Tiere die Magnetitkristalle zur Navigation?
Wie das Ganze sinnesphysiologisch im Innern von Zellen funktioniert, ist noch nicht ganz klar. Denn die Kräfte, die die Magnetit-Nanocluster im Erdmagnetfeld erfahren, sind winzig klein und nur schwer von thermischem Rauschen zu unterscheiden. Eine spannende These ist, dass die magnetischen Nanokristalle Ionenkanäle in der Zellmembran aktivieren können und dadurch Nervensignale entstehen. Man muss dazu aber auch sagen, dass es nicht einfach nur der Magnetsinn ist, der komplexen Tierarten die Orientierung über Tausende von Kilometern erlaubt, so dass etwa Lachse sicher zu ihren Laichgründen am Oberlauf eines Flusses finden. Hier kommt sicher auch eine besondere Gedächtnisfähigkeit ins Spiel, die ebenfalls noch erforscht werden will.",„Magnetit spielt die entscheidende Rolle“
"Bereits vor 3,9 Milliarden Jahren gab es vermutlich einfaches, einzelliges Leben auf der Erde – also sofort als die Erde kühl genug für flüssiges Wasser auf der Oberfläche war. Doch wie konnte Leben so schnell entstehen? Bei der Beantwortung dieser Frage sind Wissenschaftler jetzt einen großen Schritt vorangekommen: Ein Forscherteam hat in drei Meteoriten wichtige Bausteine für die Erbsubstanz DNA nachgewiesen. Vermutlich haben sich die gefundenen Moleküle bereits vor der Entstehung des Sonnensystems im Weltall gebildet, so die Wissenschaftler im Fachblatt „Nature Communications“.
Seit langem ist bekannt, dass viele organische Stoffe, die die Grundlage des Lebens bilden, bereits im Weltall entstehen können. Sogar Aminosäuren und Zuckermoleküle wurden von Forschern in Gaswolken und in zur Erde gefallenen Meteoriten bereits nachgewiesen. Dadurch gewann die Hypothese an Gewicht, dass die schnelle Entstehung des Lebens auf der Erde durch einen Zustrom von Lebensbausteinen aus dem All angeschoben wurde. Doch wie weit ging diese kosmische Unterstützung? Dieser Frage gingen Yasuhiro Oba von der Universität Hokkaido in Japan und seine Kollegen nun nach.
Mithilfe besonders empfindlicher Messinstrumente untersuchten die Forscher drei Meteoriten im Labor und wiesen dabei weitere Moleküle, nämlich eine große Vielfalt von Nukleinbasen nach – darunter Adenin, Thymin, Guanin und Cytosin. Diese vier Stoffe sind die entscheidenden Informationsträger der irdischen DNA. Sie speichern gewissermaßen den Bauplan von Lebewesen und spielen somit eine wichtige Rolle dafür, dass Lebewesen sich fortpflanzen und durch Evolution an ihre Umgebung anpassen können. Die eigentliche Information ist dabei in der Abfolge der Nukleinbasen auf den Strängen der DNA und RNA abgelegt.
„Unsere Analysetechnik ist darauf optimiert, Nukleinbasen in geringster Konzentration bis hinab zu einem Molekül unter einer Billion Molekülen nachzuweisen“, so die Wissenschaftler. Tatsächlich fanden sie die Nukleinbasen in Konzentrationen von bis zu eins unter einer Milliarde. Diese Häufigkeiten decken sich mit den Vorhersagen von Modellen, die die chemische Entwicklung in dichten Gaswolken beschreiben, aus denen Sterne und Planeten entstehen.
Die Forscher folgern daraus, dass sich die Bausteine der DNA bereits in den Gaswolken gebildet haben – noch vor der Entstehung der Sonne und der Erde. Sie sind offenbar stabil genug, um die turbulente Entstehungszeit der Planeten zu überstehen, sich in Staub und Gesteinsbrocken anzureichern und dann durch Meteoriten zur Erde zu gelangen. Im Gegensatz dazu sei die Entstehung solcher Moleküle auf der jungen Erde schwierig. „Wir vermuten daher“, so die Wissenschaftler, „dass aus dem Weltall gelieferte Nukleobasen zur Entstehung der genetischen Eigenschaften des ersten Lebens auf der Erde beitrugen.“",Bausteine des Lebens in Meteoriten entdeckt
"Flöhe zählen zu den besten Sprungkünstlern in der Natur. Bei nur zwei bis drei Millimetern Körperlänge können sie über einen halben Meter weit springen – etwa das Zweihundertfache ihrer Körpergröße. Erstmals kommt nun ein Roboter nahe an diese enorme Leistung heran. Er ist etwa 30 Zentimeter klein und erreichte mit mehr als 30 Metern eine Sprunghöhe von rund dem Hundertfachen seiner eigenen Größe. Damit stellte er einen neuen Rekord auf. Wie die Entwickler des Prototyps in der Fachzeitschrift „Nature“ berichten, könnten solche Sprungroboter in Zukunft bei Expeditionen in zerklüftetem Gelände oder sogar auf Mondmissionen eingesetzt werden.












                Sprungroboter
            



Für ihre Entwicklung hatten Elliot W. Hawkes von der University of California in Santa Barbara und seine Kollegen zunächst natürliche Sprungstrategien von Insekten und anderen Tieren analysiert. Meist basieren sie darauf, dass die Tiere Muskeln schnell anspannen. Aufbauend auf den Erkenntnissen entwickelte das Forscherteam dann einen Sprungroboter mit gebogenen Streben aus etwa 40 Zentimeter langen, dünnen Kohlefasern. Dazwischen spannten sie außerdem Gummibänder, die für zusätzliche Stabilität sorgten.
Aufgehängt an einer Mittelachse ließen sich die Streben wie zwei übereinander gekreuzte Bögen spannen. Dank eines Hebels, der beim Spannen in ein Zahnrad einrastete, ließ sich die Spannung der Bögen nach und nach erhöhen. Hierfür nutzten die Entwickler einen Elektromotor, der – anders als die natürlichen Vorbilder – über Drehbewegungen arbeitet und so stärkere Kräfte ausüben konnte. Es dauerte etwa eine halbe Minute, bis der Elektromotor die geraden Kohlefaserstreben vollständig zu runden Böden gespannt hatte.
Lösten die Forscher den Spannhebel, entspannten sich die Bögen schlagartig. Dabei beschleunigte der nur 30 Gramm schwere Sprungroboter wie ein Pfeil binnen neun Mikrosekunden auf eine Geschwindigkeit von 28 Metern pro Sekunde – umgerechnet rund 100 Kilometer pro Stunde. So erreichte er eine beachtliche Höhe von knapp 33 Metern. Wieder auf die Erde gefallen konnten die Bögen mit dem Elektromotor abermals für den nächsten Sprung gespannt werden.
Auf der Erde könnten sich solche Sprungroboter – ausgestattet mit kleinen Kameras und Sensoren – zusätzlich zu Flugdrohnen für die Erkundung schwer zugänglicher Gegenden eignen. Darüber hinaus ließe sich ein solcher Roboter besonders auf anderen Himmelskörpern einsetzen: So könnte er auf dem Mond sogar bis zu 125 Meter hohe Sprünge vollbringen, da dort die Schwerkraft nur etwa ein Sechstel so groß ist wie auf der Erde. Nach Berechnungen der Forscher könnte der Roboter dort bis zu einem halben Kilometer weit springen. Damit ließen sich bei zukünftigen Expeditionen auf dem Mond deutlich größere Regionen erkunden als heute nur mit rollenden Rovern.",Roboter stellt Rekord im Hochsprung auf
"Eigentlich dient das Weltraumteleskop TESS der NASA der Suche nach Exoplaneten. Doch nun spürten Forscher mithilfe der Mission gleich dreißig Kometen bei einem 63 Lichtjahre entfernten Stern auf. Anhand der Daten ließ sich außerdem die Größenverteilung der Kometen bei Beta Pictoris analysieren – das gelang erstmals für einen anderen Stern. Die zwischen 3 und 14 Kilometer großen Himmelskörper zeigten eine ähnliche Verteilung wie im Sonnensystem und müssten daher auch auf ähnliche Weise entstanden sein, berichten die Wissenschaftler im Fachblatt „Scientific Reports“.
Beta Pictoris ist ein junger Stern, der noch von einer Scheibe aus Gas und Staub umgeben ist, in der möglicherweise immer noch Planeten entstehen. Bereits zwei große Planeten entdeckten Astronomen in dem System und beobachteten auch schon Spuren von Kometen. „Seit dreißig Jahren werden Kometen nachgewiesen, die von der Erde aus gesehen vor dem Stern vorüberziehen“, erläutern Alain Lecavelier des Etangs von der Sorbonne Universität in Paris und seine Kollegen. Die Kometen verraten sich, weil ihr Schweif aus Gas und Staub bei den Transits einen Teil des Sternenlichts verschluckt – und diese Absorption von bestimmten Wellenlängen zeigt sich dann in Form von dunklen Linien im Lichtspektrum von Beta Pictoris.
Das Team um Lecavelier nutzte nun allerdings eine andere Beobachtungsmethode und überwachte 156 Tage lang die Helligkeit des Sterns mit dem Weltraumteleskop TESS: Denn zieht ein extrasolarer Planet von der Erde aus gesehen vor seinem Stern vorüber, schwächt er das Sternenlicht geringfügig ab und verrät sich auf diese Weise. Zu einer solchen Abschwächung – wenn auch geringer – führt auch der Transit eines Kometen mit einem großen Schweif. Mithilfe dieser Transitmethode identifizierten die Forscher insgesamt dreißig Transits von Kometen in den Daten von TESS. Und anhand der gemessenen Stärke der Abschwächung ließ sich zusätzlich auf die Größe der Kometen schließen.
„Sechzehn der Kometen haben einen Durchmesser von drei bis vier Kilometern, aber nur vier sind zwischen sechs und acht Kilometern groß und nur ein einziger ist zwischen acht und zehn Kilometern groß“, berichten die Wissenschaftler. Kleine Kometen sind bei Beta Pictoris also sehr viel häufiger, was der Verteilung in unserem Sonnensystem ähnelt – ein Indiz dafür, dass auch die Entstehung und Entwicklung der Kometen ähnlich verlief. Für die Astronomen ist das eine wichtige Erkenntnis, denn vermutlich brachten Kometen einen großen Teil des Wassers in das innere Sonnensystem und damit auch zur Erde. Solche Prozesse laufen womöglich also auch in anderen Planetensystemen, wie um Beta Pictorius, ganz ähnlich ab.",Dreißig Kometen bei einem Stern beobachtet
"In rotierenden Asteroiden wirken unterschiedliche Kräfte, die ihr Verhalten und ihre Form bestimmen. Während die Schwerkraft die Bestandteile der Asteroiden zusammenhält, treibt sie die Zentrifugalkraft auseinander. Das Wechselspiel zwischen diesen Kräften simulierte ein Forschungsteam nun in einem neuartigen Experiment mit schwebenden Plastikkügelchen. Wie sie in der Fachzeitschrift „Physical Review X“ berichten, konnten sie die Kügelchen mit Ultraschallwellen sowohl zum Schweben bringen, als auch ganze Ansammlungen der Kügelchen in Drehung versetzen.
Für ihr Experiment baute Melody Lim von der University of Chicago zusammen mit ihren Kollegen einen kleinen Kasten aus durchsichtigem Kunststoff. In diesem Kasten erzeugten sie mithilfe eines Generators Ultraschallwellen. Diese breiteten sich im Kasten aus und bildeten eine sogenannte stehende Ultraschallwelle aus, deren Auslenkung an bestimmten Stellen – den Amplitudenminima – verschwindet. Anschließend brachten die Forscher leichte, etwa 0,2 Millimeter kleine Plastikkügelchen in der Ultraschallwelle zum Schweben – ein Effekt, der als akustische Levitation bezeichnet wird. Da der Druck des Ultraschalls an der Stelle des Amplitudenminimums am geringsten ist, ordneten sich die Kügelchen exakt dort in einer Ebene an.
In den Experimenten von Lim und ihren Kollegen traten mehrere Effekte auf. Zum einen wurden die Ultraschallwellen an den Kügelchen reflektiert. Dadurch bildeten sich zwischen ihnen anziehende Kräfte aus, so dass sie sich in einem kreisrunden Areal zusammenlagerten. Mit kleinen Änderungen des Ultraschalls ließ sich dieses Areal außerdem in Rotation versetzen. Mit zunehmender Drehgeschwindigkeit nahm auch die wirkende Zentrifugalkraft zu. Somit hatten die Forscher das Zusammenspiel der Kräfte in rotierenden Asteroiden nachgestellt.
Nun beobachteten die Forscher, wie sich die Kügelchen in unterschiedlichen Szenarien verhielten. Das Ergebnis: Mit zunehmender Zentrifugalkraft verformte sich das zuvor kreisrunde Kugelareal erst zu einer Ellipse. Bei noch schnellerer Drehung wurde die Ellipse immer schmaler, bis sie sich sogar in zwei getrennte Kugelareale aufspaltete. Diese Versuche wiederholten die Forscher mit bis zu zweihundert Plastikkügelchen. Dabei beobachteten sie ein bisher nicht vollständig verstandenes Verhalten: Die getrennten Kugelareale konnten sich nach einiger Zeit wieder vereinigen. Je mehr Kügelchen in Rotation versetzt wurden, desto häufiger trat das Phänomen auf.
Auch Asteroiden zeigen ein ähnliches Verhalten: Sind die Zentrifugalkräfte groß genug, spalten sie sich in kleinere Fragmente auf. Doch je größer diese sind, desto stärker ist auch die anziehende Schwerkraft zwischen ihnen. Daher vereinigen sich einige von ihnen wieder. Mit ihrer Arbeit zeigen die Forscher das Potential von Levitationsexperimenten, um das Verhalten rotierender Asteroiden im Detail nachzustellen und zu untersuchen. In weiteren Versuchen wollen die Wissenschaftler nun Kügelchen verschiedener Größe, Formen und Materialien zum Schweben bringen und den Einfluss dieser Parameter exakt analysieren.",Asteroiden im Labor
"Jedes Lebewesen ist aus Zellen aufgebaut. Zwar ist das Wissen über diese mikroskopisch kleinen Bausteine des Lebens in den vergangenen Jahrhunderten stetig gewachsen, doch immer noch gibt es viele offene Fragen. Um die komplexen Vorgänge in Zellen zu verstehen, nähern sich Forscher nicht nur von biologischer, sondern auch von physikalischer Seite. Wie viel Physik in Zellen steckt, erklärt Ulrich Schwarz von der Universität Heidelberg in dieser Folge des Podcasts.












                Ulrich Schwarz
            



Im 17. Jahrhundert untersuchte der Physiker Robert Hooke verschiedene Dinge unter dem Mikroskop, darunter auch Kork. Er stellte fest, dass das Material aus vielen kleinen Kämmerchen bestand. Und die erinnerten ihn an die Zimmer – die Zellen – in einem Kloster. So prägte er den Begriff „Zelle“.Ulrich Schwarz: „Eine Zelle ist die kleinste Einheit des Lebens, in gewisser Weise das Elementarteilchen der Biologie. Der menschliche Körper besteht aus der unglaublichen Zahl von über 1013 verschiedenen Zellen, die in mehr als 200 verschiedene Zelltypen eingeteilt werden.“Die verschiedenen Zelltypen haben nicht nur unterschiedliche Funktionen, sie können auch sehr unterschiedlich aussehen: Rote Blutzellen – oder Blutkörperchen – sind beispielsweise scheibenförmig, Nervenzellen länglich und verästelt. Mehr dazu in der 333. Folge.",Physik der Zelle
"Schwärme mit tausenden Vögeln zeichnen eindrucksvolle Strukturen am Himmel. Doch obwohl das koordinierte Schwarmverhalten auf einfachen Prinzipien basiert, lässt es sich mathematisch nur schwer beschreiben. Ein Forschungsteam entwickelte daher ein neues Modell, um die Dynamik von kleinen und großen Schwärmen noch besser zu erfassen. Ihre Ergebnisse könnten in Zukunft sogar bei der Programmierung von künstlichen Roboterschwärmen genutzt werden, wie die Forscher in der Fachzeitschrift „Nature Communications“ berichten.
In einem Vogelschwarm reagiert jeder Vogel grundsätzlich auf das Flugverhalten seiner nächsten Nachbarn. An dieses passt er sich an und es entwickelt sich eine hervorragend koordinierte Schwarmdynamik. Auf diesem Prinzip basieren auch die Modelle, mit denen Wissenschaftler das Verhalten der Vögel simulieren. Antonio Culla von der Universität La Sapienza in Rom und seine Kollegen untersuchten nun eine weitere Eigenschaft der Vögel: Denn jeder Vogel in einem Schwarm variiert neben seiner Flugrichtung auch seine Geschwindigkeit. Benachbarte Vögel erkennen solche kleinen Änderungen leicht und reagieren entsprechend. „Große Tempoänderungen sind jedoch für einzelne Vögel sehr schwierig“, sagt Culla. Reagierten daher alle Vögel auch auf größere Tempoänderungen, würde ein Schwarm schnell seine koordinierte Dynamik verlieren.
Diese intuitive, auf Beobachtungen basierende Erkenntnis übertrugen die Forscher in ihr mathematisches Modell. Dafür nutzten sie ein Konzept, das sie als „marginales“ Tempolimit bezeichneten. Dabei werden Tempoänderungen nicht gleichwertig von den jeweiligen Nachbarn übernommen: Auf kleine Schwankungen der Geschwindigkeit reagieren die Vögel im Modell immer, größere Änderungen werden dagegen fast vollständig ignoriert.
Danach überprüften die Wissenschaftler, ob ihr mathematisches Modell natürliche Vogelschwärme tatsächlich korrekt beschreibt. Dazu filmten sie Schwärme mit hunderten Staren und analysierten danach die einzelnen Flugbahnen und Geschwindigkeiten der Vögel. Parallel berechneten sie die Schwarmdynamik mit ihren Modellen. Ohne Beachtung eines Tempolimits ergab sich nur wenig Übereinstimmung mit den Beobachtungen. Mit Tempolimit jedoch ließ sich die Dynamik mit großer Übereinstimmung modellieren.
Mit diesem Ansatz haben Culla und seine Kollegen ein Modell entwickelt, um das Schwarmverhalten von Vögeln und anderen schwarmbildenden Lebewesen wie Fischen mathematisch besser zu beschreiben. „Aber es könnte auch nützlich für die Kontrolle von Robotern und Schwärmen von Flugdrohnen sein“, sagt Culla. So könnten sich auch künstliche Schwärme dank eines eingebauten Tempolimits in Zukunft möglicherweise besser koordinieren.",Tempolimit im Vogelschwarm
"Für die kommenden Jahre sind derzeit zahlreiche Missionen zum Mond geplant – doch ohne eine zuverlässige Energieversorgung werden sie nicht auskommen. Daher schlagen Forscher nun ein Konzept vor, wie sich Treibstoff auf dem Mond selbst erzeugen ließe. In der Fachzeitschrift „Joule“ berichten sie über erste Ansätze, wie sich aus Mondgestein effiziente Katalysatoren für die Erzeugung von Wasser- und Sauerstoff bis hin zu flüssigen Energieträgern herstellen ließen. Damit bräuchten Mondraketen in Zukunft womöglich weniger Zuladung auf ihrer Reise.
„Auf dem Mond vorkommende Ressourcen zu nutzen, um die Nutzlast der Raketen zu minimieren“, lautet das Ziel des Materialforschers Yingfang Yao von der Nanjing Universität. Um diesem Ziel einen Schritt näher zu kommen, analysierte er mit seinen Kollegen Mondgestein, das im Rahmen der Mondmission Chang’e 5 Ende 2020 zur Erde transportiert worden war. Die nordöstlich des Vulkanmassivs Mons Rümker im Oceanus Procellarum gesammelten Proben waren reich an Eisen und Titan und eignen sich damit als Katalysatoren für die Treibstoffgewinnung. Diese Proben untersuchten die Forscher anschließend in Laborexperimenten auf der Erde.












                Mondgestein
            



Dabei stellte sich heraus, dass die Katalysatoren zwar noch nicht sehr effizient, aber zumindest geeignet waren, um Wasser in Sauerstoff und Wasserstoff aufzuspalten. In einem darauffolgenden Prozess ließ sich aus dem Wasserstoff und Kohlendioxid das Gas Methan herstellen. Damit wurde die Basis gelegt, um in weiteren Schritten sogar flüssige Treibstoffe produzieren zu können. Als Energiequelle für diese Herstellungsprozesse schlagen die Autoren der Studie vor, Solarstrom zu nutzen, den man mithilfe von Solarzellen auf dem Mond gewinnen könnte. Das für die Reaktionen notwendige Wasser und Kohlendioxid wiederum ließe sich beispielsweise aus der feuchten Atemluft der Mondfahrer erhalten. Auch Wasservorkommen an den Polen oder aus tieferen Mondschichten könnten in diesen Kreislauf eingespeist werden.
Diese Studie ist ein erster Ansatz, um auf dem Mond vorkommende Ressourcen für eine Versorgung mit Sauerstoff und Energie zu nutzen. In weiteren Versuchen wollen Yao und seine Kollegen die Katalysatoren aus Mondgestein noch effizienter machen. Allerdings wäre es auch in Zukunft noch notwendig, Solarzellen und benötigte Geräte zum Mond zu transportieren. Doch mit dieser Strategie scheint es nicht ausgeschlossen, die Menge an zu transportierendem Material durch Raketen zu reduzieren. Ganze Kraftwerke und Fabriken für die Versorgung von Menschen auf dem Mond bleiben aus heutiger Sicht aber noch eine Vision für eine fernere Zukunft.",Treibstoff aus Mondgestein
"Ihr Name ist Programm: Die Gravitation von Schwarzen Löchern ist derart stark, dass alles wie in ein gewaltiges Loch hineinfällt. Doch weder Materie noch Licht kann jemals wieder aus diesen Objekten entkommen – sie sind daher schwarz.
Startet eine Rakete von einem Himmelskörper aus ins Weltall, so muss sie dessen Anziehungs- oder Gravitationskraft überwinden. Dazu ist eine bestimmte Mindestgeschwindigkeit nötig. Je massereicher ein Himmelskörper, desto größer ist seine Gravitationskraft und umso höher auch die Entweich- oder Fluchtgeschwindigkeit. Um von der Erdoberfläche abzuheben, benötigt ein Raumfahrzeug daher beispielsweise deutlich mehr Schub als bei einem Start vom Mond: Die Fluchtgeschwindigkeit der Erde beträgt 11,2 Kilometer pro Sekunde, beim Mond sind es nur 2,3 Kilometer pro Sekunde.
Die Fluchtgeschwindigkeit lässt sich für beliebige Himmelskörper berechnen – für die Sonne ebenso wie für rein hypothetische Objekte mit einer extrem hohen Masse. Theoretisch kann die Fluchtgeschwindigkeit sogar größer sein als die Lichtgeschwindigkeit. Praktisch allerdings nicht. Denn gemäß der Relativitätstheorie von Albert Einstein stellt die Lichtgeschwindigkeit die höchstmögliche Geschwindigkeit im Kosmos dar. Und das bedeutet: Liegt die Fluchtgeschwindigkeit eines Objekts über diesem Limit, kann nichts mehr von seiner Oberfläche entkommen, nicht einmal Licht. Einen solchen Himmelskörper bezeichnet man daher als Schwarzes Loch. Denn es kann zwar etwas hineinfallen, aber weder Materie noch Licht gelangen jemals wieder heraus.
Wie Schwarze Löcher den Raum und die Zeit in ihrer Nähe beeinflussen, lässt sich mithilfe der Allgemeinen Relativitätstheorie beschreiben. Es zeigt sich, dass solche Objekte von einem „Ereignishorizont“ umgeben sind: Ereignisse die innerhalb dieser das Schwarze Loch einhüllenden Grenzfläche stattfinden, sind für Beobachter außerhalb der Grenzfläche nicht sichtbar.













                Schwarzes Loch mit Akkretionsscheibe
            



Gibt es solche seltsamen Objekte wirklich oder handelt es sich um rein theoretische Spekulationen? Da sich Schwarze Löcher nicht direkt beobachten lassen – schließlich senden sie kein Licht aus und reflektieren es auch nicht –, war die Antwort auf diese Frage jahrzehntelang umstritten. Doch inzwischen sind sich Astrophysiker sicher, dass es diese extremen Himmelsobjekte tatsächlich gibt. Denn auch wenn ein Schwarzes Loch selbst unsichtbar ist, seine starke Gravitation kann eindeutige Spuren im Umfeld hinterlassen.
Einen entscheidenden Hinweis liefern Akkretionsscheiben: Materie, die von außen auf ein Schwarzes Loch einströmt, sammelt sich zunächst in einer rotierenden Scheibe. Dort heizt sich die Materie durch innere Reibung stark auf und beginnt zu leuchten. Durch die Analyse der ausgesendeten Strahlung können Astronomen mehr über die Eigenschaften des zentralen Objekts erfahren, etwa über dessen Masse. Nach heutigen Erkenntnissen gibt es drei unterschiedliche Typen von Schwarzen Löchern:
   Stellare Schwarze Löcher entstehen aus Sternen: Haben massereiche Sterne ihren nuklearen Brennstoff verbraucht, explodieren sie als Supernova. Dabei kann der Kernbereich zusammenstürzen und – ausreichend Restmasse vorausgesetzt – zu einem Schwarzen Loch kollabieren. Auch durch die Kollision von zwei Neutronensternen können sich stellare Schwarze Löcher bilden. Einer der besten Kandidaten für ein stellares Schwarzes Loch in der Milchstraße ist V404 Cygni. In diesem 7800 Lichtjahre entfernten Doppelsystem strömen stetig Gas und Staub von einem Stern zu einem Schwarzen Loch mit etwa der neunfachen Sonnenmasse. Die entrissene Materie sammelt sich in einer Akkretionsscheibe und flackert immer wieder verräterisch im sichtbaren Spektrum und im Röntgenbereich auf. Die stärksten Beweise für die Existenz stellarer Schwarzer Löcher liefern inzwischen jedoch Gravitationswellen: Die Detektoren LIGO und VIRGO haben zahlreiche Ereignisse registriert, die sich nur durch die Kollision und Verschmelzung von Schwarzen Löchern mit einigen Dutzend Sonnenmassen erklären lassen.    Supermassereiche Schwarze Löcher mit der millionen- oder gar milliardenfachen Masse unserer Sonne befinden sich vermutlich in den Zentren nahezu aller Galaxien. Auch im Zentrum der Milchstraße gibt es ein Schwarzes Loch mit etwa vier Millionen Sonnenmassen. In diesem Fall lässt sich die Masse anhand der Bewegung von Sternen in der Nähe des Schwarzen Lochs sehr genau bestimmen.  Die Akkretionsscheiben supermassereicher Schwarzer Löcher können heller leuchten als ihre gesamte Heimatgalaxie. Astrophysiker sprechen dann von „aktiven“ Galaxien. Wie supermassereiche Schwarze Löcher entstanden sind, ist bislang nicht klar. Sie müssen sich jedoch im jungen Kosmos überraschend schnell – innerhalb weniger Hundert Millionen Jahre – gebildet haben. Vielleicht entstanden die Schwergewichte unter den Schwarzen Löchern bereits unmittelbar nach dem Urknall und wuchsen dann rasch an. Eine andere These besagt, dass sich im jungen Kosmos zunächst extrem massereiche Sterne formten. Sie vereinten Zehntausende von Sonnenmassen in sich und besaßen dadurch eine vergleichsweise kurze Lebensdauer. Als diese Sterne schließlich in sich zusammenstürzten, gingen aus ihnen Schwarze Löcher mit Tausenden von Sonnenmassen hervor – und bildeten den Ausgangspunkt für die heute beobachteten supermassereichen Exemplare.    Schwarze Löcher mittlerer Masse liegen bezüglich ihrer Masse zwischen den stellaren und den supermassereichen Schwarzen Löchern: Sie bringen es also auf mehrere Hundert und sogar einige Tausend Sonnenmassen. Entstehen könnten diese Objekte beispielsweise in dichten Sternhaufen, wo mehrere Sterne miteinander verschmelzen. Zwar gibt es eine Reihe von Kandidaten für mittelschwere Schwarze Löcher, etwa im Zentrum des Kugelsternhaufens Omega Centauri. Doch bislang steht der definitive Beweis für die Existenz dieses Typs noch aus.",Schwarze Löcher
"In den Zentren der meisten Galaxien befindet sich ein Schwarzes Loch mit der millionen- oder gar milliardenfachen Masse unserer Sonne. Einen direkten Blick in die unmittelbare Nähe dieser supermassereichen Objekte zu werfen, war lange nicht möglich. Erst mit dem Event Horizon Telescope gelang die erste Aufnahme eines supermassereichen Schwarzen Lochs.
Aufgrund der großen Anziehungskraft von Schwarzen Löchern kann ihnen selbst Licht nicht entkommen. Das macht eine Fotografie eigentlich unmöglich. Mit dem Event Horizon Telescope gelang dennoch der erste direkte Blick auf ein Schwarzes Loch: Das am 10. April 2019 veröffentlichte Bild zeigt zwar nicht das Schwarze Loch selbst – sein „Schatten“ ist aber deutlich als dunkle zentrale Region zu erkennen, umgeben von einer hellen ringförmigen Struktur. Das abgelichtete Objekt liegt im Zentrum der rund 55 Millionen Lichtjahre entfernten Galaxie Messier 87 und ist mit 6,5 Milliarden Sonnenmassen mehr als tausendfach schwerer als das Schwarze Loch im Zentrum der Milchstraße.
Trotz der enormen Masse handelt es sich bei Schwarzen Löchern um extrem kompakte Himmelskörper. Für die Aufnahme schalteten die mehr als dreihundert beteiligten Astronomen mehrere Radioteleskope auf der gesamten Welt zusammen, darunter ALMA und APEX in Chile sowie Observatorien in Europa, Hawaii und am Südpol. Dieses als Interferometrie bezeichnete Verfahren ermöglicht eine deutlich höhere Auflösung verglichen mit den Einzelteleskopen. Auf diese Weise lieferte das „virtuelle“ Event Horizon Telescope schließlich das spektakuläre Bild.













                Radioteleskopanlage ALMA
            



Die zugrunde liegenden Daten hat die Kollaboration seither noch eingehender untersucht, etwa indem sie auch die Schwingungsebene der empfangenen elektromagnetischen Wellen berücksichtigten. Ein signifikanter Anteil des Lichts um das supermassereiche Objekt im Zentrum der Galaxie M87 ist demnach polarisiert. Das zeige deutlich, dass der Ring um das Schwarze Loch magnetisiert ist, berichtete das Team im März 2021.
Wenige Monate später folgte eine weitere Aufnahme. Dieses Mal richteten die Astronomen den Fokus auf das Zentrum der 13 Millionen Lichtjahre von uns entfernten Radiogalaxie Centaurus A. Das supermassereiche Schwarze Loch darin ist mit 55 Millionen Sonnenmassen deutlich „leichter“. Die Beobachtungen legen nahe, dass sich Schwarze Löcher über einen weiten Massenbereich ähnlich verhalten.
Im Mai 2022 gelang schließlich auch ein  direkter Blick auf das Schwarze Loch im Zentrum der Milchstraße.  Zwar ist diese Region „nur“ 27 000 Lichtjahre von uns entfernt, doch erwies sich die Auswertung der Beobachtungsdaten als weitaus schwieriger. Denn während die aus dem Zentrum von M87 empfangene Strahlung über Stunden hinweg konstant ist, verändert sie sich im Fall des Milchstraßenzentrums im Verlauf weniger Minuten. Das erforderte völlig neue Methoden für die Analyse der Daten. Wie bei M87 lässt sich auch auf dem ersten Foto vom Schwarzen Loch im Zentrum der Milchstraße ein leuchtender Ring um einen dunklen Kern erkennen. Die Beobachtungen stimmen den Forschern zufolge sehr gut mit den Vorhersagen der Allgemeinen Relativitätstheorie überein.
Für die beteiligten Wissenschaftler ist das Foto des galaktischen Zentrums ein großer Erfolg, gleichwohl aber nur ein erster Schritt: Mit weiteren Teleskopen soll die Auflösung des Event Horizon Telescope künftig noch steigen. Damit ließen sich dann, so die Hoffnung der Astronomen, weitere Erkenntnisse über die physikalischen Vorgänge in der unmittelbaren Umgebung supermassereicher Schwarzer Löcher gewinnen.",Event Horizon Telescope
"Der Large Hadron Collider – kurz LHC – am Forschungszentrum CERN ist der derzeit größte und leistungsfähigste Teilchenbeschleuniger. Mit ihm gewinnen Physiker einzigartige Einblicke in die Welt der kleinsten Teilchen und in die dort wirkenden Kräfte. So gelang im Jahr 2012 mit dem Nachweis des Higgs-Teilchens am LHC ein Durchbruch in der Teilchenphysik. Seitdem entwickeln Forscher den LHC stetig weiter, um die bereits bekannten Bausteine des Universums noch besser zu verstehen und um rätselhaften Phänomenen wie der Dunklen Materie auf die Spur zu kommen. 
Wie ist unsere Welt aufgebaut? Und was hält sie im Innersten zusammen? Diesen und weiteren fundamentalen Fragen gehen Forscher mit dem Teilchenbeschleuniger LHC auf den Grund. Dazu werden Protonen – die positiv geladenen Bausteine der Atomkerne – in dem unterirdischen, rund 27 Kilometer langem Tunnel auf nahezu Lichtgeschwindigkeit beschleunigt. In einem der vier großen Detektoren – ALICE, ATLAS, CMS und LHCb – stoßen die Protonen dann zusammen. Die Energien, die bei solchen Kollisionen frei werden, sind so hoch, dass die Protonen nicht nur zerstört werden. Sondern es entstehen aufgrund der dabei wirkenden Kräfte auch neue Teilchen, deren Spuren sich in den Detektoren nachverfolgen lassen.












                CMS-Detektor
            



Seit etwa fünfzig Jahren existiert eine theoretische Beschreibung aller Bausteine der uns bekannten Materie und der Kräfte, die zwischen ihnen wirken – das Standardmodell der Teilchenphysik. Um diese Theorie zu überprüfen, versuchen Physiker seit Jahrzehnten, die einzelnen Elementarteilchen an Teilchenbeschleunigern nachzuweisen. Auf dieser Suche gelang im Jahr 2012 eine Sensation am LHC: Sowohl im ATLAS- als auch im CMS-Experiment wiesen Physiker erstmals das Higgs-Boson nach. Dieses Elementarteilchen verleiht über den sogenannten Higgs-Mechanismus allen anderen Elementarteilchen ihre Masse. Doch der Nachweis des Higgs-Boson ist nicht genug – nun gilt es, die Eigenschaften des Teilchens ganz genau zu untersuchen. Außerdem sind Physiker weiteren unbekannten Teilchen, etwa Kandidaten für die bislang rätselhafte Dunkle Materie, auf der Spur. Am LHC wollen sie Dunkle Materie unter Laborbedingungen produzieren und studieren.
Dafür werden sowohl der Beschleuniger selbst als auch seine Detektoren ständig weiterentwickelt und in mehrjährigen Umbau- und Wartungsphasen auf den neuesten Stand gebracht. Auch während der letzten Betriebspause zwischen den Jahren 2018 und 2021 fanden daher am LHC keine Kollisionen statt. Doch seit dem Frühjahr 2022 ist der Teilchenbeschleuniger wieder in Betrieb und liefert in seiner dritten Betriebsphase – dem sogenannten Run 3 – mit höherer Leistung und besseren Detektoren nun wieder enorme Datenmengen. Ab dem Jahr 2029 soll der LHC zum „High-Luminosity LHC“ ausgebaut werden – einem Teilchenbeschleuniger mit noch höheren Kollisionsraten. Damit sollen zukünftig etwa 15 Millionen Higgs-Bosonen im Jahr erzeugt und analysiert werden. Und auch andere extrem seltene Prozesse lassen sich dann, so hoffen die Wissenschaftler, in den Kollisionsdaten entdecken.",Large Hadron Collider – LHC
"Löst ein Erdbeben unter dem Meeresboden einen Tsunami aus, gilt es, die Wellen des Bebens in kürzester Zeit zu analysieren, um vor dem heranrollenden Tsunami zu warnen. Nun zeigten Geophysiker, dass sich mit einer neuen Methode die Beben noch schneller und exakter als bisher analysieren lassen. Die Forscher nutzten dafür Schwankungen im Schwerefeld der Erde, die dort nach einem Erdbeben auftreten und sich mit Lichtgeschwindigkeit ausbreiten. Erste Ergebnisse dieser Nachweismethode präsentieren sie in der Fachzeitschrift „Nature“.
Bisher greifen die Warnsysteme für Tsunamis auf schnelle Erdbebenwellen – auch seismische Wellen genannt – zurück. Sie breiten sich im Erdkörper aus und erreichen dort Geschwindigkeiten von 18 000 bis 50 000 Kilometern pro Stunde. Damit sind sie deutlich schneller als die Wellen eines Tsunamis, den das Beben auslöst und der sich mit bis zu 800 Kilometern pro Stunde bewegt. Zwischen dem Eintreffen der seismischen Wellen und dem Tsunami ergibt sich dann ein Zeitfenster von wenigen Minuten für die Warnsysteme.
Auf der Suche nach einem schnelleren Weg, Beben aufzuspüren, untersuchten Andrea Licciardi von der Universität Côte d’Azur in Sophia Antipolis und seine Kollegen nun die vielen Datensätze, die in Japan von zahlreichen Messstationen nach dem Tōhoku-Erdbeben am 11. März 2011 aufgezeichnet worden waren. Darin waren neben den seismischen Signalen auch Schwankungen der Gravitation zu sehen. Ursache für diese Schwankungen sind die Verschiebungen großer Erdmassen, die direkt die lokal wirkende Schwerkraft beeinflussen.
Die Signale dieser Schwankungen – sogenannte elastogravitative Signale – breiten sich mit Lichtgeschwindigkeit, also rund 300 000 Kilometern pro Sekunde, aus und kommen damit noch deutlich schneller als die Erdbebenwellen an den Messstationen an. Allerdings sind sie extrem schwach und werden zudem leicht von störenden Rauschsignalen überdeckt. Aber mit dem Einsatz von Methoden der künstlichen Intelligenz gelang es dem Forscherteam, die Signale aus den Datensätzen herauszufiltern und zu analysieren.
„Mit dieser Methode konnten wir die Stärke des Fukushima Erdbebens in Japan schneller und genauer bestimmen als mit bisherigen Systemen – und das ganz ohne seismische Wellen“, sagt Licciardi. So hatten frühere Analysen der seismischen Wellen bei dem Tsunami im März 2011 die Stärke des Bebens und damit auch die Höhe der zu erwartenden Tsumamiwellen unterschätzt. Erst mit Verzögerung hatten Untersuchungen der seismischen Wellen den tatsächlichen Wert geliefert. Die neue Methode hingegen schätzte die Stärke des damaligen Bebens direkt auf den korrekten Wert der Magnitude von 9,0.
Damit haben die Geophysiker gezeigt, dass Signale, die aus Änderungen der Schwerkraft resultieren, sich tatsächlich zu einer schnelleren und genaueren Bestimmung der Erdbebenstärke eignen. Zudem ließe sich der Zeitvorteil für eine Tsunamiwarnung wegen der lichtschnellen Ausbreitung um bis zu wenige Minuten verlängern. So könnte in naher Zukunft die Analyse der Bebendaten in den Messstationen um die Auswertung der elastogravitativen Signale erweitert werden.",Mit Lichtgeschwindigkeit vor Erdbeben warnen
"In den Zentren der meisten Galaxien befindet sich ein Schwarzes Loch mit der millionen- oder gar milliardenfachen Masse unserer Sonne – auch in der Milchstraße. Die Schwerkraft dieser Objekte ist so gewaltig, dass nicht einmal Licht aus ihnen entkommen kann. Das macht eine Fotografie eigentlich unmöglich. Mit dem Event Horizon Telescope, einem Zusammenschluss aus mehreren Radioteleskopen, gelang nun dennoch ein direkter Blick auf das Schwarze Loch im galaktischen Zentrum: Die nun in einer Sonderausgabe des Fachblatts „Astrophysical Journal Letters“ veröffentlichte Aufnahme zeigt zwar nicht das Schwarze Loch selbst – sein „Schatten“ ist aber deutlich als dunkle zentrale Region zu erkennen, umgeben von einer hellen ringförmigen Struktur.
Trotz der enormen Masse handelt es sich bei Schwarzen Löchern um extrem kompakte Himmelskörper. Um die unmittelbare Nähe dieser Objekte abzubilden, schalteten die mehr als dreihundert beteiligten Astronomen mehrere Radioteleskope auf der gesamten Welt zusammen, darunter ALMA und APEX in Chile sowie Observatorien in Europa, Hawaii und am Südpol. Dieses als Interferometrie bezeichnete Verfahren ermöglicht eine deutlich höhere Auflösung verglichen mit den Einzelteleskopen. Nach jahrelanger Vorarbeit führten die Forscher 2017 schließlich erste Beobachtungen mit dem Teleskopnetzwerk – Event Horizon Telescope genannt – durch. Mit Erfolg: Im April 2019 präsentierte das Team das erste Foto eines Schwarzen Lochs. Das Bild zeigt die Umgebung des supermassereichen Schwarzen Lochs im Zentrum der 55 Millionen Lichtjahre entfernten Galaxie M87. Mit 6,5 Milliarden Sonnenmassen ist dieses Objekt mehr als tausendfach schwerer als das Schwarze Loch im Zentrum der Milchstraße.
Tatsächlich hatten die Wissenschaftler die Radioantennen des Event Horizon Telescope vor fünf Jahren nicht nur auf diese ferne Galaxie, sondern auch auf das mit 27 000 Lichtjahren viel näher liegende Zentrum der Milchstraße gerichtet. Trotz der geringeren Distanz erwies sich die Auswertung der Beobachtungsdaten als weitaus schwieriger. „Die Strahlung des Schwarzen Lochs von M87 ist über Stunden hinweg konstant“, erläutert der an der Studie beteiligte Forscher Anton Zensus vom Max-Planck-Institut für Radioastronomie. „Das Objekt im galaktischen Zentrum dagegen verändert sich schon im Verlauf weniger Minuten. Wir mussten deshalb völlig neue Methoden für die Auswertung entwickeln.“













                Schwarze Löcher in M87 und der Milchstraße
            



Wie bei M87 lässt sich auch auf dem ersten Foto vom Schwarzen Loch im Zentrum der Milchstraße ein leuchtender Ring um einen dunklen Kern erkennen. Diesen dunklen Bereich bezeichnen die Forscher als Schatten des Schwarzen Lochs. Er ist etwa doppelt so groß wie der eigentliche Ereignishorizont – ab dieser Grenze kann weder Licht noch Materie dem Schwarzen Loch entkommen. Bei dem leuchtenden Ring handelt es sich um aufgeheiztes Gas, das um das Schwarze Loch herumwirbelt. Die enorme Anziehungskraft zwingt die von diesem Gas ausgehende Strahlung auf gekrümmte Bahnen und sorgt so für einen verzerrten Blick auf die Region um das Schwarze Loch.
Diese durch die Gravitation verursachten Effekte lassen sich mithilfe der Allgemeinen Relativitätstheorie beschreiben. Die Wissenschaftler setzten jetzt Computermodelle ein, um ihre Beobachtungen mit den Vorhersagen der Theorie zu vergleichen. Demnach stimmt die Aufnahme sehr gut mit der erwarteten Verzerrung für ein Schwarzes Loch mit der viermillionenfachen Masse der Sonne überein. Zu diesem Ergebnis kommen auch frühere Messungen, bei denen man unter anderem die Bahnen von Sternen nahe dem Schwarzen Loch nutzte, um auf die Masse des zentralen Objekts zu schließen.
Der Vergleich mit unterschiedlichen Modellen erlaubte zudem noch weitere Schlussfolgerungen. „Am besten passen die Modelle, die eine Rotation des Schwarzen Lochs annehmen“, erläutert Koautor Karl Schuster vom Institut für Millimeterwellenradioastronomie in Frankreich. „Außerdem scheint die Rotationsachse des Schwarzen Lochs mehr oder weniger in Richtung Erde geneigt zu sein“, so der Forscher weiter. Das sei ungewöhnlich, weil es nicht mit der Drehachse der Milchstraße übereinstimme.
Für die beteiligten Wissenschaftler ist das Foto des galaktischen Zentrums ein großer Erfolg, gleichwohl aber nur ein erster Schritt: „Es zeigt uns, dass unsere Methode funktioniert“, so Zensus. Für die Zukunft hofft der Forscher auf die Erweiterung des Teleskopnetzwerks – möglichst auch durch Antennen im Weltall. Denn damit ließen sich Bilder mit noch erheblich größerer Auflösung erzielen und dadurch weitere Erkenntnisse über die physikalischen Vorgänge in der unmittelbaren Umgebung supermassereicher Schwarzer Löcher gewinnen.",Schwarzes Loch in der Milchstraße abgelichtet
"Von September 2019 bis Oktober 2020 driftete das Forschungsschiff „Polarstern“ im Rahmen der MOSAiC-Expedition festgefroren an eine Eisscholle durch das Nordpolarmeer – für Wissenschaftler eine bisher einmalige Gelegenheit, die Klimaprozesse in der Arktis über ein ganzes Jahr zu verfolgen. Neben Untersuchungen der Atmosphäre und des Ozeans, widmeten sich die Forscher vor allem dem Meereis. Im Interview mit Welt der Physik berichtet Marcel Nicolaus vom Alfred-Wegener-Institut in Bremerhaven, wie sich die Scholle – auf der das Schiff eingefroren war – im Lauf der Jahreszeiten veränderte und was das über den Klimawandel verrät.
Welt der Physik: Warum war es wichtig, das arktische Eis ein Jahr lang zu beobachten?












                Marcel Nicolaus
            



Marcel Nicolaus: Zum einen konnten wir Eisschollen bisher nur ausschnitthaft untersuchen und dadurch beispielsweise ihre Entstehungsgeschichte sowie ihren weiteren Verlauf nicht berücksichtigen. Zum anderen gibt es überhaupt keine direkten Winterbeobachtungen vom Meereis in der Arktis, weil man zu dieser Zeit eigentlich nicht dorthin gelangt. Deshalb haben wir uns im Herbst mit dem Schiff der MOSAiC-Expedition im Eis einfrieren lassen. So konnten wir mit dem Eis durch ein ganzes Jahr reisen.
Was geschah mit Ihrer Scholle im Verlauf der Jahreszeiten?
Sie ist über das Jahr sehr starken Veränderungen ausgesetzt: Zunächst wächst das Eis und erreicht im Winter eine Dicke von ungefähr zwei Metern. Währenddessen bricht die Scholle durch die Strömung und den Wind auf. An den Bruchstellen bildet sich zunächst neues Eis, das sich dann wieder zusammenschiebt – sogenannte Presseisrücken entstehen und das Eis wird immer deformierter. Außerdem fällt Schnee auf die Scholle, der nicht eine glatte Schicht bildet, sondern sich unregelmäßig verteilt. Im Frühjahr schmilzt dann zuerst der Schnee von oben. Es bilden sich die Schmelztümpel, die auf Bildern so charakteristisch blau erscheinen. Wenn sich der Ozean im Sommer erwärmt hat, beginnt das Eis schließlich, von unten zu schmelzen, bis es wieder komplett verschwunden ist.
Mit welchen Methoden haben Sie die Scholle untersucht?
Wir haben die Eisdicke vor allem mit elektromagnetischen Verfahren vermessen. Außerdem beobachteten wir die Energiebilanz sehr intensiv – dafür haben wir sehr viele Messgeräte, sogenannte Thermistorketten, im Eis eingefroren, um zu sehen, wie sich die Thermodynamik des Eises verändert. Und ich persönlich beschäftige mich sehr viel mit den Wechselwirkungen zwischen Sonnenlicht und Meereis. Gemeinsam mit meinen Kollegen habe ich untersucht, wie viel Licht das Eis reflektiert und wie viel durch es hindurch in den Ozean gelangt. Dafür haben wir mit einem Tauchroboter die Scholle von unten kartiert und die Wassereigenschaften unter dem Eis erfasst. Außerdem wurden Eiskernbohrungen durchgeführt, Schneeproben genommen, Seismikstationen auf dem Eis installiert und vieles mehr.












                Einsatz eines Tauchroboters
            



Was haben Sie so über die Bewegungen des Eis erfahren?
Im Winter driftet das Eis in einer großen Packeismasse, während es im Sommer eher losgelöst voneinander in einzelnen Schollen eine freie Driftbewegung vollzieht. Die Drift ist unregelmäßig und hängt sehr vom Wetter ab. Häufig driftet man mit einer Geschwindigkeit von einem halben Knoten, also etwa einem Kilometer pro Stunde. Manchmal hängt die Schollenbewegung sehr von den Gezeiten ab, sie bewegt sich also ein Stück vor und dann wieder ein Stück zurück. Dadurch gab es Tage, an denen wir 50 Kilometer zurückgelegt haben, aber auch Tage, an denen wir fünf Kilometer gegen den Strom zurück gedriftet sind. Insgesamt war die Drift aber deutlich schneller, als wir erwartet – und auch erhofft – hatten. Denn das Eis in der Arktis ist aufgrund des Klimawandels dünner und damit mobiler geworden. Wenn wir die aktuellen Eisverhältnisse mit denen vor 20 oder vor 100 Jahren vergleichen, müssen wir feststellen, dass sich die Drift erheblich verändert hat.
Gab es bei Ihrer Mission noch weitere Überraschungen oder wurden vor allem Vermutungen bestätigt?
Es gab durchaus kleinere Überraschungen. Der Einfluss des Schnees auf das Meereis beispielsweise hat enorm an Bedeutung gewonnen und ist bislang in Klimamodellen unterrepräsentiert. Schnee ist in seinen physikalischen Eigenschaften wesentlich extremer als Meereis. Denn die Wärmeleitfähigkeit oder das Rückstreuverhalten von zehn Zentimetern Schnee entspricht den Eigenschaften von einem Meter Meereis. Schnee und Eis in der Arktis haben sich dadurch stark verändert, wodurch kleine Wetterereignisse mitunter größere Auswirkungen haben, als bislang angenommen. Der Effekt von Warmluftereignissen im Frühjahr beispielsweise verschwindet nicht nach zwei, drei Tagen wieder, sondern bleibt erhalten. Denn der Schnee schmilzt leicht und gefriert dann wieder – und das verändert die Eigenschaften des Eises.
Sie haben während der Expedition sehr stark interdisziplinär gearbeitet, beispielsweise mit Ozeanographen und Atmosphärenphysikern – warum war das für die Analyse der Daten wichtig?
Um ein Gesamtbild der Geschehnisse in der Arktis zu erhalten, ist die Analyse von Wechselwirkungen das Entscheidende, denn viele Prozesse finden an den Schnittstellen zwischen Atmosphäre und Eis oder zwischen Eis und Ozean statt. Für diese Analyse nutzen wir die Messungen von vielen verschiedenen Geräten – beispielsweise von Wolkenlasern und -radaren, die Rückschlüsse auf den Zustand der Atmosphäre zu einem bestimmten Zeitpunkt liefern. Außerdem haben wir die Daten von den Wetterstationen auf der Scholle und den Sensoren im Eis. Damit können wir also die Reaktionen des Eises zu einem bestimmten Zeitpunkt auf die vorliegenden Wetterverhältnisse bestimmen. Darüber hinaus erforschen Kollegen biologisch-physikalische Kopplungen, also wie sich etwa der Lebensraum von Mikroorganismen durch das Schmelzen der Scholle verändert. Oder sie verfolgen Stoffflüsse, also beispielsweise wie CO2 oder Methan vom Ozean durch das Eis in die Atmosphäre gelangt. Und nur wenn man all diese Informationen erfasst, lassen sich die Wechselwirkungen zwischen den verschiedenen Systemen erkennen, die wiederum wichtig sind, um numerische Modelle zu verbessern.
Wie genau lassen sich mit den neu gewonnenen Erkenntnissen die aktuellen Klimamodelle verbessern?
Wir wollen unser Klimasystem generell besser verstehen und dabei spielt die Arktis eine zentrale Rolle. Eine bessere Vorhersage der Meereisentwicklung sagt uns viel über die klimatischen Veränderungen in unseren Breiten und in anderen Teilen der Erde. Allerdings liegen in den heutigen Klimamodellen die größten Unsicherheiten in den Polarregionen – unter anderem auch, weil hier die schlechteste Datenlage vorhanden ist. Wie sich das Meereis verändert und wann die Arktis im Sommer eisfrei ist, sind also wichtige Fragen, die es zu klären gilt. Deshalb haben wir jetzt erst einmal die gesammelten Informationen unserer Scholle und einiger Schollen in der Umgebung erfasst. Die untersuchten Wechselwirkungen werden wir in den kommenden Jahren in bestehende Modelle integrieren.
Fahren Sie bald wieder in die Arktis?
Ich bin nun schon seit 20 Jahren immer wieder in der Arktis und ich werde auch diesen Sommer wieder dorthin fahren – zunächst auf eine kürzere Expedition, um Bojen mit autonomen Messsystemen auszubringen, die dann im Herbst und Winter Richtung Nordpol treiben. Die nächste größere Expedition mit der „Polarstern“ steht für mich dann nächstes Jahr im Herbst an.",„Das Eis ist dünner und mobiler geworden“
"Algen gelten als effiziente Lieferanten von Biomasse. Doch auch auf anderem Wege lassen sie sich zur Energiegewinnung nutzen: So gelang es Forschern, mit bestimmten Cyanobakterien – besser bekannt als Blaualgen – Strom zu erzeugen. Dazu bauten sie einen kleinen Algentank von der Größe einer herkömmlichen AA-Batterie. Die Algen erzeugten darin mittels Photosynthese genug Strom, um bereits ein Jahr lang einen Mikroprozessor zu betreiben. Ihren Prototypen stellen die Wissenschaftler in der Fachzeitschrift „Energy & Environmental Science“ vor.
Paolo Bombelli und seine Kollegen von der University of Cambridge griffen für diese Algenbatterie zu einem mit Wasser gefüllten, durchsichtigen Gefäß. Darin tauchten sie ein Büschel aus feinen Aluminiumfäden ein, an denen die Bakterien hafteten. Diese nutzten einfallendes Sonnenlicht, um mittels Photosynthese Sauerstoff zu erzeugen. Gleichzeitig floss zwischen den dünnen Aluminiumfäden und der Gegenelektrode, die von den Fäden durch eine gasdurchlässige Membran aus Teflon separiert war, Strom. Ganz verstanden ist der Mechanismus hierfür zwar noch nicht, verantwortlich waren aber wahrscheinlich Elektronen, die an den Elektroden von den Cyanobakterien selbst abgegeben werden.
Mit der Algenbatterie hatten die Forscher somit eine Art biologische Solarzelle konstruiert. Tagsüber lieferte sie eine Leistung von 4,2 Mikrowatt pro Quadratzentimeter Elektrodenfläche – das ist zwar selbst im Vergleich zu anderen Methoden wenig, doch für kleine Elektronikmodule ausreichend. Und selbst in den Nachtstunden ohne Sonnenlicht lag die Leistung der Zelle immerhin noch bei rund fünf Prozent der Tagesleistung. Auch die Langlebigkeit der Zelle überraschte die Forscher: „Wir dachten, die Stromerzeugung würde nach einigen Wochen enden“, sagt Bombelli. Doch stattdessen erzeugten die Algen über Monate hinweg genügend Strom, um einen kleinen Computerprozessor durchgehend zu betreiben.
Dieses Experiment zeigt, dass Kulturen aus Cyanobakterien mittels Photosynthese genügend Strom liefern können, um kleine elektronische Bauteile zu versorgen. So ließen sich in Zukunft möglicherweise Abermilliarden von kleinen Lithium-Ionen-Batterien wenigstens teilweise durch die handlichen Algenbatterien ersetzen. Wie lange die Cyanobakterien überleben und genug Strom erzeugen, wissen die Forscher jedoch noch nicht. Denn nach rund einem Jahr läuft die Bakterienbatterie in Cambridge noch immer.",Blaualgen produzieren Solarstrom
"Im Jahr 2008 waren Astronomen auf rätselhafte Gammastrahlung aus der zentralen Verdickung unserer Milchstraße gestoßen – seither suchen sie nach einer Erklärung für diese Strahlung. Eine Vermutung war, dass sie durch den Zerfall von Teilchen der rätselhaften Dunklen Materie entstehen könnte. Doch nun zeigte ein Forscherteam: Es handelt sich stattdessen um Strahlung von schnell rotierenden Neutronensternen, so die Wissenschaftler im Fachblatt „Nature Astronomy“.
Unsere Milchstraße ist eine Spiralgalaxie. Der Großteil ihrer Sterne und das meiste Gas befinden sich in einer rotierenden Scheibe mit ausgeprägten Spiralarmen. Doch zum Zentrum hin verdickt sich die Scheibe. Aus dieser Region, dem „Bulge“, hatten frühere Beobachtungen mit dem internationalen Satellitenobservatorium Fermi diffus verteilte hochenergetische Gammastrahlung empfangen. Strahlung, die vom Zentrum der Milchstraße nach außen hin schwächer wird – und zwar in ähnlicher Weise, wie auch die Sterndichte abnimmt. Von Anfang an favorisierten Astronomen hierfür zwei Erklärungen: Den Zerfall von Teilchen der Dunklen Materie – oder die Strahlung schnell rotierender Neutronensterne mit starken Magnetfeldern. Solche Neutronensterne drehen sich in wenigen Millisekunden um sich selbst und heißen daher auch Millisekundenpulsare.
Doch eine ganze Reihe von Modellrechnungen stießen beim Ansatz, die Strahlung mit solchen Pulsaren zu erklären, immer wieder auf das gleiche Problem: Es schien zu wenig solcher Pulsare in dieser Region der Milchstraße zu geben: Etwa 1000 solcher Pulsare müsste es dort geben, um die Strahlung zu erklären – doch auf gerade einmal 42 solcher Objekte war man bislang gestoßen. Auch die Verteilung der beobachteten Pulsare in der Milchstraße ließ sich bislang nicht mit dem Erklärungsmodell in Einklang bringen.
Bei den bisherigen Berechnungen war jedoch etwas wichtiges übersehen worden, wie Anuj Gautam von der Australian National University in Canberra und seine Kollegen jetzt zeigen. So gibt es einen bislang nicht berücksichtigten Weg, wie die Millisekundenpulsare entstehen. Dabei strömt in einem System aus zwei Sternen Materie von einem großen Stern auf einen Weißen Zwerg. Dadurch dreht sich der Weiße Zwerg immer schneller. Schließlich wächst seine Masse so sehr an, dass er unter seiner eigenen Anziehungskraft kollabiert – ein Millisekundenpulsar entsteht.
Gautam und seine Kollegen simulierten diesen Prozess und erhielten so eine Vorhersage, wie viele Millisekundenpulsare auf diese Weise entstehen. Anhand dessen berechneten die Forscher wiederum, wie viel Gammastrahlung diese Objekte erzeugen. „Das Ergebnis ist in guter Übereinstimmung mit der beobachteten Gammastrahlung durch Fermi“, sagt Gautams Kollege Roland Crocker. Außerdem ließe sich das Modell durch zukünftige Beobachtungen überprüfen: So sollte das derzeit im Bau befindliche Cherenkov Telescope Array – zwei große Antennenanlagen zum Nachweis von Gammastrahlung auf La Palma und in Chile – in der Lage sein, einige dieser Pulsare aufzuspüren.",Pulsare erklären rätselhafte Gammastrahlung
"Vor 3,6 bis 3,0 Milliarden Jahren war der Mars – ganz ähnlich der Erde – noch ein warmer Planet mit flüssigem Wasser auf der Oberfläche. Doch dann kühlte er schlagartig ab, das Wasser verschwand und er wurde zu dem Wüstenplaneten, den wir heute kennen. Der Verlust des Treibhausgases Kohlendioxid, so die gängige Hypothese, löste den Klimaumschwung aus. Doch neue Modelle einer Forschungsgruppe zeigen jetzt, dass diese Annahme nicht in Einklang mit der Klimageschichte des Mars steht. Vielmehr müssten andere Ursachen eine entscheidende Rolle gespielt haben, berichten die Planetenforscher im Fachblatt „Science Advances“.
Zahlreiche ausgetrocknete Flussbetten, Fließspuren um Krater und sogar Küstenlinien eines ausgedehnten Ozeans belegen, dass auf dem Mars einst ein lebensfreundlicheres Klima geherrscht hat. Möglicherweise ist damals, wie auf der Erde, auch auf dem Mars Leben entstanden. Widerstandfähige Einzeller könnten tief im Marsboden sogar bis heute überlebt haben. Warum aber hat sich unser Nachbarplanet vor drei Milliarden Jahren plötzlich um durchschnittlich zehn Grad abgekühlt? Um eine Antwort auf diese Frage zu finden, haben Edwin Kite von der University of Chicago und seine Kollegen von Wasser beeinflusste Strukturen – in erster Linie Flussbetten – untersucht.
Diese Strukturen stehen in engem Zusammenhang mit dem Klima auf dem Mars. Wie sie sich im Lauf der Geschichte des Planeten verändert haben, konnten die Forscher nun anhand von geologischen Daten feststellen. „Die Veränderungen lassen sich mit einem Schmelzwassermodell vergleichen, das wir wiederum aus Simulationen des Marsklimas erhalten“, erläutern Kite und seine Kollegen. In das Klimamodell des Roten Planeten gehen entscheidend die Dichte und die chemische Zusammensetzung seiner Atmosphäre ein – denn es sind die Treibhausgase, die den Planeten erwärmen. Die Forscher führten zahlreiche Simulationen mit unterschiedlichen atmosphärischen Veränderungen aus – und stießen auf einen überraschenden Befund.
Denn bislang sahen die meisten Planetenforscher den Verlust von Kohlendioxid als Hauptursache des Klimaumschwungs auf dem Mars. „Doch für uns unerwartet zeigt die Analyse des Treibhauseffekts innerhalb unserer globalen Klimamodelle, dass die Klimaänderung nicht in erster Linie durch Kohlendioxid, sondern durch andere Effekte angetrieben wurde.“
Welche Effekte das allerdings waren, können die Forscher aus ihren Modellen nicht ablesen. So könnten die Abnahme von Wasserdampf in der Atmosphäre oder die Bildung von Wolken aus Eiskristallen in der Hochatmosphäre, die einen signifikanten Anteil der Sonnenstrahlung reflektieren, mögliche Ursachen sein. Auch könnten andere Treibhausgase als Kohlendioxid eine Rolle gespielt haben. Unklar ist zudem, ob das Verschwinden des Wassers von der Oberfläche des Mars eine Folge oder eine Mitursache der Klimaänderung war. Die Forscher hoffen nun, aus der Analyse von Bodenproben weitere Informationen über die Klimageschichte des Roten Planeten zu erhalten.",Warum kühlte der Mars ab?
"Nach dem Urknall sollten Materie und Antimaterie eigentlich zu gleichen Teilen entstanden sein. Doch unser Universum besteht heute nur noch aus Materie – dieses Ungleichgewicht gehört zu den spannendsten Rätseln der Teilchenphysik. Auf der Suche nach möglichen Erklärungen haben Forscher nun eine überraschende Entdeckung für Antiprotonen in supraflüssigem Helium gemacht. Im Interview mit Welt der Physik spricht Masaki Hori vom Max-Planck-Institut für Quantenoptik in Garching über dieses bislang nicht erklärbare Phänomen.
Welt der Physik: Wie untersuchen Sie Antimaterie?












                Masaki Hori
            



Masaki Hori: Es gibt viele verschiedene Methoden, Antiprotonen und andere Teilchen aus Antimaterie zu analysieren. Meine Arbeitsgruppe hat sich darauf spezialisiert, sogenannte exotische Atome zu erzeugen, bei denen ein Elektron eines gewöhnlichen Atoms durch ein Antimaterieteilchen ersetzt wird. In den nun veröffentlichten Experimenten haben wir Antiprotonen in Helium eingefangen. Das ermöglicht es, hochgenaue spektroskopische Untersuchungen an diesen Atomen durchzuführen. Dabei werden die Atome mit Laserlicht bestrahlt und beobachtet, welcher Teil des elektromagnetischen Spektrums absorbiert wird. Indem wir dann diese Spektrallinien analysieren, lassen sich die Eigenschaften der Antimaterie sehr gut überprüfen.
Mit welcher Art von Atomen haben Sie gearbeitet?
Antiprotonen haben die gleichen Eigenschaften wie Protonen, allerdings genau die entgegengesetzte Ladung – sie sind also negativ geladen wie Elektronen. Für unsere Experimente haben wir den Antiprotonen-Strahl am Forschungszentrum CERN genutzt: Die Antiprotonen entstehen bei Kollisionen der hochenergetischen Protonen am dortigen Teilchenbeschleuniger und werden dann mit Hilfe des Antiproton Decelerators heruntergebremst. Die abgebremsten Antiprotonen haben wir dann auf eine Probe aus Helium geleitet. Dort können sie von den Heliumatomen eingefangen werden, indem eines der beiden Elektronen aus der Hülle verdrängt wird und das Antiproton seinen Platz einnimmt. Ein solches Atom besteht dann aus einem Atomkern mit einer Hülle aus einem Elektron und einem Antiproton. Man nennt diese Atome auch antiprotonisches Helium.
Wie stabil sind solche exotischen Atome?
Sie überleben typischerweise nur Mikrosekunden, also millionstel Sekunden. Das ist aber lang genug, um mithilfe von Laserstrahlen die spektroskopischen Untersuchungen zu machen. Der Grund für diese kurze Lebensdauer: Wenn Antiprotonen in die Nähe von Atomkernen gelangen, reagieren sie sofort mit den Protonen im Atomkern und zerstrahlen zu purer Energie. Normalerweise finden spektroskopische Experimente im gasförmigen Zustand statt. Nun haben wir aber festgestellt, dass dies überraschenderweise auch in bestimmten Flüssigkeiten sehr gut funktioniert.
Warum ist das so überraschend?
Spektrallinien – wie etwa die berühmten Fraunhofer-Linien im Sonnenspektrum – untersucht man üblicherweise in Gas. Denn je dichter ein Material ist, desto mehr werden die spektralen Übergänge durch Stöße zwischen den Atomen und durch wechselseitige elektromagnetische Einflüsse gestört. Dadurch verbreitern sich die Spektrallinien massiv und spektrale Untersuchungen werden immer schwieriger. In Flüssigkeiten können sich Spektrallinien um mehr als den Faktor eine Million verbreitern und werden oft praktisch unkenntlich, so dass sie sich von anderen Spektrallinien nicht mehr unterscheiden lassen. Wie wir nun festgestellt haben, gilt das aber nicht für Atome mit Antiprotonen in supraflüssigem Helium.













                Experiment am Forschungszentrum CERN
            



Welche besonderen Eigenschaften besitzt denn supraflüssiges Helium?
Diese Flüssigkeit entsteht, wenn man flüssiges Helium nochmals stark abkühlt, auf Temperaturen von nur etwa zwei Grad über dem absoluten Temperatur-Nullpunkt. Dann verliert das Helium seine innere Reibung und geht in den sogenannten supraflüssigen Zustand über – ähnlich wie Supraleiter bei sehr tiefen Temperaturen ihren elektrischen Widerstand praktisch vollständig verlieren. Supraflüssiges Helium ist sehr transparent und extrem kalt, was spektroskopische Untersuchungen begünstigt. Allerdings trifft das nicht auf alle Atome zu: Wenn man bestimmte gewöhnliche Atome in supraflüssigem Helium löst, sind deren Spektrallinien trotzdem stark verbreitert, wie auch in gewöhnlichen Flüssigkeiten. Bei antiprotonischem Helium gilt das nach unseren neuen Ergebnissen aber nicht.
Haben Sie eine Erklärung für dieses überraschende Verhalten?
Bislang gibt es meines Wissens keine theoretische Erklärung, wir waren davon selbst völlig überrascht. Das Schöne an Naturwissenschaften ist ja, dass die Natur einem immer wieder etwas völlig Unerwartetes präsentiert. Ein Grund könnte darin liegen, dass antiprotonische Heliumatome ihre Größe während der Experimente kaum ändern. Das allein erklärt allerdings nicht die plötzliche Abnahme der Linienbreite bei der Abkühlung zu supraflüssigem Helium. Wir wollen künftig untersuchen, ob das nur bei antiprotonischem Helium so gut funktioniert, oder ob auch andere exotische Atomsorten genauso scharfe Spektrallinien zeigen.
Gibt es schon mögliche Anwendungen für das neu beobachtete Phänomen?
Da alles noch sehr neu ist, kann ich nur spekulieren. Aber es ist zum Beispiel denkbar, mit einem solchen System nach Antimaterie in der kosmischen Strahlung zu suchen. Denn viele Forschungssatelliten haben zu Kühlungszwecken supraflüssiges Helium an Bord. Wenn Antiprotonen aus der kosmischen Strahlung dort eingefangen werden, könnte man das mit einer passenden Laserquelle und einem Detektor möglicherweise sichtbar machen. Aber wie gut das technisch machbar ist und ob das besser ist als andere Nachweismethoden, müssten sich Experten für Satellitenexperimente überlegen.",„Wir waren davon selbst völlig überrascht“
"Sehr viel mehr Zwerggalaxien als bislang angenommen enthalten in ihrem Zentrum ein massereiches Schwarzes Loch. Das stellt nun ein Forschungsteam fest, nachdem es alte Beobachtungsdaten von Galaxien mithilfe eines verbesserten Verfahrens erneut untersucht hat: Bisherige Methoden hatten die Schwarzen Löcher demnach in vielen der kleinen Sternsysteme übersehen. Das Ergebnis liefert somit auch einen Einblick in die Entwicklung supermassereicher Schwarzer Löcher. Diese können entstehen, wenn die Schwarzen Löcher in den Zwerggalaxien bei der Bildung größerer Galaxien miteinander verschmelzen, so die Wissenschaftler im Fachblatt „Astrophysical Journal“.
Große Galaxien wie unsere Milchstraße bilden sich – so die gängige Erklärung – im Lauf der kosmischen Geschichte, indem sich kleinere Sternsysteme miteinander vereinen. Ein Beispiel sind die Magellanschen Wolken in der Nähe der Milchstraße, die in ferner Zukunft mit dieser verschmelzen und sie weiter anwachsen lassen. Dabei – so die Annahme – verschmelzen auch die Schwarzen Löcher, die sich in nahezu allen Galaxienzentren befinden, und wachsen immer weiter an.
Tatsächlich enthalten zwar viele Zwerggalaxien Schwarze Löcher mit weit über der tausendfachen Masse der Sonne, aus denen sich schließlich Schwarze Löcher mit der millionen- oder gar milliardenfachen Masse der Sonne bilden können. Doch ob es genügend von ihnen gibt, um die Entstehung der zahlreichen supermassereichen Schwarzen Löcher zu erklären, war bislang unklar. Denn Astronomen identifizieren die Schwarzen Löcher anhand der Strahlung, die diese aus den Zentralregionen der Zwerggalaxien aussenden. Dort befinden sich jedoch häufig auch viele neu entstehende Sterne, die eine ähnliche Strahlung abgeben.
„Bei den bisherigen Analysen wurden daher zweifelhafte Fälle aussortiert“, erläutert Sheila Kannappan von der University of North Carolina – eine unbefriedigende Situation nach Ansicht der Forscherin. Gemeinsam mit ihrem Team entwickelte sie daher einen Test, der Schwarze Löcher unabhängig von der etwaigen Entstehung neuer Sterne identifizieren kann. Ihr Kollege Chris Richardson von der Elon University zeigte parallel dazu mit neuen Computersimulationen, welche Strahlung zu erwarten ist, wenn Zwerggalaxien mit einer hohen Sternentstehungsrate zugleich ein massereiches Schwarzes Loch enthalten.
Mit diesen beiden Verfahren durchsuchten die Wissenschaftler die Daten mehrerer umfangreicher Galaxienkataloge erneut – mit einem überraschenden Ergebnis: „Diese neue Art von Schwarzen Löchern fand sich in nahezu allen Zwerggalaxien“, sagt Mugdha Polimera von der University of North Carolina. „Ehrlich gesagt zweifelte ich zunächst an unserem Verfahren, als ich diese Zahlen sah!“
Doch die Anzahl der von dem Team aufgespürten Schwarzen Löcher übertrifft alle früheren Untersuchungen tatsächlich deutlich – und bestätigt so, dass Schwarze Löcher durch die Verschmelzung von Zwerggalaxien zu großen Galaxien anwachsen. „Die von uns gefundenen Schwarzen Löcher sind die Bausteine, aus denen supermassereiche Schwarze Löcher wie jenes im Zentrum der Milchstraße entstehen“, lässt Kannappan keinen Zweifel.",Eine Fundgrube für Schwarze Löcher
"Die Schwerkraft von Schwarzen Löchern ist so gewaltig, dass nicht einmal Licht aus ihnen entkommen kann. Ein Nachweis dieser Objekte gestaltet sich dadurch schwierig – und so war ihre Existenz lange umstritten. Wie alles mit der vagen Idee von „Dunklen Sternen“ begann und was wir heute über diese extremen Objekte wissen, berichtet Reinhard Genzel vom Max-Planck-Institut für extraterrestrische Physik in Garching in dieser Folge des Podcasts.












                Reinhard Genzel
            



Im Zentrum unserer Galaxis – rund 27 000 Lichtjahre von der Erde entfernt – befindet sich ein supermassereiches Schwarzes Loch. Davon sind Astronomen inzwischen überzeugt. Vor einigen Jahren sah das noch ganz anders aus.Reinhard Genzel: „Im Prinzip wissen wir seit rund hundert Jahren etwas über Schwarze Löcher als theoretische Objekte. Und erst die letzten zwanzig oder fünfzehn Jahre hat es experimentell auf einmal gedonnert und geknallt.“Zum Donnern und Knallen leistete der Astrophysiker und Nobelpreisträger einen entscheidenden Beitrag: In den 1990er-Jahren lieferten Reinhard Genzel und sein Team den ersten konkreten Hinweis auf ein Schwarzes Loch im galaktischen Zentrum. Mehr dazu in der 334. Folge.",Schwarze Löcher
"Zwei neuartige Bildgebungsverfahren – Dunkelfeldradiografie und Dunkelfeldtomografie genannt – erzielen einen bislang unerreichten Bildkontrast mit Röntgenstrahlen. Das eröffnet gänzlich neue diagnostische Möglichkeiten. So lässt sich auf den zwei- beziehungsweise dreidimensionalen Aufnahmen beispielsweise der Zustand von Lungenbläschen erkennen. Wie die beiden Verfahren funktionieren, erklärt Franz Pfeiffer von der Technischen Universität München im Interview mit Welt der Physik.
Welt der Physik: Was ist das Besondere an der neuen Röntgenmethode?
Franz Pfeiffer: Die dabei eingesetzte Dunkelfeldtechnik stammt ursprünglich aus der optischen Mikroskopie, wo sie schon seit Jahrzehnten etabliert ist. Im Gegensatz zur normalen Mikroskopie besteht die Idee bei Dunkelfeldaufnahmen darin, nicht das Licht aufzunehmen, das durch ein Objekt hindurchgegangen und dabei abgeschwächt worden ist. Stattdessen nimmt man nur das Licht auf, das gestreut worden ist – daher der Name „Dunkelfeld“. Diese Technik hat bestimmte Vor- und Nachteile. Aber insbesondere kann sie Dinge sichtbar machen, die auf gewöhnlichen Aufnahmen nicht zu sehen sind.













                Franz Pfeiffer am Dunkelfeldtomografen
            



Was lässt sich auf Dunkelfeldaufnahmen denn erkennen, was sonst verborgen bliebe?
Auf gewöhnlichen Röntgenbildern sieht man vor allem den Kontrast zwischen dichtem Gewebe wie Knochen oder Knorpeln und dem weniger dichten Weichgewebe. Aber sehr feine Strukturen wie etwa Lungenbläschen lassen sich damit nicht auflösen. Hier kann die Dunkelfeldtechnik ihre Vorteile ausspielen. So werden Röntgenstrahlen an den Grenzflächen zwischen Luft und Gewebe in einem kleinen Winkel gestreut. Wenn es nun gelingt, dieses Streulicht von den restlichen Röntgenstrahlen zu trennen, kann man damit den Zustand der Lungenbläschen diagnostisch verwertbar bestimmen. Das ist uns letztes Jahr schon am Patienten in zweidimensionalen Aufnahmen gelungen. Nun haben wir den ersten Dunkelfeldcomputertomografen entwickelt, der dreidimensionale Aufnahmen ermöglicht.
Die Dunkelfeldtechnik wird bereits seit Jahrzehnten in der Mikroskopie eingesetzt, sagten Sie. Warum hat es so lange gedauert, sie auf die Röntgentechnik zu übertragen?
Eine große Rolle spielt vor allem der wesentlich größere technologische Aufwand. In der optischen Mikroskopie gibt es schon seit Jahrzehnten entsprechende Filter und Optiken, um Dunkelfeldaufnahmen zu machen. Das lässt sich leider nicht direkt in die Welt der Röntgentechnik übertragen, da Röntgenstrahlen eine sehr viel kürzere Wellenlänge haben und in Gewebe nur unter sehr kleinem Winkel gestreut werden.
Wie haben Sie dieses Problem gelöst?
Wir arbeiten mit sogenannten Röntgengittern. Das sind einige Zentimeter große, dünne Platten aus einem für Röntgenstrahlen durchlässigen Material wie Silizium. Hierin werden in sehr feinem Abstand – nur wenige Mikrometer – feine, parallele Linien hineingeätzt. Diese werden mit Material wie Gold, Blei oder Wolfram gefüllt, die Röntgenstrahlen absorbieren. Die Gitter in entsprechender Größe und Qualität herzustellen, ist übrigens gar nicht so einfach. Unsere Kolleginnen und Kollegen vom Karlsruher Institut für Technologie haben sie uns geliefert. Wenn man nun mehrere solcher Gitter hinter der Röntgenquelle sowie vor und hinter dem zu durchleuchtenden Objekt anbringt, können wir die in kleinem Winkel gestreuten Röntgenstrahlen von den übrigen unterscheiden und so Dunkelfeldaufnahmen erstellen. Um diese Aufnahmen schließlich zu einem dreidimensionalen Bild zusammenzusetzen, benötigt man natürlich noch einiges an Computerleistung. Aber diese Techniken sind in der herkömmlichen Computertomografie bereits etabliert.












                Dunkelfeldtomograf
            



Kam der neuartige Computertomograf schon zum Einsatz?
Ja, wir haben das Verfahren bereits an Mäusen sowie an sogenannten Phantomen getestet. Dies sind von den Proportionen her menschenähnliche Modelle des Oberkörpers. Die Lungenbläschen haben wir durch einen neoprenartigen Schaum simuliert, wobei die Porengröße entsprechend angepasst war. Die Ergebnisse sowohl am Tiermodell als auch an den Phantomen waren vielversprechend, sodass wir nun dabei sind, den Genehmigungsprozess für erste Versuche am Menschen zu durchlaufen.
Wie sieht es mit der Strahlenbelastung im Vergleich zu herkömmlichen Röntgenaufnahmen aus?
Einfache zweidimensionale Aufnahmen, sogenannte Radiografien, haben heutzutage eine verschwindend geringe Strahlenbelastung. Mit der Dunkelfeldmethode liegt sie etwas höher, dafür werden aber auch viel mehr Strukturen sichtbar. Bei dreidimensionalen Computertomografien verhält sich das ähnlich. Die Strahlenbelastungen sind dort allerdings wesentlich höher, weil viele Röntgenaufnahmen aus verschiedenen Winkeln kombiniert werden. Wie immer in der Medizin wird man hier Nutzen und Risiken im Einzelfall abwägen müssen. Aber angesichts des potenziell lebensbedrohlichen Charakters vieler Lungenkrankheiten sollte eine leicht höhere Strahlenbelastung kein Hindernis darstellen.
Wann rechnen Sie mit ersten klinischen Anwendungen?
Die zweidimensionalen Dunkelfeldradiografien sind bereits auf großes Interesse unter Ärzten und Medizinphysikern gestoßen. Hier gibt es bereits laufende Studien zu verschiedenen Krankheiten wie der chronisch obstruktiven Lungenerkrankung, Covid-19, Lungenkrebs, Pneumothorax und anderen Krankheiten. Auch die Dunkelfeldtomografie wird sicherlich ihren Weg in die klinische Anwendung finden. Allerdings ist der apparative Aufwand deutlich höher, sodass die Industrie hier noch einige Jahre an geeigneten Produktionsverfahren tüfteln müssen wird.",„Großes Interesse unter Ärzten“
"Künstliche, mit Sensoren bestückte Häute verleihen Robotern bereits rudimentäre Sinne für Wärme oder Berührungen. Meistens bestehen die dünnen Hüllen aus Kunststoffen auf der Basis von Silikonen. Doch nun gingen Materialforscher einen Schritt weiter und züchteten eine Haut aus lebenden Zellen. In der Fachzeitschrift „Matter“ beschreiben sie, wie sie einen Roboterfinger mit dieser wasserabweisenden und selbstheilenden Haut ausstatten konnten.
Die Grundlage der neuartigen Roboterhaut bildet ein Verfahren, mit dem Forscher im Labor bereits flache Hautstücke gezüchtet haben. Doch diese ließen sich nicht bündig auf Roboter übertragen. Daher züchteten Shoji Takeuchi von der Universität Tokio und seine Kollegen die bionische Haut nun direkt auf der Oberfläche eines dreigliedrigen Roboterfingers: Sie stülpten einen Schlauch über den Roboter und befüllten ihn mit einer Nährlösung aus dem Strukturprotein Kollagen und lebenden Fibroblasten – die wichtigsten Hautzellen im menschlichen Bindegewebe. Nach wenigen Tagen entstand so eine geschlossene Haut, die sich eng an den Roboterfinger anschmiegte.
In einem zweiten Schritt fügten sie einen weiteren Zelltyp, die Keratinozyten, hinzu. Diese Hautzellen sind der häufigste Zelltyp der oberen Hautschicht und produzieren die stabilisierende Hornsubstanz Keratin. Nach zwei weiteren Wochen war der Roboterfinger schließlich vollkommen von einer stabilen und wasserabweisenden Haut umhüllt. Die neuartige Roboterhaut hielt dutzenden Biegebewegungen mühelos stand. Traten Risse auf, konnten diese mit einem speziellen Pflaster aus Kollagen wieder verschlossen werden.
Mehr als einige Tage hielt die Roboterhaut allerdings nicht. Trocknete die Haut, verlor sie besonders schnell ihre Flexibilität und riss bei Bewegungen. Dieses Problem will das Team um Takeuchi in weiteren Versuchen beispielsweise mit filigranen Kanälen beheben, um die Roboterhaut vor dem Austrocknen zu bewahren. Zusätzlich könnte die Haut in Zukunft mit Sensoren für Tastempfindungen und Temperatur ausgestattet werden. „Diese Arbeit ist ein erster Schritt hin zu Robotern, die komplett mit einer lebenden Haut umhüllt werden können“, sagt Takeuchi. Eine mögliche Anwendung sehen die Forscher in humanoiden Robotern. Dank ihrer bionischen Haut könnten sie menschliche Mimiken besser als ihre Vorgänger in Kunststoffhüllen imitieren.",Selbstheilende Roboterhaut
"Doppelsterne entstehen üblicherweise gemeinsam aus ein und derselben Gaswolke und in unmittelbarer Nachbarschaft zueinander. Doch nun ist ein Forscherteam auf ein junges Sternenpaar gestoßen, das offenbar auf andere Art entstanden ist: Die beiden Sterne haben sich zunächst weit entfernt voneinander gebildet und sich erst später einander angenähert. Bei ihrem Annäherungsprozess verdrehen die beiden Sterne das Magnetfeld in ihrer Umgebung und beeinflussen so die Entstehung anderer Sterne, wie die Wissenschaftler im Fachblatt „Astrophysical Journal“ berichten.
Auf diese Erkenntnisse sind Erin Cox von der Northwestern University in den USA und ihr Team bei Beobachtungen der etwa 700 Lichtjahre entfernten Gaswolke Lynds 483 gestoßen. Dort befinden sich frisch entstandene, junge Sterne, es bilden sich aber auch weiterhin neue Sterne. Die Forscher beobachteten diese Gaswolke mit dem Infrarotteleskop SOFIA, das sich an Bord einer umgebauten Boeing 747 befindet, und bemerkten dabei, dass das Magnetfeld in der Sternentstehungsregion ungewöhnlich verdrillt ist.
Eben eine solche Verdrehung des Magnetfelds sagten einige theoretische Arbeiten voraus. „Wir glauben, dass hier, in Lynds 483, genau das passiert. Doch die Theorie kann das eine sagen – und die Beobachtungen etwas ganz anderes“, so Cox. Die Forscher beschlossen deshalb, sich auf die Suche nach der Ursache des verdrehten Magnetfelds zu machen.
Ihre Beobachtungen mit der fliegenden Sternwarte zeigten schließlich einen jungen Stern, eingehüllt in einen dichten Kokon aus Gas, von dem die besondere Form des Magnetfelds auszugehen schien. Weitere Beobachtungen mit der Radioteleskopanlage ALMA in Chile entlarvten daraufhin noch einen zweiten Stern innerhalb dieser Gashülle. Diese Sterne sind weit entfernt voneinander entstanden, haben sich dann angenähert und schließlich einen Doppelstern gebildet, erklären die Forscher.
Anhand dieser Entdeckung ließen sich die bisherigen Theorien zwar bestätigen – ganz geklärt ist das Phänomen jedoch noch nicht. So ist noch unklar, warum sich die beiden Sterne aufeinander zubewegen. Die Forscher sind sich aber sicher, dass es die Bewegung der Sterne ist, die zu der Verdrillung des Magnetfelds führt. Vermutlich sei dieser Prozess auch wichtig, damit die Sterne ein stabiles System bilden. Denn dies ist nur möglich, wenn sich ihre Bewegung umeinander verlangsamt – was wohl daran liegt, dass das Magnetfeld Energie aus der Drehbewegung der Sterne nach außen an die Gaswolke abführt.",Ein besonderer Doppelstern
"Sinkt das Thermometer auf etwa minus 273 Grad Celsius, ist der absolute Nullpunkt der Temperatur erreicht. Nichts bewegt sich mehr – alle Teilchen sind vollständig eingefroren. Doch dieser Nullpunkt lässt sich nie perfekt erreichen und Stoffe auch nur annähernd auf diese Temperatur abzukühlen, ist bereits schwierig. Nun hat ein Forscherteam von der Universität Singapur ein neues Verfahren vorgestellt, mit dem sich eine ganze Klasse von Atomen erstmals in diesen Bereich herunterkühlen lässt: Für die Atome aus der dritten Hauptgruppe des Periodensystems der Elemente gab es bislang keine vergleichbare Kühlmethode, berichten die Forscher im Fachblatt „Physical Review A“.
Die Temperatur definiert sich physikalisch über die Bewegung kleinster Teilchen: Je weniger sich etwa Moleküle oder Atome in einem Gas oder einer Flüssigkeit umher bewegen oder an ihrer Position in einem Feststoff schwingen, desto geringer ist die Temperatur. Am absoluten Nullpunkt kommt diese Bewegung vollständig zum Erliegen. Doch je näher man in einem Experiment an diesen Zustand gelangt, desto aufwendiger wird es, die Atome immer weiter abzustoppen.
Um dies jedoch zu erreichen, nutzt man magneto-optische Fallen. In diesen besonderen Aufbauten aus Lasern und magnetischen Feldern strahlt man mit intensiven Lasern auf eine Atomwolke, wobei die Frequenz der Laserstrahlen leicht unterhalb einer Anregungsfrequenz der Atome liegt. Fliegen die Atome auf den Laserstrahl zu, werden sie abgebremst. Dies wird immer wieder wiederholt, wodurch sich die Atome schrittweise bis nahe an den absoluten Nullpunkt abbremsen lassen.
Dieses Standardverfahren ist zwar mittlerweile gut etabliert, funktioniert allerdings nicht bei allen Atomsorten – insbesondere nicht bei Elementen der dritten Hauptgruppe, zu denen die Elemente Indium, Bor, Aluminium, Gallium und Thallium gehören. Um Indiumatome so stark abzukühlen, entwickelten Travis Nicholson und sein Team ein neues Verfahren, bei dem sie mit verschiedenen stärker schwingenden Indiumatomen arbeiteten. Dazu mussten sie alle Komponenten eigens für ihr neues Experiment herrichten. Mit diesem Aufbau gelang es ihnen, ein Gas aus rund 500 Millionen Indiumatomen einzufangen. Diese Atomwolke kühlten sie dann durch eine geschickte Kombination von Laserstrahlen.
Wie die Autoren betonen, lässt sich ihr Kühlschema im Prinzip auf alle anderen Atome der dritten Hauptgruppe anwenden. Das dürfte sich auch für andere Experimente als hilfreich erweisen. Denn bei sehr tiefen Temperaturen nahe am absoluten Nullpunkt treten zahlreiche besondere Quantenphänomene wie etwa Supraleitung auf. Da sich nun eine ganze Gruppe von Atomen bis in diese Regionen herunterkühlen lässt, dürfte es nur eine Frage der Zeit sein, bis eine Reihe von Forschungsgruppen weltweit diese Methode einsetzen – zum Beispiel für die Entwicklung zukünftiger Atomuhren.",Neues Kühlverfahren für Atome
"Günstiger, flexibler, effizienter – diese Vorteile bieten Solarzellen auf Basis von Perowskiten im Vergleich zum bisher dominierenden Halbleitermaterial Silizium. Auch ein hoher Wirkungsgrad der besten Laborzellen von 25,7 Prozent steht der Effizienz von Siliziumsolarzellen kaum noch nach. Allein die mangelnde Haltbarkeit der Perowskite bremst bislang die serielle Fertigung. Doch nun gelang es Forschern mit einer extrem dünnen Schutzschicht die Langlebigkeit ihrer Prototypen drastisch zu steigern. Wie sie in der Fachzeitschrift „Science"" berichten, können ihre Perowskitsolarzellen bei bis zu 35 Grad Celsius länger als fünf Jahre Solarstrom erzeugen.
Besonders empfindlich sind bisher die Sonnenlicht absorbierende Perowskitschicht und die darunter liegende Schicht zum Transport der elektrischen Ladungsträger. Daher fügten nun Yueh-Lin Loo von der Princeton University und ihr Team zwischen diese beiden Lagen eine nur wenige Atome dicke Schutzschicht aus den Elementen Caesium, Blei und Iod hinzu. Das sogenannte zweidimensionale Material wirkte sich stark stabilisierend auf die umgebenden Schichten aus, die besonders wichtig für die solare Stromerzeugung sind. Erste Beobachtungen zeigten bereits, dass die Effizienz der Perowskitsolarzelle selbst nach sechs Monaten nicht abnahm.
Diese überraschende Stabilität analysierten Loo und ihre Kollegen daraufhin noch genauer, indem sie eine Art Stresstest für Solarzellen entwickelten. „Eine Haltbarkeit von 30 Jahren ist gewünscht, doch können wir unsere Zellen nicht 30 Jahre lang testen"", sagt Loo. Daher setzten die Forscher ihre Solarzellen nicht nur dauerhaft künstlichem Sonnenlicht aus, sondern heizten sie auch auf hohe Temperaturen von bis zu 110 Grad Celsius auf. Unter diesen Bedingungen ließ sich der Alterungsprozess der Solarzellen stark beschleunigen. Aus den gesammelten Daten schlossen Loo und ihr Team dann auf die tatsächliche Lebensdauer unter kühleren, realistischeren Bedingungen.
Der Stresstest zeigte, dass bei Temperaturen von 110 Grad Celsius der Wirkungsgrad der Solarzellen erst nach mehr als 2100 Stunden auf etwa 80 Prozent des urspünglichen Wirkungsgrades abfiel. Bei 35 Grad Celsius haben Loo und ihre Kollegen dagegen kaum Leistungsverluste beobachtet. Genauere Berechnungen ergaben, dass die Solarzellen bei gemäßigten Temperaturen eine verblüffend lange Haltbarkeit von mehr als fünf Jahren erreichen. Mit neuen Materialvarianten und Fertigungsverfahren ist es demnach möglich, dass Perowskitsolarzellen zukünftig auch 20 bis 30 Jahre lang Solarstrom mit hoher Effizienz erzeugen können, so die Forscher. Parallel ließe sich auch der bislang relativ geringe Wirkungsgrad des Prototypen von 17,4 Prozent weiter steigern. Damit besteht eine realistische Chance, dass in wenigen Jahren reine Perowskitsolarzellen mit mehr als 20 Prozent Wirkungsgrad oder auch Perowskit-Silizium-Tandemzellen mit bis zu 30 Prozent Wirkungsgrad in Serie produziert werden können.",Haltbarkeit von Solarzellen verlängert
"Rund 80 Millionen Tonnen Plastikmüll verschmutzen die Weltmeere nach aktuellen Schätzungen. Daher werden bereits unterschiedliche Verfahren erprobt, um die Plastikteile nach und nach wieder aus den Meeren zu fischen. Doch besonders die höchstens fünf Millimeter großen Mikroplastikpartikel lassen sich nur sehr schwer einsammeln. Um dieses Problem anzugehen, haben Materialforscher nun einen kleinen Roboterfisch entwickelt, an dem kleinste Plastikteilchen haften bleiben. Mithilfe von Wärmestrahlung ließ sich der Prototyp aus einem speziellen Schichtmaterial durch das Wasser steuern, wie Forscher in der Fachzeitschrift „Nano Letters“ berichten.
Beim Bau ihres Roboterfischs ließen sich Xinxing Zhang und seine Kollegen von der Sichuan-Universität in China von dem natürlichen Material Perlmutt inspirieren, das in hauchdünnen Schichten die innere Schale einiger Muscheln bildet. Mit ebenso hauchdünnen Lagen aus Graphen und dem Kunststoff Polyurethan imitierten die Forscher den schichtweisen Aufbau von Perlmutt. Dank der unterschiedlichen Eigenschaften von Graphen und Polyurethan gelang es ihnen, ein leichtes und dennoch stabiles Material zu entwickeln, das zudem auf Wärmestrahlung reagierte.












                Roboterfisch
            



Aus diesem Schichtmaterial schnitten die Forscher knapp zwei Zentimeter lange Stücke in der Form kleiner Fische aus. Diese verteilten sie in einem mit Wasser gefüllten Behälter, in dem außerdem Mikroplastikpartikel schwammen. Traf nun Wärmestrahlung von außen in kurzen Impulsen auf die Fische, reagierte das Material mit einer mechanischen Bewegung, vergleichbar mit einem Flossenschlag. Mit einer Geschwindigkeit von etwa fünf Zentimetern pro Sekunde schwammen die Fische daraufhin durch das Wasser und sammelten dabei Mikroplastik auf, das an dem Material haften bleibt.
Die Forscher konnten zeigen, dass sich ihr neuartiges Material aus mehreren Graphen- und Kunststoffschichten dazu eignet, um kontrolliert Mikroplastik aufzusammeln. Als Anwendung wären nun mit Sensoren bestückte Roboterfische vorstellbar, die die Wasserqualität und auch die lokale Belastung mit Mikroplastik messen können. Zum Einsammeln großer Mengen Mikroplastik sind diese Roboterfische aber weniger geeignet. Effizienter und vor allem günstiger wären spezielle Wasserfilter, um selbst kleinste Mikroplastikpartikel zu entfernen.",Roboterfisch sammelt Mikroplastik
"Verglichen mit der Erde ist die Venus ein lebensfeindlicher Planet – mit etwa dem hundertfachen Druck und einer Temperatur um 450 Grad Celsius auf der Planetenoberfläche. Doch in der dichten Atmosphäre der Venus, in einer Höhe von 50 bis 60 Kilometern, herrschen eine Temperatur und ein Druck, die denen an der Erdoberfläche ähneln. Seit langem spekulieren Astrobiologen daher, ob dort nicht zumindest bakterielles Leben existieren könnte. Diese Hypothese musste ein Forschungsteam nun jedoch verwerfen. Der Stoffwechsel solcher Mikroben wäre nicht mit der beobachteten Zusammensetzung der Venusatmosphäre in Einklang zu bringen, so die Wissenschaftler im Fachblatt „Nature Communications“.
„Wir haben zwei Jahre damit zugebracht, die seltsame, auf Schwefel basierende Chemie in den Wolken der Venus zu erklären“, erläutert Paul Rimmer von der University of Cambridge. Denn insbesondere die Häufigkeit von Schwefeldioxid in der Atmosphäre der Venus ist rätselhaft. Auf der Erde stammt Schwefeldioxid hauptsächlich aus Vulkanen. Auch auf der Venus gibt es aktive Vulkane, die Schwefeldioxid in die Atmosphäre blasen könnten. Doch der Anteil dieses Gases ist lediglich in der unteren Wolkenschicht hoch – darüber nimmt er rapide wieder ab. Irgendein Prozess müsse dort also das Schwefeldioxid verbrauchen, so Rimmer und seine Kollegen.
„Leben ist ziemlich gut darin, eine seltsame chemische Zusammensetzung zu erzeugen. Deshalb haben wir nach Wegen gesucht, mithilfe von Bakterien die Beobachtungen zu erklären“, erläutert Rimmer. Die Idee der Forscher: Das Schwefeldioxid dient Bakterien in der temperierten Region der Atmosphäre als Nahrung und Energiequelle. Die Wissenschaftler stellten also eine Liste möglicher Stoffwechselreaktionen auf der Basis von Schwefeldioxid auf, um zu sehen, ob sich damit die Reduktion des Gases erklären ließe. Und damit waren sie zunächst erfolgreich: Bakterien könnten tatsächlich den Anteil an Schwefeldioxid reduzieren, sodass er mit zunehmender Höhe absinkt.
Doch der vermeintliche Erfolg hat einen Haken: Ein solcher Stoffwechsel produziert stets Ausscheidungen – andere Moleküle, die jedoch in der Venusatmosphäre nicht vorhanden sind. Somit mussten die Forscher ihre Hypothese wieder verwerfen. „Unsere Modelle zeigen, dass es nicht funktioniert“, so Rimmers Kollege Sean Jordan. „Sie verstoßen gegen alles, was wir über die Atmosphäre der Venus wissen.“
Damit bleibt das Rätsel des Schwefeldioxids in der Venusatmosphäre weiterhin ungelöst. Die Forscher wollen nun nichtbiologische Ansätze für die seltsame Chemie in den Wolken des Planeten untersuchen. Außerdem hoffen sie, ihre Methode schon bald auch auf Planeten bei anderen Sternen anwenden zu können. Mit dem neuen James-Webb-Weltraumteleskop beispielsweise könnten sich dort Schwefelverbindungen nachweisen lassen. „Das, was wir bei der Venus gelernt haben, können wir dann auch bei Exoplaneten anwenden“, so Rimmer.",Keine Lebenszeichen in der Atmosphäre der Venus
"Explodierende Sterne, verschmelzende Galaxien, Schwarze Löcher: Viele Objekte und Prozesse in unserem Universum sind extrem. Sie können Teilchen auf kaum vorstellbare Energien beschleunigen. Um diese Prozesse noch besser zu verstehen, beobachten Forscher unter anderem Novae, also explosive Ausbrüche in Doppelsternsystemen. Im Interview mit Welt der Physik berichtet Alison Mitchell von der Universität Erlangen-Nürnberg von einer Nova, die Teilchen sogar bis auf Geschwindigkeiten am theoretischen Limit bringt.
Welt der Physik: Was interessiert Sie an explodierenden Sternen?












                Alison Mitchell
            



Alison Mitchell: Uns interessiert der Ursprung hochenergetischer Teilchen im Kosmos. Wir vermuten, dass sie ihren Ursprung in Supernovae haben – also Sternexplosionen, bei denen ein massereicher Stern den Großteil seiner Masse ins All hinausschleudert, wobei sein Inneres kollabiert. Solche Supernovae beobachten wir allerdings in unserer Galaxie nicht so oft. Im Vergleich dazu beobachten wir viel häufiger kleinere Explosionen in Doppelsternsystemen, bei denen der Stern die Explosion überlebt  – sogenannte Novae. Wir gehen davon aus, dass sie ähnlich wie Supernovae funktionieren und aufgrund der Häufigkeit haben wir eine viel bessere Chance zu verstehen, was während so einer Explosion geschieht.
Was geschieht bei einer Nova?
Novae beobachten wir in Systemen aus typischerweise einem großen Stern und einem Weißen Zwerg, einem Überrest eines Sterns ähnlich unserer Sonne. Der Weiße Zwerg hat eine sehr hohe Anziehungskraft und zieht dadurch Materie von dem größeren Begleitstern zu sich hinüber. Die angehäufte Materie heizt sich langsam auf, bis sie so heiß ist, dass die Kernfusion einsetzt. Genau an dem Punkt erfolgt eine thermonukleare Explosion, das nennen wir Nova. Diese Explosion löst Schockwellen aus, die sich um das Doppelsternsystem ausbreiten und dabei hochenergetische Gammastrahlung erzeugen. Der Weiße Zwerg übersteht diesen Ausbruch, das heißt, eine Explosion kann wieder passieren. In einigen Systemen sehen wir Novae immer wieder. Auch RS Ophiuchi, die Nova, die wir letztes Jahr beobachtet haben, ist so ein wiederkehrendes System.
Bei dieser Nova haben Sie etwas Besonderes beobachtet. Was genau?
Die Nova war ungefähr hundertmal energetischer als alle Novae, die wir je zuvor beobachtet haben. Gammastrahlung umfasst ja einen ziemlich großen Energiebereich. Mit unseren Teleskopen können wir die sehr hochenergetische Gammastrahlung beobachten, die bei der Explosion frei wird. Die höchste Energie, die wir jetzt gemessen haben, lag bei 1 Teraelektronenvolt, das heißt, ein einziges Photon hat fast eine halbe Billion Mal mehr Energie als ein Photon im sichtbaren Licht.
Haben Sie das erwartet?












                Nova in einem Doppelsternsystem
            



Nicht unbedingt. Das ist die höchstmögliche Energie, die unsere Theorien für Novae vorhersagen. Höher kann die Energie theoretisch eigentlich nicht sein. Es ist das erste Mal, dass wir sehen, dass in der Natur diese Grenze – diese maximale Energie – tatsächlich erreicht wird.
Woher kommt dieses theoretische Limit?
Es kommt aus der Theorie der Teilchenbeschleunigung in einem Schock und wie oft sich ein Teilchen durch den Schock hin- und herbewegen kann. Immer wenn ein Teilchen in der Nähe des Schocks ist, gewinnt es Energie – bis die Energie so hoch ist, dass es vom Schock wegfliegt. Die Teilchen werden also im Schock beschleunigt und müssen erst auf Materie treffen, um Gammastrahlen zu erzeugen. Die Energie der erzeugten Gammastrahlung hängt von vielen Faktoren ab: unter anderem von der Geschwindigkeit, mit der sich der Schock nach der Explosion ausbreitet und der Dichte des umgebenden Mediums. Gammastrahlung bei 1 Teraelektronenvolt bedeutet, die Teilchen im Schock hatten eine Energie von 10 Teraelektronenvolt.
Warum ist es für die Forschung wichtig, dass Sie das theoretische Limit beobachtet haben?
Wir glauben, dass der Prozess der Teilchenbeschleunigung in Novae und Supernovae-Systemen derselbe ist. Wenn die Teilchenbeschleunigung in Supernovae genauso effizient funktioniert, unterstützt das unsere Lieblingshypothese, nämlich dass Supernovaexplosionen verantwortlich für den Ursprung der kosmischen Strahlung sind. Bei RS Ophiuchi konnten wir zum ersten Mal den zeitlichen Verlauf des Ausbruchs verfolgen. Wir konnten den Beschleunigungsprozess in Echtzeit beobachten – praktisch wie in einem Film. In früheren Beobachtungen hat das Signal dafür nicht ausgereicht. Wir wissen jetzt also für eine Nova, dass die theoretische Grenze der erzeugten Energie erreichbar ist. Die Frage ist: Ist das ein seltener Fall oder passiert es ziemlich oft in der Natur? Wir hoffen natürlich, dass es nicht so selten ist.
Was passiert als nächstes?
Teilweise abwarten und auf neue Beobachtungen hoffen. Aber wir sitzen natürlich nicht nur herum und warten, denn wir können schon vieles aus den bereits existierenden Daten lernen. Zum Beispiel könnte es sein, dass es in früheren Novae schon ähnlich stark beschleunigte Teilchen gab, unser Experiment aber nicht sensibel genug war, um diese zu messen. Wir wollen verstehen, ob wir diesen Fall ausschließen können. Und natürlich wollen wir weitere Novae beobachten. Vor etwa 7 Jahren haben wir das Nova-Programm innerhalb der H.E.S.S-Kollaboration gestartet. Mit dem Programm entscheiden wir, welche Novae wir mit den H.E.S.S-Gammastrahlenteleskopen in Namibia beobachten wollen. Dafür gibt es eine Reihe von Kriterien. Etwa ein- bis zweimal im Jahr beobachten wir Novae, die mindestens ein Kriterium erfüllen. RS Ophiuchi war die erste Nova, die alle Kriterien gleichzeitig erfüllt hat.












                H.E.S.S. Observatorium in Namibia
            



Wie erfahren Sie rechtzeitig, dass gerade eine Nova ausbricht?
Es gibt verschiedene Netzwerke für astronomische Mitteilungen, sogenannte Alerts. Die erste Meldung zu RS Ophiuchi kam von Hobbyastronomen, die einen Stern in der Nacht vom 8. auf den 9. August 2021 entdeckten, der plötzlich viel heller war als sonst. Da Novaereignisse nicht nur Gammastrahlen, sondern auch Licht im sichtbaren Bereich erzeugen, konnten sie RS Ophiuchi während des Ausbruchs mit einem einfachen Fernglas und sogar mit bloßem Auge erkennen. Innerhalb von 24 Stunden nach der Meldung haben wir mehrere Teleskope auf das Sternsystem gerichtet. Mit den H.E.S.S-Teleskopen beobachten wir die sehr hochenergetische Gammastrahlung, die erst einige Tage nach dem größten Signal im sichtbaren Bereich ihr Maximum erreicht. Wir haben also die Chance, auch ein paar Tage nach dem ersten Alert noch etwas zu messen.
Was erwarten Sie von zukünftigen Beobachtungen?
Zunächst wollen wir unsere Beobachtungskampagnen weiter verbessern und natürlich weitere Novae sehen. Bald könnte das Cherenkov-Telescope-Array CTA uns helfen, schneller ans Ziel zu kommen. Die Teleskopanlage befindet sich zurzeit im Bau und wird in Zukunft an zwei Standorten auf La Palma und in den chilenischen Anden stehen. Dadurch werden wir dann zeitgleich einen größeren Teil des Himmels beobachten. Zurzeit verpassen wir Ereignisse, wenn beispielsweise ein Novaausbruch tagsüber stattfindet. Mit unseren Beobachtungen vergangenes Jahr haben wir also auch Glück gehabt. Von zukünftigen Instrumenten dürfen wir sicher mehr Leistung erwarten, aber es wird wohl auch immer ein wenig Glück im Spiel bleiben.",„Wir erreichen das theoretische Limit“
"Hurrikan, Taifun oder Zyklon – je nach Meeresregion tragen tropische Wirbelstürme andere Namen. Doch überall verursachen sie massive Schäden, sobald sie vom Meer auf bewohnte Küstengebiete treffen. Wie sich die Anzahl und die Stärke der gewaltigen Stürme in Zeiten des Klimawandels verändern, wird unter Klimaforschern nach wie vor intensiv und teils kontrovers diskutiert. Nun ermittelte eine Arbeitsgruppe, dass zumindest die Anzahl der Wirbelstürme im Lauf des 20. Jahrhunderts abgenommen hat. In der Fachzeitschrift „Nature Climate Change“ veröffentlichten sie ihre Analyse, die sowohl auf Satellitendaten als auch für die Zeit ab 1850 auf aufwendigen Simulationen beruht.
Seit den 1970er-Jahren werden tropische Wirbelstürme mit Satelliten für die Erdbeobachtung immer genauer aufgezeichnet und analysiert. Doch für die Jahrzehnte davor bis hin zu vorindustriellen Zeiträumen ab 1850 liegen solche Daten nicht vor. Daher griffen Savin S. Chand von der Federation University Australia und seine Kollegen für diese Zeitspanne auf ältere verfügbare Daten wie Meerestemperaturen und Wetterbeobachtungen zurück. Mit komplexen Simulationen konnten sie aus diesen Datensätzen auf die wahrscheinliche Anzahl von Wirbelstürmen über dem Pazifik, dem Atlantik und dem Indischen Ozean zurückschließen.
Ihre Analyse zeigte einen klaren, wenn auch überraschenden Trend für die Anzahl der Wirbelstürme: So nahm sie im Lauf des gesamten 20. Jahrhunderts und bis zum Jahr 2012 im Vergleich zur vorindustriellen Zeit zwischen 1850 und 1900 um etwa 13 Prozent ab. Dieser Abwärtstrend war für die Jahrzehnte ab 1950 sogar mit einem Rückgang von 23 Prozent noch deutlicher ausgeprägt. Für diese Entwicklung machen die Forscher die veränderten Bedingungen in der untersten Schicht der Erdatmosphäre – der sogenannten Troposphäre – verantwortlich: Durch die Erderwärmung schwächten sich großräumige Zirkulationssysteme wie die Hadley- oder Walker-Zirkulation in tropischen Regionen um den Äquator ab. Damit verringerten sich auch Ströme in den Tropenregionen, die feuchte und warme Luft von der Meeresoberfläche nach oben transportieren, sowie die Luftfeuchtigkeit in der mittleren Troposphäre – beides sind Voraussetzungen für die Bildung von Wirbelstürmen.
Diese umfassende Untersuchung legt nahe, dass die Anzahl tropischer Wirbelstürme im Zuge der Erderwärmung auf mittlerweile knapp mehr als ein Grad tatsächlich abgenommen hat und auch noch weiter abnehmen könnte. Allerdings ist dies kein Zeichen für eine Entwarnung. Denn parallel zeigen die vergangenen Jahrzehnte, dass der Anteil stärkerer Wirbelstürme zunimmt. Zudem verlagern sich die Stürme in Richtung der Erdpole und treffen auf Küsten, die immer weiter vom Äquator entfernt sind. Dort sind die Städte und die Bewohner jedoch schlechter auf diese Extremwetterereignisse vorbereitet, so dass Wirbelstürme trotz einer Abnahme ihrer Anzahl durchaus immer stärkere Schäden verursachen könnten.",Anzahl der Wirbelstürme ging zurück
"In vielen Farben schillern die Flügel bunter Schmetterlinge. Doch dafür sind nicht etwa Farbstoffe verantwortlich, sondern filigrane Strukturen: Sie reduzieren oder verstärken die Reflexion bestimmter Farben. Diesen Effekt übertrugen Forscher nun auf kleine Objekte aus dem 3D-Drucker. Sie stellten verschiedenartige, bunt schillernde Objekte her, wie sie in den Fachzeitschriften „Proceedings of the National Academy of Sciences“ und „Advanced Functional Materials“ berichten. Lediglich in den jeweiligen Rezepten für die Druckertinte unterscheiden sich die beiden Ansätze.
Entscheidend für die schillernden Strukturen ist ein bestimmter Effekt: Für diesen müssen die Abmessungen der winzigen Strukturen etwa so groß wie die jeweils reflektierten Wellenlängen des Lichts sein. Fällt sichtbares Licht auf die Objekte, werden dann nur bestimmte Farben reflektiert, andere dagegen nicht. Materialien, die basierend auf diesem Effekt farbig erscheinen, heißen auch Strukturfarben.
Als ein solches Material wählten Luoran Shang und ihre Kollegen von der Fudan-Universität in Shanghai Flüssigkristalle, die sich einerseits wie eine Flüssigkeit verhalten und andererseits auch eine strenge Anordnung der Moleküle wie in Kristallen aufweisen. Die Forscher verwendeten hierzu lange, stäbchenförmige Moleküle, die sich zu winzigen Spiralen anordnen. Diese Flüssigkristalle vermischten sie mit Gelatine und speziellen Kunststoffen, um eine flüssige, druckbare Tinte zu erhalten. Zusätzlich fügten sie Nanoröhrchen aus Kohlenstoff hinzu, die einfallendes Licht gut absorbieren können. Damit lässt sich nach Aussage der Forscher der Farbeffekt der gedruckten Strukturfarbe verstärken.
In mehreren Testläufen druckten Shang und ihre Kollegen zahlreiche kleine Objekte von Spiralen und Pyramiden bis hin zu einem winzigen Schmetterling. Je nach Zusammensetzung der Druckertinte ließen sich so Färbungen von Violett über Gelb und Grün bis Rot erreichen. Zudem veränderten die Objekte ihre Farben je nach Temperatur und Feuchtigkeit. Dank dieser Eigenschaft wären die gedruckten Objekte sogar als einfache Sensoren geeignet, die durch ihre Farbe beispielsweise anzeigen, wie warm oder trocken es in der Umgebung ist.
Auch die Arbeitsgruppe um Silvia Vignolini von der University of Cambridge nutzte für ihre Druckertinte ähnliche Strukturfarben auf der Basis von Flüssigkristallen. Sie fügten zwar keine Nanoröhrchen hinzu, was die gedruckten Objekte weniger stark farbig schillern ließ. Allerdings nutzte Vignolini nur Wasser als Zusatz für ihre Druckertinte. Damit ist ihre Tinte vollständig biologisch abbaubar und umweltfreundlicher.",Ohne Farbstoffe – und doch bunt schillernd
"Mit den hochbrillanten Röntgenstrahlen der Synchrotronquelle ESRF in Grenoble wird es erstmals möglich sein, auch große menschliche Organe und sogar den ganzen Körper in bislang unerreichter Präzision zu durchleuchten. Dieses Projekt mit dem Namen „Human Organ Atlas“ ist mit der Lunge von Covid-19-Opfern gestartet und soll nun auch über andere Organe Aufschluss bringen. Im Interview mit Welt der Physik sprechen Peter Lee vom University College London und Simon Zabler vom Fraunhofer-Institut für Integrierte Schaltungen in Deggendorf über die Hintergründe der neuen Technologie.
Welt der Physik: Welche Röntgentechniken gibt es, um menschliche Organe zu untersuchen?












                Peter Lee
            



Peter Lee: Es gibt heute verschiedene Methoden, um mit Röntgenstrahlung den menschlichen Körper oder einzelne Teile von ihm zu untersuchen. Je nachdem, wie groß das zu untersuchende Gewebe ist, lassen sich dabei unterschiedliche Auflösungen erzielen. Für die klinische Anwendung etwa sind Computertomografen, also CT, das Mittel der Wahl. Sie erlauben aber nur gröbere Auflösungen von etwa einem halben Millimeter bei Organen von lebenden Patienten. Bei totem Gewebe kann man mit stärkeren Röntgenstrahlen arbeiten und erhält dann eine Auflösung von rund einem zehntel Millimeter. Unsere neue Röntgentechnik gibt uns die Möglichkeit, den gesamten Körper mit einer Genauigkeit von rund zwanzig Mikrometern zu untersuchen. Außerdem lässt sich nach Belieben in einzelne Bereiche hineinzoomen – lokal ebenfalls mit einer Präzision von unter einem Mikrometer.
Und damit wollen Sie eine Art Landkarte des menschlichen Körpers erstellen?
Ich muss zunächst dazu sagen, dass unsere Methode nicht für den klinischen Einsatz am lebenden Patienten geeignet ist. Dafür sind die von uns genutzten Röntgenstrahlen viel zu stark und gefährlich. Unsere Forschungsgruppe nutzt Organe oder Körper von Verstorbenen, die wir für die medizinische Forschung untersuchen. Mit unserem Projekt, dem Human Organ Atlas, werden wir ein vielfach genaueres Verständnis von der Funktionsweise des menschlichen Körpers und von verschiedenen Krankheiten erlangen können, als es mit anderen Techniken möglich ist. Denn durch die hochaufgelösten Aufnahmen von ganzen Organen oder sogar dem ganzen Körper lassen sich systemische Zusammenhänge besser erkennen – wie zum Beispiel der Blutkreislauf oder die Nervenbahnen zwischen und innerhalb von Organen miteinander verbunden sind. Dadurch lassen sich auch Pathologien viel besser erfassen.
Welche Arten von Gewebe haben Sie denn bislang schon untersucht?
Wir haben die Lunge von Patienten untersucht, die an einer Covid-19-Infektion verstorben sind. Dabei haben wir starke Hinweise gefunden, dass das vaskuläre System sich bei einer starken Erkrankung bei vielen Patienten verändert hat – dass sich also die feinen Blutgefäße sozusagen falsch verbunden haben, und zwar über große Teile des Lungengewebes hinweg. Diese Erkenntnis ließ sich mit den herkömmlichen Methoden nicht gewinnen. In Zukunft wollen wir aber insbesondere das Gehirn und das Nervensystem sowie das Herz untersuchen. Das wird uns hoffentlich Aufschluss über neurodegenerative Krankheiten oder Herzerkrankungen geben.",„Genaueres Verständnis des menschlichen Körpers“
"Knapp eine Milliarde Jahre nach dem Urknall leuchteten im jungen Kosmos bereits die ersten Quasare, also aktive Galaxienkerne, auf. Doch wie diese supermassereichen Schwarzen Löcher in – astronomisch gesehen – so kurzer Zeit entstanden sind, war bislang unklar. Mithilfe von Computersimulationen fanden Astronomen nun eine mögliche Erklärung: Zunächst verdichteten sich Ströme aus kaltem, turbulentem Gas in den entstehenden Galaxien zu den ersten Schwarzen Löchern mit zehn- bis hunderttausend Sonnenmassen. Wie die Wissenschaftler im Fachblatt „Nature“ berichten, wuchsen diese Objekte dann zu den hell leuchtenden, supermassereichen Schwarzen Löchern an.
Im heutigen Kosmos beherbergt nahezu jede Galaxie in ihrem Zentrum ein supermassereiches Schwarzes Loch mit der millionen- bis milliardenfachen Masse unserer Sonne. Zunächst dachten Astronomen, die Schwarzen Löcher würden im Lauf der kosmischen Geschichte mehr oder weniger gleichmäßig an Masse zunehmen, da sie aufgrund ihrer enormen Schwerkraft Materie aus ihrer Umgebung anziehen. Diese Vorstellung musste jedoch korrigiert werden, als Forscher zahlreiche Quasare im jungen Kosmos aufspürten. Denn auch bei Quasaren handelt es sich um supermassereiche Schwarze Löcher, in die Materie einströmt und sich dabei erhitzt – deshalb leuchten Quasare heller als die Galaxien, in deren Zentren sie stehen.
„Die Voraussetzung für das Entstehen von Quasaren ist, dass es bereits Schwarze Löcher mit zehn- bis hunderttausend Sonnenmassen gibt. Für deren Entstehung gab es bislang keine schlüssige Erklärung“, erläutern Muhammad Latif von der Universität der Vereinigten Arabischen Emirate und seine Kollegen. Zwar gibt es bestimmte Bedingungen, unter denen diese Schwarzen Löcher entstehen können, doch sie treten viel zu selten auf, um die Häufigkeit der Quasare im jungen Kosmos zu erklären. Nun gelang es den Astronomen allerdings, mithilfe von hochauflösenden Computersimulationen das Entstehen der ersten Schwarzen Löcher zu erklären: Während die Galaxien im jungen Kosmos entstanden, strömte kühles Gas in sie ein und führte dort zu starken Turbulenzen. Diese Verwirbelungen verhinderten, dass sich aus dem Gas, wie sonst üblich, Sterne bildeten.
Erst als das angesammelte kühle Gas eine Masse von 30 000 bis 40 000 Sonnenmassen erreichte, kollabierte die dichte Gaswolke unter ihrer eigenen Schwerkraft und bildete ein entsprechend großes Schwarzes Loch. Diese Objekte dienten dann als Ausgangspunkte für die Quasare, berichten Latif und seine Kollegen. Zudem zeigte sich, dass dieser Prozess häufig genug auftritt, um auch die Anzahl der Quasare zu erklären. „Die ersten Quasare waren also eine natürliche Konsequenz der Strukturbildung im jungen Kosmos“, so die Forscher.",Wie Quasare im jungen Kosmos entstanden
"Auf seiner Umlaufbahn um die Erde kommt uns der Mond mehrmals im Jahr besonders nahe. Dies verspricht einen besonders großen und hellen Vollmond am Nachthimmel. Doch obwohl dieser Effekt mit bloßem Auge kaum wahrnehmbar ist, bringt der Supermond so manchen Beobachter ins Staunen. Tatsächlich liegt das aber eher an einem anderen Effekt.
Immer wieder ist von ihm die Rede: dem Supermond. Ungewöhnlich groß soll der Erdtrabant zu bestimmten Zeitpunkten im Jahr am Himmel erscheinen. Und wer dann – gutes Wetter vorausgesetzt – nach draußen geht und nach dem Mond schaut, wird vielleicht wirklich erstaunt sein. Geradezu majestätisch schwebt der Vollmond über dem Horizont.
Wenn der Mond uns besonders nahe ist
Tatsächlich ist „Supermond“ weder ein historisch geprägter noch ein astronomischer oder allgemein wissenschaftlicher Begriff. Vielmehr war es ein Astrologe, Richard Nolle, der 1979 erstmals über einen Supermond schrieb, der für Erdbeben und Vulkanausbrüche verantwortlich sein sollte – nämlich immer dann, wenn sich der Erdtrabant bei Vollmond oder Neumond besonders nah an der Erde befindet.
Doch auch wenn Nolles Überlegungen nicht auf wissenschaftlichen Grundlagen beruhen – ganz von der Hand zu weisen sind sie nicht. Denn die Gezeitenkräfte von Mond und Sonne führen nicht nur zu Ebbe und Flut, sondern auch zu Bewegungen in der Erdkruste. Stehen Sonne, Erde und Mond nahezu in einer geraden Linie, so addieren sich die Gezeitenkräfte von Sonne und Mond und es kommt zu besonders starken Springtiden. Die Hochwasser, umgangssprachlich häufig als Springfluten bezeichnet, fallen etwa 20 Prozent höher als gewöhnliche Hochwasser aus. Tatsächlich treten parallel zu Springfluten auch vermehrt starke Erdbeben auf, wie etwa ein Team um den Geologen Satoshi Ide 2016 anhand der Analyse von über 10 000 Erdbeben zeigte.
Da der Mond sich aber nicht auf einer Kreisbahn, sondern auf einer deutlich elliptischen Bahn um die Erde bewegt, fallen nicht alle Springfluten gleich aus. Der Abstand des Erdtrabanten von unserem Planeten variiert zwischen 356 400 und 406 700 Kilometern. Durchaus naheliegend also, dass die Auswirkungen der Gezeiten stärker sind, wenn sich der Mond bei einer Springflut zusätzlich in Erdnähe befindet. Allerdings ist dieser Einfluss sehr gering: Gerade einmal rund zwei Prozent stärker sind Springfluten im Durchschnitt, wenn der Mond sich in Erdnähe befindet. Entsprechend ließ sich ein Zusammenhang zwischen dem Supermond und Erdbeben bislang nicht nachweisen.
Größer und heller












                Supermond und Minimond im Vergleich
            



Gleichwohl, der Begriff des Supermonds wurde in den vergangenen Jahren vermehrt aufgegriffen – allerdings anders als ursprünglich von Nolle gemeint: Heute bezieht sich die Bezeichnung nicht mehr auf vermeintliche Auswirkungen des nahen Monds auf Erdbeben oder Vulkanausbrüche. Stattdessen ist lediglich ein Vollmond in erdnaher Position und die damit einhergehende Größe und Helligkeit des Erdtrabanten am Nachthimmel gemeint. Denn wenn der Mond in Erdnähe ist, so ist der scheinbare Durchmesser des Monds um bis zu 14 Prozent größer als der eines Monds in Erdferne – einem sogenannten Minimond. Und mit der größeren Fläche, die der Supermond am Nachthimmel einnimmt, steigt auch die wahrgenommene Helligkeit des Erdtrabanten. Um bis zu 30 Prozent wirkt der Mond in Erdnähe heller als in Erdferne.
Das allerdings sind Extremwerte. Die Umlaufbahn des Monds ist keineswegs unveränderlich, sondern variiert aufgrund der Anziehungskräfte der Sonne und der großen Planeten. So schwankt der Abstand des Monds von der Erde im Perigäum, dem erdnächsten Punkt des jeweiligen Orbits, zwischen 356 400 und 370 400 Kilometern. Die theoretisch möglichen Extremwerte treten daher nur selten auf. Das wirft die Frage auf, ab wann ein Vollmond als Supervollmond gilt. Nolle selbst hat seine Definition im Laufe der Jahrzehnte immer wieder geändert – aktuell verwendet er einen willkürlich gewählten Abstand von weniger als 368 630 Kilometern bei Vollmond als Limit.
Nur ein naher Vollmond ist auch ein Supermond













                Abstände des Monds von der Erde bei einem Supermond und einem Minimond
            



Damit gibt es jeweils zwei bis vier Supermonde pro Jahr. Am 14. Juni 2022 war der Erdtrabant 357 658 Kilometer von der Erde entfernt. Beim darauffolgenden Vollmond am 13. Juli steht der Mond mit 357 418 Kilometern der Erde sogar noch etwas näher – nur einem Monat nach dem Vollmond im Juni kommt es nun also schon zum nächsten Supermond.
Oft wird als weiteres Kriterium herangezogen, dass der exakte Zeitpunkt des Vollmonds und der exakte Zeitpunkt der Erdnähe in ein und dieselbe Nacht fallen. Doch diese Definition hängt vom Ort des Beobachters ab.
Eine lohnenswerte Beobachtung?












                Vollmond
            



Ob ein bestimmter Vollmond also „super“ ist oder nicht, darüber lässt sich folglich streiten. Auch eine besondere wissenschaftliche Bedeutung hat das Phänomen „Supermond“ nicht. Lohnt sich aber nicht trotzdem der Blick zum Erdtrabanten, wenn er uns besonders nahe steht? Schließlich sind die Unterschiede in scheinbarer Größe und Helligkeit durchaus beachtlich. Tatsächlich sind jedoch am Nachthimmel mit bloßem Auge die Unterschiede zwischen einem normalen Vollmond und einem Supermond aus Mangel an Vergleichsobjekten kaum zu erkennen. So erscheint ein hoch am Himmel stehender Supermond dem Beobachter nicht viel anders als jeder andere Vollmond.
Dennoch gehen viele Menschen, motiviert durch Berichte über den Supermond, in den Abendstunden nach draußen und sind erstaunt über die Größe des Erdtrabanten, wenn sie ihn kurz nach seinem Aufgang in Horizontnähe sehen. Die scheinbar enorme Größe des Monds am Horizont beruht jedoch auf einem ganz anderen Phänomen: Eine optische Täuschung lässt den Mond nahe am Horizont größer erscheinen, als wenn sie hoch am Himmel stehen. Mit dem Supermond hat das also nichts zu tun.",Was macht den Mond zum Supermond?
"Im Zusammenhang mit der Covid-19-Pandemie erlangten Aerosole eine eher berüchtigte Bekanntheit – als Träger von Virusmaterial. Feinstaub hat einen ähnlich schlechten Ruf. Dabei umfassen Aerosole einfach alle festen und flüssigen Schwebeteilchen in der Luft. Und nicht alle sind gesundheitsschädlich. Ina Tegen vom Leibniz-Institut für Troposphärenforschung in Leipzig erklärt in dieser Folge des Podcasts, warum manche Aerosole schädlich und andere durchaus nützlich sein können.












                Ina Tegen
            



Auch wenn eine frische Brise weht und die Luft absolut rein wirkt, finden sich unzählige winzige Partikel darin – Sandkörnchen, Vulkanasche, Pollen oder Meersalz beispielsweise. All diese winzigen Teilchen unterscheiden sich hinsichtlich ihrer Herkunft, ihrer chemischen Zusammensetzung und ihrer physikalischen Beschaffenheit. Und doch werden sie alle unter dem Begriff „Aerosole“ zusammengefasst.Ina Tegen: „Aerosole sind luftgetragene Partikel – ein Gemisch aus verschiedenen partikulären Substanzen, die sowohl fest als auch flüssig sein können. Und wir finden sie in Größenbereichen von mehreren Nanometern bis mehreren Mikrometern, also millionstel bis tausendstel Millimeter.“Die Partikelgröße ist entscheidend, wenn es um die Luftqualität beziehungsweise die gesundheitlichen Risiken von Feinstaub geht. Denn Partikel, die kleiner sind als zweieinhalb Mikrometer, können tief in die Lunge eindringen, noch kleinere Staubteilchen sogar bis ins Blut. Für Städte gibt es daher gesetzlich festgelegte Grenzwerte für Feinstaub unterschiedlicher Größe. Mehr dazu in der 335. Folge.",Aerosole
"Kleinste Objekte lassen sich etwa mithilfe elektrischer Ladungen, magnetischer Anziehung oder durch Laserstrahlen gezielt bewegen, ohne direkt berührt zu werden. Nun gelang dieses Kunststück sogar mit Schallwellen: Damit versetzte eine Gruppe von Physikern winzige, an den Objekten angebrachte Luftblasen in Schwingung und brachte die Objekte infolgedessen in die gewollte Position. Wie sie in der Fachzeitschrift „Physical Review Letters“ berichtet, ließe sich dieses Phänomen weiterentwickeln und in Produktionsanlagen oder in der Robotik einsetzen.
Mit dem Druck von Schallwellen lassen sich schon seit einigen Jahren winzige Plastikkügelchen oder Bakterien durch eine Flüssigkeit steuern. Doch für größere Objekte sind die dabei wirkenden Kräfte zu klein. Dieses Problem umgingen Athanasios Athanassiadis und seine Kollegen vom Max-Planck-Institut für Intelligente Systeme in Stuttgart mit einem neuen Experiment: In zwei flachen Silikonscheiben von der Größe einer Ein-Cent-Münze formten sie jeweils 121 zylindrische Mulden – jede mit einem Durchmesser von etwa 0,3 Millimetern. Eine der Scheiben deponierten die Forscher mit der Muldenseite nach unten auf den Boden einer kleinen, mit Wasser gefüllten Petrischale. Die zweite Scheibe legten sie ebenfalls mit der Muldenseite nach unten auf die Wasseroberfläche. In jeder der Mulden befand sich außerdem eine kleine Luftblase.
Danach versetzten die Forscher die Luftblasen durch Schallwellen mit einer Frequenz von 3300 Hertz in Schwingung. Dadurch schwankte auch der Luftdruck im Inneren der Luftblasen. Daraufhin bildeten sich zwischen den gegenüberliegenden Luftblasen Anziehungskräfte – sogenannte Bjerknes-Kräfte – aus. Sie lassen die Druckdifferenzen sich schnell wieder ausgleichen. „Diese Anziehungskräfte können genutzt werden, um Objekte von der Größe weniger Zentimeter zu bewegen“, sagt Athanassiadis.
Tatsächlich gelang es so, die Scheiben gezielt zu bewegen: Innerhalb von ein bis zwei Sekunden bewegte sich die schwimmende Muldenscheibe direkt über die zweite Scheibe am Boden und verharrte genau über dieser. In einem zweiten Experiment ordneten die Forscher die Mulden und damit die Luftblasen asymmetrisch an. Abermals wurde die schwimmende Scheibe über die Scheibe am Boden gezogen. Doch diesmal verharrte sie nicht an einer Position, sondern drehte sich wegen der unregelmäßig wirkenden Anziehungskräfte über der Bodenscheibe.
„Wir waren sehr überrascht, wie stark der Effekt der Luftblasen war“, sagt Athanassiadis. Er ist davon überzeugt, dass dieser Effekt vielversprechend für neue Anwendungen in der Mikrorobotik und für Produktionsprozesse ist. In weiteren Versuchen wollen die Forscher überprüfen, wie sich die Anordnung der Luftblasen und deren Größe auswirken könnte. Dabei haben sie auch kleinere Objekte im Fokus, um diese über die Anziehungskräfte zwischen den Luftblasen kontrolliert anzuordnen.",Vibrierende Luftblasen ziehen sich an
"Das James-Webb-Weltraumteleskop erlaubt einen bisher unerreichten Blick in den Kosmos: Mit verschiedenen Instrumenten an Bord beobachtet das Observatorium infrarotes Licht, das etwa von den ersten Sternen und Galaxien im Universum stammt oder durch die Atmosphären ferner Planeten drang. In den kommenden Jahren versprechen die gesammelten Daten viele neue Erkenntnisse und spannende Entdeckungen.
Am 25. Dezember 2021 startete das James-Webb-Teleskop ins Weltall, um den Himmel im Nah- und Mittelinfrarotbereich zu beobachten – also bei Wellenlängen, die etwas länger sind als im Fall des sichtbaren Lichts. Verglichen mit dem Hubble-Weltraumteleskop ist es dabei hundertfach empfindlicher und kann so auch sehr leuchtschwache Objekte aufspüren. Dazu zählen beispielsweise Galaxien im frühen Universum, die bereits kurz nach dem Urknall entstanden. Ebenso lässt sich die Entstehung und Entwicklung von Sternen und extrasolaren Planeten mit dem neuen Weltraumteleskop verfolgen. Und auch unser eigenes Sonnensystem soll es in den Blick nehmen.
Um den infraroten Wellenlängenbereich ungestört beobachten zu können, muss das Teleskop selbst extrem kalt sein: Die Betriebstemperatur liegt bei weniger als minus 230, teils sogar minus 260 Grad Celsius. Andernfalls würde die eigene Wärmestrahlung nämlich die Signale aus dem Kosmos überlagern. Ein riesiger Sonnenschutzschild – etwa so groß wie ein Tennisplatz – schirmt die Instrumente von dem Licht und der Wärme der Sonne ab. Zusätzlich sorgt ein Kühlsystem an Bord für die nötige Kälte.
Neben leistungsstarken Kameras setzen Astronomen auch Spektrografen ein, um das infrarote Weltall zu erkunden. Diese Instrumente spalten das eintreffende Licht wie ein Prisma in seine verschiedenen Wellenlängen auf und liefern so wertvolle Informationen über die Quellen des Lichts. So lässt sich etwa die chemische Zusammensetzung von Planetenatmosphären untersuchen, wenn die fernen Welten vor ihrem Stern vorbeiziehen. Moleküle wie Wasser, Kohlendioxid, Methan oder Sauerstoff in ihren Atmosphären würden sich im Spektrum als charakteristische Linien zeigen.
Der Primärspiegel des Teleskops hat einen Durchmesser von 6,5 Meter und besteht aus 18 sechseckigen Segmenten, jeweils überzogen mit einer mikroskopisch dünnen Goldschicht. Infrarotlicht wird so besonders gut reflektiert und kann anschließend zu den verschiedenen Instrumenten an Bord geleitet werden. Benannt ist das James-Webb-Weltraumteleskop übrigens nach James Edwin Webb, der von Februar 1961 bis Oktober 1968 die US-Raumfahrtbehörde NASA leitete. Im Juli 2022 wurden die ersten Farbbilder und spektroskopischen Daten des Teleskops veröffentlicht.",James-Webb-Teleskop
"In der Luft wirbelnde Staubkörnchen können selbst harte und stabile Oberflächen im Laufe der Zeit schädigen. Unter dieser Erosion leiden etwa die Rotorblätter von Windrädern, Flugzeugflügel oder Solarmodule in Wüstenregionen. Nicht nur die Art der Staubkörner und die Windgeschwindigkeiten beeinflussen dieses schmirgelnde Verhalten, sondern auch die zwischen den Staubkörnern wirkenden Haftkräfte spielen dabei eine Rolle. Zu diesem Ergebnis kamen Wissenschaftler nun mithilfe eines neu entwickelten Experiments. Wie sie in der Fachzeitschrift „Physical Review Fluids“ berichten, lassen sich damit in Zukunft womöglich bessere Reinigungsmethoden für von Erosion gefährdete Oberflächen entwickeln.
Um Erosionseffekte durch Staub zu untersuchen, wurden im Labor bisher meist winzige Glaskügelchen genutzt, die aber nicht zu größeren Konglomeraten zusammenballen. Dadurch spielten die in der Natur wirkenden Haftkräfte – beispielsweise zwischen feuchten Staubkörnchen – bei diesen Experimenten keine Rolle. Das änderten Alban Sauret von der University of California in Santa Barbara und seine Kollegen nun allerdings mit einem neuen Experiment: Dafür beschichteten sie zunächst knapp einen Millimeter große Glaskügelchen mehrmals mit einem Silikon. Je dicker diese Schichten waren, desto besser hafteten die Glaskügelchen aneinander.












                Aufbau des Experiments
            



In einem nächsten Schritt schichteten die Forscher die Kügelchen aufeinander und glätteten die Oberfläche des groben Pulvers. Auf diese Oberfläche richteten sie einen senkrecht einströmenden Luftstrahl mit veränderbarer Strömungsgeschwindigkeit. Das Ergebnis: Je stärker die Haftkräfte zwischen den Kügelchen waren, desto schnellere Luftströmungen waren nötig, um Erosionseffekte auf der Oberfläche zu verursachen. Dabei bildeten sich sogar kleine Krater, die sich mithilfe einer Lasermethode nachweisen ließen. Die Tiefe dieser Krater nahm mit steigender Strömungsgeschwindigkeit und abnehmenden Haftkräften zu.
Mit diesem Grundlagenexperiment lässt sich nun der Einfluss der Haftkräfte zwischen Staubkörnchen genauer analysieren. „Damit können wir vorhersagen, wie die Haftkräfte eine beginnende Erosion verändern“, sagt Sauret. Konkrete Anwendungen erwartet er beispielsweise bei der Reinigung von Solarmodulen, die oftmals mit aneinanderhaftenden Staubkörnchen verschmutzt sind. So ließe sich die Luftströmung aus Reinigungsdüsen optimieren, um die Staubschicht möglichst schonend von den Solarmodulen zu entfernen. „Das ist besonders für trockene Regionen mit Wassermangel von großem Interesse“, so Sauret.",Schmirgelnder Staub
"In der Großen Magellanschen Wolke – einer Nachbargalaxie der Milchstraße – hat ein Team von Astronomen ein stellares Schwarzes Loch entdeckt, das keine Strahlung abgibt. Es ließ sich nur aufgrund seiner Anziehungskraft aufspüren und ist das erste bekannte „ruhige“ Schwarze Loch außerhalb der Milchstraße. Zudem sei dieses Schwarze Loch offenbar nicht bei einer Sternexplosion, sondern durch den direkten Kollaps eines Sterns entstanden, so die Wissenschaftler im Fachblatt „Nature Astronomy“.
Astronomen vermuten, dass es in der Milchstraße und den umliegenden Nachbargalaxien – der lokalen Gruppe – Milliarden stellarer Schwarzer Löcher gibt. Stellare Schwarze Löcher sind die Überreste von großen Sternen mit mehr als der 15-fachen Masse unserer Sonne. Wenn ein solcher Stern seinen nuklearen Brennstoff aufgebraucht hat, kollabiert er unter seiner eigenen Anziehungskraft zu einem Schwarzen Loch. In der Regel geht dieser Kollaps mit einer Explosion, auch Supernova genannt, einher, wobei die äußere Hülle des Sterns ins All hinausgeschleudert wird.
Tatsächlich bekannt sind bislang allerdings nur wenige stellare Schwarze Löcher – und zwar solche, die mit einem Stern in ihrer Nähe um ein gemeinsames Zentrum kreisen. Oft saugen die Schwarzen Löcher in solchen Doppelsystemen mit ihrer starken Schwerkraft gasförmige Materie von ihrem Partnerstern ab. Dabei heizt sich das Gas auf und sendet hochenergetische Röntgenstrahlung aus, die Forscher mit Teleskopen beobachten können und somit das Schwarze Loch aufspüren.
Neben solchen leuchtenden Schwarzen Löchern sollte es aber auch viele ruhige Schwarze Löcher geben, in die keine Materie einströmt. Für die Suche nach dieser Art von Schwarzen Löchern haben Tomer Shenar von der Universität Löwen in Belgien und seine Kollegen knapp tausend massereiche Sterne in der Region des Tarantelnebels in der Großen Magellanschen Wolke ins Visier genommen. Und das Team wurde fündig: Als sie den Stern namens VFTS 243 mit dem Very Large Telescope der Europäischen Südsternwarte ESO beobachteten, entdeckten sie eine periodische Bewegung. Offenbar befindet sich in der Nähe des Sterns ein weiteres Himmelsobjekt, das mit ihm gemeinsam ein enges Doppelsystem bildet. Alle 10,4 Tage umkreisen sich die beiden Objekte gegenseitig, was die periodische Bewegung des Sterns VFTS 243 erklärt.
Doch die Forscher konnten in den Teleskopen keine Strahlung eines weiteren Sterns entdecken – das zweite Objekt blieb völlig unsichtbar. Während der Stern die 24-fache Sonnenmasse besitzt, muss das zweite Objekt im Doppelsystem mindestens eine Masse von neun Sonnenmassen umfassen. „Aus der Masse des unsichtbaren Objekts folgt damit, dass es sich um ein Schwarzes Loch handeln muss“, stellten Shenar und seine Kollegen fest. Da das Schwarze Loch auch keine Röntgenstrahlung aussendet, ist es dem Team erstmals gelungen, ein ruhiges Schwarzes Loch außerhalb der Milchstraße aufzuspüren – ein wichtiger erster Schritt, um die Häufigkeit derartiger Objekte einzuschätzen.
Als die Forscher das Doppelsystem daraufhin noch genauer untersuchten, machten sie eine weitere überraschende Beobachtung: Die Umlaufbahnen der beiden Himmelsobjekte sind offenbar nahezu kreisförmig. Wenn jedoch bei der Entstehung eines stellaren Schwarzen Lochs der kollabierende Stern seine äußere Hülle ins All abstößt, erhält das Schwarze Loch einen kräftigen Stoß – und gelangt so auf eine stark elliptische Umlaufbahn. In extremen Fällen kann das Schwarze Loch sogar aus dem Doppelsystem herauskatapultiert werden.
Die kreisförmige Umlaufbahn deute daher laut den Forschen darauf hin, dass das Schwarze Loch von VFTS 243 ohne eine Supernova entstanden sei. „In letzter Zeit gibt es immer wieder Hinweise auf ein solches Szenario“, erläutert Shenar. „Aber unsere Studie liefert wohl einen der bislang direktesten Hinweise dafür.“ Auch wenn zwei Schwarze Löcher miteinander verschmelzen, spielen ihre Umlaufbahnen eine wichtige Rolle. Somit haben die neuen Erkenntnisse auch Konsequenzen auf die Interpretation von Gravitationswellen, die mit großen Detektoranlagen von solchen Ereignissen empfangen werden.",Ruhiges Schwarzes Loch außerhalb der Milchstraße
"Nach vier Jahren veröffentlichte die europäische Weltraumorganisation ESA jüngst einen weiteren Datenkatalog des Weltraumteleskops Gaia. Neben über 1,8 Milliarden Sternen enthält der dritte Datensatz – Gaia DR3 – auch zahlreiche Galaxien, Quasare und Asteroiden. Von einem Teil dieser Objekte wurden nicht nur der Ort und die Geschwindigkeit, sondern erstmals auch die abgegebene Strahlung – das sogenannte Spektrum – mit in den Katalog aufgenommen. Welche Bedeutung diese neuen Daten für die Astronomie haben, berichtet Stefan Jordan von der Universität Heidelberg im Interview mit Welt der Physik.
Welt der Physik: Was ist der Hauptzweck der Gaia-Mission?












                Stefan Jordan
            



Stefan Jordan: Gaia ist eine europäische Satellitenmission, die eine möglichst umfangreiche Präzisionskartierung von astronomischen Objekten zum Ziel hat. Gaia befindet sich am sogenannten Lagrangepunkt L2, also von der Sonne aus gesehen rund 1,5 Millionen Kilometer hinter der Erde. An diesem für Weltraumteleskope sehr populären Beobachtungsort befindet sich jetzt auch das James-Webb-Weltraumteleskop. Im Gegensatz zu diesem observiert Gaia aber nicht einzelne, scharf begrenzte Himmelsregionen, sondern den gesamten Himmel.
Wie funktioniert das?
Dazu rotiert Gaia langsam alle sechs Stunden – also viermal am Tag – um ihre eigene Achse. Außerdem umläuft Gaia gemeinsam mit der Erde einmal im Jahr die Sonne. Dabei tasten die zwei Hauptspiegel schrittweise den gesamten Himmel ab und können dank der hochwertigen Kameratechnik an Bord eine riesige Menge an astronomischen Objekten präzise vermessen. Aus diesen Daten lassen sich der Ort, die Bewegung und bei manchen Objekten – abhängig von der Größe, Entfernung und Helligkeit – auch das Spektrum bestimmen.
Wie vermisst Gaia all diese Objekte?
Während der Satellit um die Sonne kreist, ändert sich der Ort von Gaia und damit die Richtung zu den Sternen der Milchstraße. Aus dieser scheinbaren Richtungsänderung der Sterne, die man Parallaxe nennt, lässt sich die Entfernung der Sterne bestimmen. Je weiter weg die Sterne sind, desto kleiner ist der Effekt. Aber Gaia kann mit ihrer hochpräzisen und stabilen Optik sowie einer Atomuhr an Bord die Entfernung vieler Sterne unserer Milchstraße hervorragend bestimmen. Gemeinsam mit der Beobachtungsrichtung, in der wir ausreichend nahe Himmelskörper sehen, können wir so alle drei Raumkoordinaten angeben.
Und wie lassen sich die Geschwindigkeiten der Objekte bestimmen?
Aus Messungen, die wir zu unterschiedlichen Zeitpunkten durchführen, kann Gaia die Bewegung der Sterne senkrecht zur Blickrichtung bestimmen. Die Geschwindigkeit eines Sternes auf uns zu oder von uns weg – die Radialgeschwindigkeit – lässt sich aus der Verschiebung von Spektrallinien anhand des Dopplereffekts nachweisen. Wenn das alles klappt, dann erhalten wir den kompletten Satz von Orts- und Bewegungskoordinaten eines Objekts. Von insgesamt 33 Millionen Objekten haben wir jetzt neben dem Ort auch alle drei Komponenten der Raumgeschwindigkeiten. Im vorherigen, zweiten Datenkatalog wurden nur von sieben Millionen Objekten die Radialgeschwindigkeiten veröffentlicht. Ebenfalls Teil des neuen Gaia-Katalogs sind nun eine Million qualitativ hochwertiger Spektren, die für die Radialgeschwindigkeitsbestimmung benutzt wurden. Diese erlauben auch eine Analyse der chemischen Zusammensetzung der Sternatmosphären.













                Geschwindigkeiten der Sterne in der Milchstraße
            



Was lässt sich daraus lernen?
Die Bestimmung dieser fundamentalen Parameter ist die Grundvoraussetzung für die Interpretation fast aller astronomischen Beobachtungen. Aus der Bewegung verschiedener Sterngruppen in der Milchstraße können wir zusammen mit Computersimulationen die Vergangenheit unserer Galaxie erforschen – also etwa wann sie sich verschiedene Zwerggalaxien einverleibt hat. Es fanden sich sogar deutliche Hinweise, dass eine nahe Passage einer solchen Zwerggalaxie die Entstehung unserer Sonne vor zirka 4,5 Milliarden Jahren ausgelöst hat. Dies ist nur ein Beispiel. Gaias Ergebnisse, die ja für jeden frei zugänglich sind, bringen praktisch alle Forschungsgebiete der Astrophysik voran. Pro Tag erscheinen rund fünf wissenschaftliche Veröffentlichungen auf der Basis von Gaia-Daten, und zwar zu den verschiedensten Gebieten der Astronomie und Astrophysik. Einige Forschungsarbeiten benutzen nur einen Zahlenwert, etwa die Entfernung eines Sterns, in anderen werden Millionen von Daten für neue Erkenntnisse benutzt.
Wie bedeutend ist das für die derzeitige Entwicklung in der Astronomie?
Gaia funktioniert unglaublich gut. Nicht nur die schiere Anzahl an vermessenen Sternen, sondern auch die Qualität der Daten übertrifft alle bisherigen vergleichbaren Sternkataloge. Ein Beispiel für die Genauigkeit und Sehschärfe von Gaia: Die Kameras könnten eine Kerze in 30 000 Kilometern Entfernung leuchten sehen. Und wenn ein helles Licht auf der Oberfläche des Mondes – den Gaia aber in Wirklichkeit nicht anvisieren kann – um vier Zentimeter verschoben würde, ließe sich das auch nachweisen.
Gab es auch schon Probleme während der Mission?
In der Tat läuft bei so komplexen Missionen nicht immer alles wie geplant. So schwingt zum Beispiel die Blickrichtung von Gaias beiden Teleskopen stärker hin und her als gedacht. Im Lauf der Jahre versteht man aber Gaias Messinstrumente immer besser und kann von einer Datenveröffentlichung zur nächsten viele Effekte ausgleichen und die Präzision noch einmal erhöhen.
Was war für Sie selbst die größte Überraschung?
Am meisten hat mich erstaunt, dass sich mit den Daten von Gaia auch Asteroseismologie machen lässt. Bei diesem relativ jungen Forschungsgebiet geht es darum, die Schwingungen von Sternen zu analysieren, um daraus etwas über den inneren Aufbau von Sternen zu lernen – ähnlich wie man aus seismischen Messungen auf der Erde etwas über ihr Inneres lernt.
Und welche anderen Himmelsobjekte als Sterne wurden mit Gaia vermessen?
Wir haben jetzt mit Gaia auch die Umlaufbahnen von rund 150 000 Asteroiden in unserem Sonnensystem bestimmt. Und für 60 000 Asteroiden hat Gaia hochwertige Spektren aufgenommen. Das sind acht Mal mehr, als bislang von irdischen Teleskopen gewonnen werden konnten. Aus dem Reflexionsvermögen der Asteroiden kann man einiges über deren Mineralogie erfahren – also über ihre Zusammensetzung an der Oberfläche. Darüber hinaus enthält Gaia DR3 noch große Spezialkataloge zu veränderlichen Sternen, Doppelsternen, Galaxien und Quasaren. Wir dürfen gespannt sein, was findige Forscherinnen und Forscher mit all diesen Daten nun anstellen werden. Denn der dritte Datensatz von Gaia ist eine riesige astronomische Schatzkiste, die die Wissenschaftler nun nutzen können.",„Eine riesige astronomische Schatzkiste“
"Hagel in Golfballgröße – solche Extremwetterereignisse sind in Mitteleuropa eher selten. Dennoch kommt es immer wieder zu Schäden durch Hageleinschlag, und manchmal ist Hagel der Vorläufer von Starkregen, der für Überflutungen sorgen kann. Um solche Ereignisse besser vorherzusagen, versuchen Meteorologen, ihre Modelle immer weiter zu verbessern. Unter anderem untersuchen sie dafür, wie genau Hagel entsteht und was bei seinem Weg durch die Atmosphäre passiert. Wie solche Hagelexperimente im Labor funktionieren, berichtet Miklós Szakáll von der Universität Mainz im Interview mit Welt der Physik.
Welt der Physik: Wie entstehen Graupel und Hagel?












                Miklós Szakáll
            



Miklós Szakáll: Graupel und Hagel entstehen entweder, wenn die Lufttemperatur niedrig genug ist und Regentropfen gefrieren. Diese Eispartikel können anwachsen, wenn sie mit Wassertröpfchen kollidieren. Befinden sich die Eiskügelchen in einer Gewitterwolke, in der hoher Aufwind und hohe Luftfeuchtigkeit herrschen und wo viele unterkühlte Wassertröpfchen vorhanden sind, dann können sie sehr schnell anwachsen. Denn der Aufwind hält die Eiskörner lange genug in der Wolke, sodass immer mehr Wassertröpfchen an ihrer Oberfläche gefrieren können oder sich Wasserdampf an der Oberfläche der Eiskügelchen anlagern kann. Wenn sie eine gewisse Größe erreicht haben, kann der Aufwind sie nicht mehr tragen und die Eiskörner fallen als Hagel oder Graupel aus der Gewitterwolke heraus. Man spricht ab einem Durchmesser von fünf Millimetern von Hagel, darunter von Graupel.
Welche Folgen kann vor allem Hagel haben?
Autos oder Häuser können beschädigt werden, aber auch Tiere und Menschen können getroffen und verletzt werden. Die Landwirtschaft ist auch sehr von Hagelschäden betroffen, vor allem bei Obst und Getreide. Und wenn die Hagelkörner auf dem Weg zum Erdboden komplett schmelzen, gibt es Starkregen. Das ist zum Beispiel im Sommer 2021 bei der Flutkatastrophe in Rheinland-Pfalz und Nordrhein-Westfalen passiert.
Wovon hängt es ab, ob die Hagelkörner als Starkregen auf die Erde treffen?
Das hängt davon ab, ob der Hagel schmilzt oder nicht. Denn wenn Temperaturen von 20 oder 30 Grad Celsius am Boden herrschen, ist erst ab einer bestimmten Höhe die Temperatur nicht mehr über null Grad Celsius. Entscheidend ist also: Wo genau liegt diese Null-Grad-Grenze? Dann ist es eine Frage der Luftfeuchte und der Geschwindigkeit, mit der der Hagel fällt. Denn wenn er lang genug fallen kann, taut er komplett auf und wird zu Regen.
Wie haben Sie das Fallen von Hagelkörnern untersucht?












                Künstliche Hagelkörner
            



Wir hatten eine Kooperation mit Forschern aus den USA. In den USA ist es ein bisschen einfacher Daten zu sammeln – es hagelt einfach häufiger. Die Kollegen sind den Stürmen gefolgt, haben verschiedene Hagelkörner gesammelt und direkt die Masse und mit einem 3D-Scanner auch ihre Form bestimmt. Dadurch existiert jetzt eine sehr große Datenbank von verschiedenen Hagelkörnern. Wir haben mit einem 3D-Drucker dann Modelle dieser unregelmäßig geformten Hagelkörner mit unterschiedlichen Dichten ausgedruckt. Denn mit künstlichen Hagelkörnern kann man Experimente beliebig oft wiederholen, ohne dass die Hagelkörner schmelzen oder ihre Form verändern. So konnten wir untersuchen, wie sich diese Hagelkörner in unserem vertikalen Windkanal verhalten.
Was ist ein vertikaler Windkanal?
In dem Windkanal können wir messen, wie schnell Hagelkörner fallen, wie sie sich verhalten und ob sie rotieren oder eher ruhig in der Luftströmung liegen. Wenn ein Hagelkorn aus einer Wolke fällt, wird es von der Gravitationskraft beschleunigt und gleichzeitig vom Luftwiderstand gebremst. Sind der Luftwiderstand und die Gravitationskraft gleich groß, fällt das Hagelkorn mit einer konstanten Geschwindigkeit – dann ist die Beschleunigung null. In diesem Fall ist es egal, ob man ein fallendes Hagelkorn in ruhender Luft betrachtet oder ein ruhendes Hagelkorn, an dem die Luft von unten nach oben vorbeiströmt. Das ist physikalisch dasselbe. Diesen Zustand stellen wir in unserem Windkanal her. Wir stellen die Geschwindigkeit des vertikalen Windes im Experiment so ein, dass der Hagel gerade eben schwebt. Dann weiß man genau, wie hoch die Fallgeschwindigkeit von diesem Hagelkorn in der Atmosphäre wäre. So können wir den Hagel im Labor untersuchen.
Und was haben Sie genau untersucht?












                Windkanal
            



Mit dem künstlichen Hagel haben wir vor allem die Geschwindigkeit und den Querschnitt zur Luftströmung, sozusagen die Angriffsfläche, für verschiedene Massen gemessen. Es war wichtig zu schauen: Wie orientiert sich das einzelne Hagelkorn? Und welche Annahmen über den Querschnitt kann man machen? Kann man zur theoretischen Berechnung der Fallgeschwindigkeit die größte Dimension des Korns nehmen oder nur den mittleren Durchmesser? Denn je kleiner der Querschnitt zur Luftströmung, desto niedriger ist auch der Luftwiderstand und dann würde das Hagelkorn schneller fallen.
Welche Erkenntnisse haben Sie aus diesen Experimenten gewonnen?
Letztlich haben wir die sogenannte Reynoldszahl der verschiedenen Hagelkörner ermittelt. Die Reynoldszahl ergibt sich aus der Größe eines Objekts, der Geschwindigkeit der Luftströmung und ihrer Viskosität. Mit dieser Zahl arbeiten wir oft in der Wolkenphysik, weil sie Auskunft darüber gibt, wie sich ein Objekt durch eine Luftströmung bewegt – also zum Beispiel ein Hagelkorn oder auch ein Regentropfen. Dadurch, dass wir systematisch unterschiedliche Typen von künstlichen Hagelkörnern untersucht haben, konnten wir erstmals eine Gleichung ableiten, mit der man die Reynoldszahl für beliebig große Hagelkörner berechnen kann – im Grunde also deren Fallgeschwindigkeit vorhersagen kann. Damit lassen sich dann hoffentlich die Modelle zur Wettervorhersage verbessern.
Machen Sie auch Versuche mit echten Hagelkörnern?
Ja, denn wir können die Luft im Windkanal auf bis zu minus 30 Grad Celsius kühlen. So lässt sich zum Beispiel tatsächlich das Gefrieren von Eiskristallen zu Hagelkörnern beobachten. Wir können diese kalte Luft aber auch langsam erwärmen und damit das Fallen eines Hagelkorns aus einer Wolke simulieren. Mithilfe solcher Experimente überprüfen wir dann, wie gut unsere theoretischen Modelle sind. Denn wenn wir den Prozess des Schmelzens verstehen und korrekt beschreiben können, hilft auch das für bessere Vorhersagen.
Welche Bedeutung haben Ihre Experimente für die Zukunft?
Man erwartet durch den Klimawandel immer mehr Extremwetterereignisse, also mehr Regen oder mehr Hagel, da die Aufwinde stärker werden und auch mehr Wasserdampf in der Luft sein wird. Die Hagelkörner können dadurch theoretisch immer größer werden. Gleichzeitig steigt aber aufgrund der globalen Erwärmung auch die Null-Grad-Grenze, wodurch die Hagelkörner mehr Zeit haben, um zu schmelzen. Für uns stellt sich daher die Frage, ob dann die riesigen Hagelkörner als Starkregen oder als Hagel auf die Erde treffen und wie genau all diese Prozesse ablaufen. Das ist eben noch nicht genau bekannt.","„Das hilft, die Modelle zur Wettervorhersage zu verbessern“"
"Bei einer Strahlentherapie wird Tumorgewebe gezielt ionisierender Strahlung ausgesetzt und dadurch geschädigt. Neben energiereicher elektromagnetischer Strahlung kommen dabei auch Ionen zum Einsatz, also elektrisch geladene Atome. Welche Teilchen sich eignen, wie sich Tumorzellen damit zerstören lassen und wie Forscher die Krebstherapie mit Ionen künftig weiterentwickeln wollen, erklärt Thomas Haberer vom Heidelberger Ionenstrahl-Therapiezentrum in dieser Folge des Podcasts.












                Thomas Haberer
            



Jedes Jahr erkranken in Deutschland rund eine halbe Million Menschen an Krebs. Viele dieser Patienten können durch Operationen, Chemo- oder Strahlentherapie – oder eine Kombination dieser Therapien – erfolgreich behandelt werden. Während die Chemotherapie auf chemische Substanzen setzt, um den Tumor zu bekämpfen, macht man sich in der Strahlentherapie die ionisierende Wirkung von energiereichen elektromagnetischen Wellen und Teilchen zunutze.Thomas Haberer: „Von Ionisierung reden wir, wenn Atome des durchstrahlten Gewebes geladen werden. Typischerweise heißt das, dass Elektronen aus der Elektronenhülle entfernt werden.“Wenn einzelne Elektronen in ihrer Hülle fehlen, sind Atome besonders reaktionsfreudig. Dadurch finden chemische Reaktionen in den Tumorzellen statt, die letztlich dazu führen, dass wichtige Zellbestandteile zerstört werden. Aber auch direkte Treffer können relevante Biomoleküle ausschalten und die Krebszellen so daran hindern, sich zu vermehren. Mehr dazu in der 336. Folge.",Krebstherapie mit Ionen
"Synthetisches Kerosin ist die große Hoffnung der Luftfahrt, um in Zukunft möglichst klimafreundlich fliegen zu können. Denn rein elektrische Antriebe werden unter anderem wegen des hohen Gewichts selbst bester Lithium-Ionen-Batterien höchstens für kleine Sportflieger genügen. Große Passagierjets bleiben dagegen weiterhin auf flüssige Treibstoffe angewiesen. Nun gelang es einer Forschungsgruppe, das begehrte Kerosin aus Wasser und Kohlendioxid mittels Sonnenhitze zu erzeugen. Details zu dem Prozess mit einem Solarturm präsentieren die Wissenschaftler in der Fachzeitschrift „Joule“.
„Wir sind die Ersten, die die gesamte Prozesskette von Wasser und Kohlendioxid bis zum Kerosin in einem Solarturmsystem demonstrierten“, sagt Aldo Steinfeld von der Eidgenössischen Technischen Hochschule Zürich. Dazu nutzte er mit seinen Kollegen ein Solarkraftwerk in Móstoles nahe Madrid. Mit insgesamt 169 Spiegeln – sogenannten Heliostaten mit jeweils drei Quadratmetern Spiegelfläche – lenkten sie einfallendes Sonnenlicht gebündelt auf die Spitze eines Solarturms. Dort befindet sich der Reaktorraum, in dem es bis zu 1500 Grad Celsius heiß werden kann.












                Hochporöse Keramik
            



Herzstück des Solarreaktors ist ein gut 18 Kilogramm schwerer Block aus einer hochporösen Keramik aus Ceriumoxid. Durch diese Kammer ließen die Forscher Kohlendioxid und Wasserdampf strömen. Bei der vorherrschenden Hitze gab das Ceriumoxid zuerst etwas Sauerstoff ab. Daraufhin reagierte es sowohl mit dem Wasser als auch mit dem Kohlendioxid in der Kammer und entzog den Gasen wiederum Sauerstoff. Das Ergebnis: ein Gasgemisch aus Wasserstoff und Kohlenmonoxid – sogenanntes Synthesegas. Dieses leiteten die Forscher in einen weiteren Reaktor am Fuße des Solarturms, wo sie es mithilfe von Katalysatoren und unter hohen Drücken und Temperaturen zu flüssigen Treibstoffen wie Kerosin oder Diesel umwandelten.
Noch ist dieses Verfahrens nicht sehr effizient. Aber immerhin 4,1 Prozent der Energie des Sonnenlichts ließ sich nutzen, um das Synthesegas zu erzeugen. Steinfeld und seine Kollegen halten es jedoch für möglich, einen Wirkungsgrad von bis zu 15 Prozent zu erreichen, indem sie die Prozessschritte und den Keramikblock weiter optimieren. In den nächsten Schritten könnten sich durch größere Solarturmanlagen außerdem die noch hohen Kosten für solar erzeugtes Kerosin drastisch senken lassen. Womöglich lässt sich auch in nicht allzu ferner Zukunft Kohlendioxid für die Treibstoffproduktion direkt aus der Luft verwenden. Wird das Kerosin danach wieder in den Flugzeugtriebwerken verbrannt, wird es wieder freigesetzt, belastet die Atmosphäre aber nicht zusätzlich. Insgesamt käme man so dem Traum vom klimaneutralen Fliegen mit Solarenergie einen deutlichen Schritt näher.",Kerosin aus Sonnenenergie
"Seit langem rätseln Astronomen, warum viele der eisigen Objekte, die jenseits der Planeten unseres Sonnensystems um die Sonne kreisen, eine rötlich gefärbte Oberfläche besitzen. Jetzt glauben drei Forscher, eine Lösung gefunden zu haben: Eisvulkanismus könnte die Ursache sein. Dabei trete auch Methan aus dem Inneren der Himmelskörper aus, wandle sich unter dem Einfluss der kosmischen Strahlung in schwerere Moleküle um und lagere sich dann als rötlicher Belag auf der Oberfläche ab, schreiben die Wissenschaftler im Fachblatt „Nature Communications“.
Stephanie Menten, Michael Sori und Ali Bramson von der Purdue University in den USA stießen auf diese Erklärung, als sie sich mit Charon, dem größten Mond des Zwergplaneten Pluto, befassten. Auch dieser zeigt eine auffällige rote Färbung, jedoch nur an seinen Polen. Bislang gingen die Planetenforscher davon aus, dass diese roten Polkappen durch Methan entstehen, das aus der Atmosphäre Plutos in den Weltraum entweicht. Demnach zieht der Plutomond Charon einen Teil des Methans an, wodurch es sich in Form einer Eisschicht an den Polen ablagert. Kosmische Strahlung aus dem All löst dann, so die Annahme, chemische Reaktionen der Methanmoleküle aus, die über lange Zeiträume zur Bildung schwererer Moleküle wie etwa Tholin führen – und diese verursachen eine rötliche Färbung.
„Doch auch auf ähnlich großen Objekten im Kuipergürtel, die nicht an einen Zwergplaneten gebunden sind, gibt es solche aus Methan entstandenen Moleküle“, erläutern Menten und ihre Kollegen. Deshalb erschien ihnen diese Erklärung zu kurz zu greifen – und sie machten sich auf die Suche nach einer Ursache, die ohne einen weiteren Himmelskörper auskommen sollte.
Dabei hatten sie folgende Vermutung: Das Methan stamme womöglich aus dem Inneren der rötlich gefärbten Objekte selbst. Tatsächlich gibt es auf Charon Spuren von Kryovulkanismus, Vulkanismus also, bei dem nicht heißes Gesteinsmagma, sondern ein Gemisch aus gefrorenem und flüssigem Wasser aus dem Inneren an die Oberfläche dringt. Dieses Eismagma enthält auch Methan, das bei kleinen Himmelskörpern zwar schnell ins Weltall entweicht. Aber ein kleiner Anteil davon lagert sich als Methaneis auf der Oberfläche ab und ist dort der kosmischen Strahlung ausgesetzt.
Menten und ihre Kollegen errechneten daraufhin, dass Eisvulkane insgesamt über eine Billion Tonnen Methan an die Oberfläche von Charon transportiert haben – und dass sich dieses Methan bevorzugt an den Polen abgelagert haben sollte. Diese Menge, so die Forscher, reiche aus, um die rötliche Färbung der Pole auch ohne den Einfluss von Pluto zu erklären. Außerdem sei ein solcher Prozess auch auf anderen Objekten des Kuipergürtels möglich und könne die rote Färbung zahlreicher Asteroiden erklären, so die Forscher: „Das Ausströmen von Methan aus dem Inneren könnte also ein häufiger und wichtiger Prozess im gesamten Kuipergürtel sein.“",Warum viele ferne Asteroiden rot sind
"Die Arktis erwärmt sich im Zuge des Klimawandels deutlich schneller als andere Regionen. Die Erwärmung im Vergleich zum weltweiten Durchschnitt – die sogenannte polare Verstärkung – wurde bislang etwa auf die doppelte Geschwindigkeit abgeschätzt. Doch eine neue Analyse zeigt nun, dass sich die Arktis in der nördlichen Polarregion sogar vierfach schneller erwärmt als der globale Durchschnitt. Wie Klimaforscher in der Fachzeitschrift „Commications Earth & Environment“ berichten, gründet diese deutliche Korrektur auf Messdaten der vergangenen 43 Jahre.
„Wir fokussierten uns auf eine 1979 beginnende Periode, da seitdem verlässlichere Messdaten vorliegen und die starke Erwärmung in den 1970er Jahren begann“, sagt Mika Rantanen vom Finnischen Meteorologischen Institut in Helsinki, der zusammen mit seinen Kollegen die Region nördlich des Polarkreises untersuchte. In diese Analyse flossen vor allem Satellitendaten ein. Dabei stellten sie fest: Liegt die durchschnittliche Erderwärmung derzeit bei 0,19 Grad Celsius pro Jahrzehnt, zeigt die neue Auswertung für die Arktis über dieselbe Zeitspanne eine Erwärmung um 0,73 Grad Celsius. Regional fanden Rantanen und seine Kollegen rund um die Barentssee nördlich von Norwegen sogar eine siebenfach beschleunigte Erwärmung im Vergleich zum globalen Mittel.












                Polare Verstärkung
            



Die Ursachen für diese starke polare Verstärkung sind vielfältig und werden in der Fachwelt intensiv diskutiert. Großen Anteil hat beispielsweise die Eis-Albedo-Rückkopplung. Diese berücksichtigt den Rückgang der Eisschichten, die das Sonnenlicht reflektieren. Die dadurch freigelegten Wasser- und Landflächen absorbieren wegen ihrer dunkleren Färbung mehr Sonnenlicht und wärmen sich stärker auf. Zudem strahlen die kälteren Oberflächen der Arktis weniger Wärme ab als wärmere Oberflächen südlich des Polarkreises. In Zukunft wird sich die Erwärmung zudem dadurch etwas beschleunigen, dass vor allem in Europa und Asien die Luftverschmutzung zurückgeht. Denn Schmutzpartikel in der Atmosphäre schirmen einen Teil der wärmenden Sonnenstrahlung ab. Der Effekt ist allerdings relativ klein, so dass die heutige oder eine stärkere Luftverschmutzung die polare Verstärkung nicht verhindern könnte.
Diese auf jüngeren Messdaten beruhende Studie legt nahe, dass die beschleunigte Erwärmung der Arktis in Klimamodellen bisher unterschätzt wurde. Allerdings tragen auch natürliche Schwankungen zu der Erwärmung bei. Doch die Wahrscheinlichkeit, dass diese allein für die nun beobachtete polare Verstärkung verantwortlich sind, ist nach Aussage der Forscher extrem gering. Sie schlagen nun vor, die Mechanismen der polaren Verstärkung noch genauer zu untersuchen. Diese Ergebnisse könnten dann zu noch besseren Klimamodellen führen.",Erwärmung der Arktis drastisch unterschätzt
"Stetig nehmen die Ozeane auf der Erde Sauerstoff aus der Atmosphäre auf und schaffen so einen Lebensraum für viele Wasserbewohner. Diesen einfachen Prozess nutzen Forscher in Klimamodellen, um den Sauerstoffgehalt der Meere abzuschätzen. Doch vor allem in der Tiefsee greift die Erklärung zu kurz, wie eine Forschergruppe mit einem neuen Modell nun zeigt. Denn auch die Verschiebung der Kontinente innerhalb der letzten 540 Millionen Jahre hat den Transport von Sauerstoff in tiefere Wasserschichten wesentlich beeinflusst, berichten die Wissenschaftler in der Fachzeitschrift „Nature“.
Der Sauerstoffgehalt prägt den Lebensraum der Meere seit Jahrmillionen: Während ein Mangel an Sauerstoff in der Erdgeschichte zu einem drastischen Rückgang der Arten führte, förderte eine gute Versorgung mit Sauerstoff dagegen eine enorme Vielfalt an Meeresbewohnern. Alexandre Pohl von der Universität Bourgogne Franche-Comté in Frankreich und seine Kollegen wollten nun mögliche Einflüsse auf den Sauerstoffgehalt der Ozeane untersuchen. Dazu nutzen sie ein Modell, in dem sie den Sauerstoffanteil in der Atmosphäre auf einem konstanten Niveau hielten. Zudem modellierten die Wissenschaftler den Transport von Sauerstoff innerhalb der Meere über einen Zeitraum von 540 Millionen Jahren. Denn auf dieser Zeitskala veränderten die Kontinente aufgrund der Plattentektonik ihre Positionen, was sich deutlich auf die Meeresströmungen und damit auch auf den Sauerstofftransport auswirkte.
Bislang ist man davon ausgegangen, dass der Sauerstoffgehalt der Ozeane vor allem durch den Sauerstoffgehalt der Atmosphäre bestimmt wurde und sich daher in dem Modell der Forscher nicht wesentlich verändern sollte. Doch die neuen Berechnungen zeigen, dass der Einfluss der Meeresströmungen viel stärker war als bisher angenommen. So fanden Pohl und seine Kollegen heraus, dass etwa vor 540 bis 460 Millionen Jahren in der Tiefsee ein Mangel an Sauerstoff herrschte. Vor 420 Millionen Jahren änderte sich diese Situation laut den Simulationen der Forscher jedoch und das Gas gelangte über einen verstärkten Austausch zwischen oberen und tiefen Wasserschichten auch in die Tiefsee. Ein deutlicher Anstieg der Artenvielfalt war die Folge.
Um die Sauerstoffkonzentration in den Meeren im Laufe der Erdgeschichte exakt zu bestimmen, ist das Modell zwar zu stark vereinfacht. Doch es zeigt deutlich, dass die Plattentektonik den Sauerstoffgehalt in größeren Wassertiefen wesentlich beeinflusste. Damit belegen die Forscher, dass der Sauerstofftransport in den Ozeanen über viel komplexere Prozesse abläuft als bisher angenommen und in zukünftigen Modellen stärker berücksichtigt werden sollte. So könnten beispielsweise Bohrkerne aus Sedimenten der Meere genauer analysiert werden und einen besseren Einblick in die Entwicklung des Klimas und die Lebensbedingungen im Laufe der Erdgeschichte ermöglichen.",Die Belüftung der Ozeane
"Woher stammt die hochenergetische kosmische Strahlung? Auf diese Fragen haben Forscher jetzt eine Antwort gefunden und damit eine lange gehegte Vermutung von Astronomen bestätigt. Die Strahlung stammt offenbar von natürlichen Teilchenbeschleunigern in der Umgebung von Überresten explodierter Sterne, berichten die Wissenschaftler im Fachblatt „Physical Review Letters“.
Auf unsere Erde prasseln ständig Teilchen – hauptsächlich Protonen, aber auch schwerere Atomkerne – aus dem All ein. Die meisten von ihnen haben eine relativ geringe Energie von einigen Gigaelektronenvolt oder weniger. Zu großen Teilen stammen sie von Eruptionen auf der Oberfläche der Sonne. Doch es gibt auch Teilchen mit Energien im Bereich um ein Petaelektronenvolt – das entspricht einer um ein Millionenfaches höheren Energie.
Seit langem vermuten Astronomen, dass es natürliche Teilchenbeschleuniger im All geben müsse. Dort lösen etwa Supernovaexplosionen starke Stoßwellen aus, die dann elektrisch geladene Teilchen auf hohe Energien beschleunigen. Aufgrund der enormen Energien um ein Petaelektronenvolt – kurz PeV – heißen solche Teilchenbeschleuniger auch PeVatrons. Doch bislang war es nicht gelungen, sie aufzuspüren. Das Problem: Geladene Teilchen wie die Protonen der kosmischen Strahlungen bewegen sich nicht geradlinig durch das Weltall, denn starke Magnetfelder lenken diese Teilchen ab. Dadurch lässt sich nicht einfach von der Richtung, aus der sie kommen, auf den Herkunftsort der Teilchen rückschließen.
Astronomen müssen daher auf indirekte Weise nach PeVatrons suchen: Sie fahnden nach hochenergetischer Gammastrahlung, die entsteht, wenn kosmische Strahlung mit interstellarem Gas, also dem Gas zwischen Sternen, wechselwirkt. Allerdings ist dieser Zusammenhang nicht eindeutig: Auch wenn die kosmische Hintergrundstrahlung – das Strahlungsecho des Urknalls – an Elektronen im Weltall gestreut wird, kann derartige Gammastrahlung entstehen.
Ke Fang von der University of Wisconsin und ihre Kollegen haben nun Daten analysiert, die das Satellitenobservatorium Fermi und andere Teleskope zwölf Jahre lang von G106.3+2.7, dem 2600 Lichtjahre entfernten Überrest einer Supernova, gesammelt haben. Während Fermi auf Gammastrahlung spezialisiert ist, lieferten die anderen Observatorien zusätzliche Daten zur Röntgenstrahlung und Radiowellen, die von demselben beobachteten Objekt ausgehen.
Wie die Untersuchung der Forscher zeigt, passt die Energieverteilung der Strahlung von dem Supernovaüberrest über diesen weiten Bereich von Radiowellen bis zur Gammastrahlung sehr gut zu einem PeVatron – aber nicht zum alternativen Szenario der Streuung von Hintergrundstrahlung. G106.3+2.7 scheint nach Ansicht von Fang und ihren Kollegen daher eine der wichtigsten Quellen hochenergetischer kosmischer Teilchen zu sein. Nun hoffen die Forscher, mit ihrer Methode noch weitere kosmische Teilchenbeschleuniger aufzuspüren.",Einem Teilchenbeschleuniger im All auf der Spur
"Vor etwas mehr als fünfzig Jahren, am 20. Juli 1969, landeten Neil Armstrong und Buzz Aldrin auf dem Erdtrabanten. Zwischen Ankündigung und Umsetzung einer bemannten Mondlandung vergingen mehr als acht Jahre. Auch für die Zukunft geplante Missionen zum Mond benötigen lange Vorarbeit – ganz so einfach scheint es also nicht zu sein, von der Erde zum Mond zu gelangen. 
„Ich glaube, dass sich die Vereinigten Staaten das Ziel setzen sollten, noch vor Ende dieses Jahrzehnts einen Menschen auf dem Mond landen zu lassen und ihn wieder sicher zur Erde zurückzubringen“, proklamierte US-Präsident John F. Kennedy am 25. Mai 1961. Mehr als acht Jahre vergingen, bis die NASA diesen kühnen Plan in die Tat umsetzte.
In dieser Zeit entwickelte die Raumfahrtbehörde nicht nur die nötige Technik für einen Flug zum Mond, es galt auch die beste Reiseroute zum Erdtrabanten zu finden. Denn anders als auf der Erde ändern Start- und Endpunkt im Weltall während des Reiseverlaufs ihre Position relativ zueinander. Alles ist in ständiger Bewegung: Die Erde dreht sich um sich selbst und kreist um die Sonne. Und auch das Ziel – in diesem Fall der Mond – kreist um die Erde und mit ihr gemeinsam um die Sonne.
Eine kurvenreiche Flugbahn
Könnte man mit einer Rakete nicht trotzdem einfach von der Erde direkt zum Mond fliegen? Physikalisch spricht nichts dagegen. Doch für einen solchen Flug würde man einen extrem starken Antrieb und sehr viel Treibstoff benötigen. Und so versuchen die Raumfahrtwissenschaftler, die Eigenbewegungen und Anziehungskräfte der Himmelskörper optimal auszunutzen – und damit Energie zu sparen. Das beginnt bereits beim Start: Raketen schießt man bevorzugt in Äquatornähe und in Richtung der Erddrehung ins All. Allein dadurch nimmt eine Rakete eine Geschwindigkeit von 1674 Kilometern pro Stunde mit. Für eine typische Umlaufbahn um die Erde in einer Höhe von 300 Kilometern muss ein Raumfahrzeug allerdings eine Geschwindigkeit von 28 000 Kilometern pro Stunde erreichen.












                Hohmann-Bahn
            



Um aus einer Erdumlaufbahn möglichst energiesparend an einen weiter entfernten Ort – wie den Mond – zu gelangen, gibt es nun verschiedene Varianten. Eine ist die sogenannte Hohmann-Bahn: Dabei handelt es sich um eine Ellipse, in deren Brennpunkt sich die Erde befindet. Der erdnächste Punkt dieser Ellipse berührt die ursprüngliche Umlaufbahn um die Erde, der erdfernste Punkt der Ellipse befindet sich in der gewünschten Umlaufbahn – in diesem Fall also in der Umlaufbahn des Mondes. Bereits 1925 beschrieb der deutsche Raumfahrtpionier Walter Hohmann diesen Übergang zwischen zwei Bahnen in seinem Buch „Die Erreichbarkeit der Himmelskörper“.
Um auf eine solche zum Mond führende Ellipsenbahn zu gelangen, muss das Raumfahrzeug auf eine Geschwindigkeit von etwa 40 000 Kilometern pro Stunde beschleunigt werden. Die Triebwerke müssen dabei genau im richtigen Moment zünden, damit die angestrebte Hohmann-Bahn im erdfernsten Punkt tatsächlich auf den sich bewegenden Mond trifft. Dieses Flugmanöver war in den 1950er- und 1960er-Jahren sowohl technisch als auch rechnerisch eine Herausforderung. Jedes heutige Smartphone ist den für die Apollo-Missionen verwendeten Computern der NASA millionenfach überlegen. Und um in eine Umlaufbahn um den Erdtrabanten zu gelangen oder weich auf dem Mond zu landen, ist sogar eine noch kompliziertere Flugbahn nötig.
Zahlreiche Anläufe bis zur Mondlandung
In zahlreichen Versuchen tasteten sich die Raumfahrtbehörden langsam an den Mond heran: Nach etlichen Fehlstarts raste am 4. Januar 1959 die sowjetische Sonde Lunik 1 am Erdtrabanten vorbei – in einem Abstand von etwa 6000 Kilometern. Am 12. September 1959 schlug mit Lunik 2 erstmals eine Raumsonde auf dem Mond auf. Die erste weiche Landung auf dem Mond gelang am 3. Februar 1966 der ebenfalls sowjetischen Sonde Luna 9. Und am 3. April desselben Jahres schwenkte Luna 10 erstmals in eine Umlaufbahn um den Erdtrabanten ein. Am 24. Dezember 1968 erreichte mit Apollo 8 dann erstmals ein bemanntes Raumschiff den Mond und umkreiste ihn innerhalb von zwanzig Stunden insgesamt zehnmal. Und schon sieben Monate später gelang der NASA mit Apollo 11 dann die erste bemannte Mondlandung.













                Verlauf bemannter Mondmissionen wie Apollo 11 oder Artemis 3
            



Apollo 11 startete am 16. Juli 1969 und erreichte zwölf Minuten später planmäßig die Erdumlaufbahn. Die Rakete umkreiste unseren Planeten eineinhalbmal, bevor sie auf Mondkurs ging. Nach 76 Stunden erreichten Neil Armstrong, Edwin Aldrin und Michael Collins den rund 380 000 Kilometer entfernten Erdtrabanten. Am 19. Juli schwenkten die Astronauten in eine Mondumlaufbahn ein. Einen Tag später koppelte die Landefähre – mit Armstrong und Aldrin an Bord – ab und setzte wenig später auf der Mondoberfläche auf. Am 21. Juli um 3:56 Uhr mitteleuropäischer Zeit betrat Armstrong als erster Mensch den Mond, zwanzig Minuten später folgte ihm Aldrin. Zweieinhalb Stunden dauerte der erste Ausflug auf die Mondoberfläche. Nach rund 21 Stunden auf dem Mond ging es dann zurück, erst zum Apollo-Raumschiff und dann zur Erde. Am 24. Juli 1969 landete die Kapsel mit den drei Astronauten im Pazifik.
Die Anreise der Apollo-Astronauten zum Mond dauerte drei Tage und vier Stunden. Eine kurze Flugzeit ist bei bemannten Missionen ein entscheidendes Kriterium, denn sie bedeutet eine geringere Strahlenbelastung für die Raumfahrer. Bei unbemannten Sonden spielt die Flugzeit dagegen eine geringe Rolle. So sind auch Flugrouten denkbar, die Monate dauern, dafür aber wenig Energie kosten. Solche Bahnen führen zunächst meist weit aus dem Erde-Mond-System heraus und machen sich die Anziehungskraft der Sonne zunutze, um schließlich zum Mond zurückzukehren. Statt der schubstarken chemischen Antriebe lassen sich dafür auch elektrische Antriebe verwenden. Ein weiterer großer Vorteil: Die Raumsonden nähern sich dem Mond mit einer geringen Relativgeschwindigkeit, wodurch nur geringe Korrekturen nötig sind, um in eine Umlaufbahn um den Erdtrabanten einzuschwenken.",Wie fliegt man zum Mond?
"Teilchen, deren Eigenschaften auch weit entfernt voneinander miteinander verknüpft bleiben und dadurch Informationen austauschen können – ein Phänomen, das Albert Einstein einst als „spukhafte Fernwirkung“ bezeichnete und das in der Quantenphysik als Verschränkung bekannt ist. Dieses Phänomen und noch weitere Besonderheiten der Quantenphysik spielen aktuell eine wichtige Rolle bei der Entwicklung von Quantensimulatoren und Quantencomputern. Im Interview mit Welt der Physik erzählt Markus Ternes vom Forschungszentrum Jülich und der RWTH Aachen, wie er und seine Kollegen zwei Atome beim Informationsaustausch beobachtet haben, um die Verschränkung von Teilchen noch besser zu verstehen.
Welt der Physik: Was genau ist ein Quantensimulator?












                Markus Ternes
            



Markus Ternes: Bekannter sind sicherlich die Quantencomputer. Die Idee hinter Quantencomputern ist ja, eine universelle Quanten-Rechenmaschine zu konstruieren – ähnlich wie ein klassischer Computer, nur viel leistungsstärker. Im Gegensatz zu einem Quantencomputer kann ein Quantensimulator immer nur eine bestimmte Art von Experiment realisieren, dieses dafür aber sehr kontrolliert. Ungefähr so, als hätte man einen Flugsimulator anstelle eines universellen Computers. Ein Beispiel für einen Quantensimulator ist etwa ein künstliches Quantensystem aus Atomen.
Und Sie erforschen, wie die Atome in diesem Quantensystem miteinander interagieren. Wie sieht so ein Experiment aus?
Wir haben zunächst ein sehr simples System gewählt: zwei Titanatome auf einer Magnesiumoxidoberfläche. Die Atome haben wir etwa ein Milliardstel Meter voneinander entfernt auf die Oberfläche gesetzt – das ist für Atome relativ weit. Von Titanatomen wussten wir bereits, dass sie sehr gute Qubits sind. Qubits bezeichnen die kleinsten Rechen- und Speichereinheiten in Quantensimulatoren und Quantencomputern, so wie die klassischen Bits in konventionellen Computern. Ein Qubit hat gegenüber dem Bit den Vorteil, dass es nicht nur zwei Werte „1“ oder „0“ annehmen kann, sondern auch Überlagerungen aus „1“ und „0“. Dadurch werden viel mehr parallele Rechenoperationen möglich.
Wie wird denn ein Titanatom zu einem Qubit?
Wir bringen die Titanatome in unseren Experimenten in ein Magnetfeld und simulieren solche Überlagerungszustände zwischen „1“ und „0“ mithilfe ihrer Spins – also der Eigendrehimpulse der Atome, die sich wie kleine Stabmagnete in einem Magnetfeld verhalten. Die Spins der Titanatome richten sich in einem Magnetfeld erst einmal bevorzugt parallel oder entgegengesetzt zum Feld aus – diese Ausrichtung entspricht dann den Werten „1“ und „0“ des Qubits. Wir können ein Titanatom aber auch in einen Überlagerungszustand von „0“ und „1“ bringen, das entspricht dann einem gedrehten Spin. Dafür legen wir beispielsweise ein wechselndes Magnetfeld an oder stören das Atom durch einen Stromimpuls. Auf diese Art haben wir zwei Qubits mithilfe von zwei Titanatomen simuliert.
Und was genau haben Sie untersucht?
Aus der Theorie der Quantenmechanik wissen wir, dass die Spins der Atome miteinander verschränkt, also in ihrem Verhalten nicht unabhängig voneinander, sind. Unsere Frage war: Wenn wir eines der Atome mit einem kurzen Impuls stören, was geschieht mit dem Gesamtsystem? Wir konnten zeigen, dass das gesamte System instabil wird und zwischen zwei Zuständen hin und her schwingt – wie eine Art Tanz –, bis beide Spins schließlich einen neuen gemeinsamen und stabilen Zustand gefunden haben. Für die Anwendung in Quantensimulatoren heißt das: Wenn wir zwei Qubits in einen bestimmten Zustand bringen wollen, müssen wir das normalerweise nacheinander machen. Unser Versuch hat gezeigt, dass es unter bestimmten Bedingungen einfacher geht: Man regt nur das eine an und wartet eine gewisse Zeit, und dann sind beide in dem gewünschten Zustand.
Das heißt, Sie nutzen die Verschränkung der Atome aus, um bestimmte Zustände im Quantensimulator zu erzeugen?
Ja genau. Und jetzt ist die Idee, hier weiterzugehen. Was passiert, wenn wir drei oder vier Qubits betrachten? Je mehr wir nehmen, desto kompliziertere Strukturen können wir anschauen. Dann stellt sich die Frage: Was passiert, wenn die Qubits unterschiedliche Abstände haben? Was bedeutet eine Störung von einem dann für die anderen? Wie würde ihr Tanz, wir nennen es Flip-Flop, dann aussehen?
Wieso ist dieses Verhalten so wichtig zu verstehen?
Die Frage, wenn wir einen größeren Quantensimulator haben, ist ja: Woher wissen wir, dass er funktioniert? Wir können nicht einfach Zwischenergebnisse ansehen, dann müssten wir wieder von vorne anfangen. Das ist auch eine Eigenschaft der Quantenmechanik. Denn sobald wir eine Messung vornehmen, ist es unumgänglich, dass wir den Zustand des Systems ändern. Mit unserem Ansatz können wir jetzt klein anfangen. Unsere ersten Experimente zum Verhalten eines einzelnen Titanatoms könnte man noch mit Stift und Zettel lösen. Dann nehmen wir zwei, drei, vier, fünf Atome – das kann man immer noch vollständig mit klassischen Computern berechnen. Und dann könnten wir zu größeren Systemen übergehen und hätten die Gewissheit, dass das System funktioniert.
In Jülich entsteht beispielweise gerade der Quantencomputer QSolid. Was erwarten Sie von solchen Projekten?
Im gewissen Sinne leben wir gerade in einer Zeitenwende, ähnlich wie Ende der 1940er-, Anfang der 1950er-Jahre in der klassischen Computerwelt. Damals waren Computer extrem kompliziert zu bedienende Systeme und nur etwas für Spezialisten. Man musste bis auf die elektronische Ebene hinuntergehen. Geändert hat sich die Situation dadurch, dass es später Computer gab, die ausprobiert werden konnten, die ersten Computerspiele entstanden. Ich glaube, wir werden eine ähnliche Geschichte mit Quantencomputern erleben. Dafür brauchen wir Geräte vor Ort und Menschen, die mit diesen experimentieren können. Ich denke, dass wir Dinge sehen werden, die wir bis heute noch gar nicht in Betracht gezogen haben.",„Zwischen zwei Zuständen“
"Der Mond ist das auffälligste Objekt am nächtlichen Himmel – und neben der Erde der einzige Himmelskörper, den Menschen bislang betreten haben.
In der Frühzeit des Sonnensystems – vor etwa 4,5 Milliarden Jahren – stieß ein marsgroßer Himmelskörper mit der urzeitlichen Erde zusammen. Aus dieser Kollision ging der Mond hervor: Er formte sich aus Trümmern, die durch den heftigen Aufprall ins Weltall geschleudert worden waren. Mit seiner Anziehungskraft hat der Erdtrabant die Entwicklung unseres Planeten erheblich beeinflusst. So stabilisiert der Mond die Rotationsachse der Erde und sorgt damit für stabile Klimaverhältnisse. Und die vom Mond verursachten Gezeiten schufen einzigartige Lebensräume zwischen Meer und Land.
Es ist ein glücklicher Zufall, dass Sonne und Mond von der Erde aus gesehen etwa gleich groß erscheinen. Denn nur dadurch kommt es zu spektakulären Sonnenfinsternissen, wenn sich der Mond auf seiner Bahn vor die Sonne schiebt. Die genaue Vorhersage von solchen Ereignissen spielte eine wichtige Rolle in der Geschichte der Astronomie.
Auch heute ist der Mond noch Gegenstand der Forschung. Anhand von Gesteinsproben versuchen Wissenschaftler beispielsweise zu verstehen, wie genau der Erdtrabant aus den Bruchstücken der Kollision entstand und wie er sich seither entwickelte. Auch Einschlagkrater auf der Mondoberfläche liefern wertvolle Hinweise auf das Geschehen im jungen Sonnensystem. Seit Ende der 1950er-Jahre helfen Mondsonden – über einhundert sind es inzwischen – dabei, dem Erdtrabanten seine Geheimnisse zu entlocken. Die Sonden kartieren die lunare Oberfläche und analysieren deren chemische Zusammensetzung.
Aufgrund seiner Nähe zur Erde ist der Mond auch ein Ziel für bemannte Flüge. In den Jahren 1969 bis 1972 betraten insgesamt zwölf Astronauten im Rahmen des Apollo-Projekts die Mondoberfläche. Von ihnen zurückgelassene seismische Messgeräte zeichnen noch heute Erschütterungen durch Mondbeben und Meteoriteneinschläge auf.
Und schon in wenigen Jahren sollen wieder Menschen auf dem Erdtrabanten landen, um dort eine permanente Basis einzurichten. Zentral für dieses Vorhaben ist in den Polarregionen vermutetes Wassereis: Es ließe sich sowohl zur Versorgung mit Wasser und Sauerstoff als auch zur Produktion von Raketentreibstoff nutzen. So könnte der Erdtrabant zum Sprungbrett für Flüge zu weiter entfernten Himmelskörpern im Sonnensystem werden.",Der Mond – Trabant unserer Erde
"Aus der dünnen Atmosphäre des Mars ließe sich theoretisch Sauerstoff für Atemluft und Raketentreibstoff gewinnen. Ob dieser Prozess auch in der Praxis funktioniert, war bislang allerdings unerforscht. Doch nun gelang es Forschern mit einem Experiment an Bord des Rovers Perseverance, erstmals Sauerstoff aus der Marsluft zu erzeugen – wenn auch nur 50 Gramm im Laufe von einem Jahr. Das Experiment sei ein wichtiger Schritt für künftige bemannte Missionen zu unserem Nachbarplaneten, schreiben die beteiligten Wissenschaftler im Fachblatt „Science Advances“.
Die Atmosphäre auf dem Mars besteht überwiegend aus Kohlendioxid. Astronauten müssten auf eine Reise zum Roten Planeten also ausreichend Sauerstoff zum Atmen mitnehmen. Außerdem ist Sauerstoff ein wichtiger Bestandteil von Raketentreibstoff, ohne den eine Rückkehr zur Erde nicht möglich wäre. Doch der Transport großer Mengen an Sauerstoff zum Mars ist enorm teuer und aufwendig. Daher forschen Wissenschaftler an der Möglichkeit, Sauerstoff direkt vor Ort auf dem Mars herzustellen.












                Das Instrument MOXIE
            



Mit diesem Ziel wurde das Instrument namens MOXIE an Bord des Rovers Perseverance im Rahmen der NASA-Mission „Mars 2020“ am 18. Februar 2021 auf den Mars befördert. Im Laufe eines Jahres führten Jeffrey Hoffman vom Massachusetts Institute of Technology in den USA und seine Kollegen insgesamt sieben Testläufe mit MOXIE durch: Zunächst strömt dazu die Marsluft durch den extrem feinen Filter des Instruments, um den allgegenwärtigen Staub zu entfernen. Da der atmosphärische Druck auf dem Mars lediglich etwa ein Hundertstel des Luftdrucks auf der Erde beträgt, verdichtet MOXIE das Kohlendioxid außerdem mithilfe einer Pumpe und erhitzt es anschließend auf 800 Grad Celsius.
Im letzten Schritt gelangt das Kohlendioxid in eine spezielle Zelle, in der mithilfe eines Katalysators und einer elektrischen Spannung je ein Sauerstoffatom von den Kohlendioxidmolekülen abgespalten wird. Dadurch entstehen Kohlenmonoxid – bestehend aus je einem Kohlenstoffatom und einem Sauerstoffatom – sowie reiner Sauerstoff, der dann für die Atemluft und für Raketentreibstoff zur Verfügung stehen soll. Im MOXIE-Experiment wurde allerdings lediglich die Reinheit des gewonnenen Sauerstoffs überprüft – dann wurde es wieder in die Marsatmosphäre abgeblasen.
Die Sauerstoffproduktion in den Testläufen funktionierte problemlos – und zwar zu jeder Tages- und Nachtzeit, sowie während aller vier Jahreszeiten auf dem Mars. Für den Start eines Rückflugs vom Mars werden nach Schätzungen der Forscher etwa 31 Tonnen Sauerstoff benötigt – davon ist das MOXIE-Experiment mit den bislang erzeugten 50 Gramm noch weit entfernt. Doch die Wissenschaftler sehen keinerlei Hindernisse für den Bau erheblich größerer Geräte, die zudem durch weitere technische Verbesserungen noch effektiver arbeiten könnten. „MOXIE ist der erste Schritt zu einem viele hundert Mal größeren System für eine bemannte Erforschung des Mars“, so die Forscher.",Sauerstoff aus der Atmosphäre des Mars
"Wie sich Sonnenlicht in elektrischen Strom umwandeln lässt, wie Forscher den Wirkungsgrad von Solarzellen künftig weiter steigern wollen und welches Potenzial die Photovoltaik bietet, erklärt Christiane Becker von der Hochschule für Technik und Wirtschaft Berlin und dem Helmholtz-Zentrum Berlin in dieser Folge des Podcasts.












                Christiane Becker
            



Der Anteil von Solarstrom an der weltweit erzeugten Strommenge ist in den vergangenen Jahren kontinuierlich gestiegen. Auch in Zukunft dürfte Solarenergie immer wichtiger werden – Luft nach oben gibt es jedenfalls noch.Christiane Becker: „Also je nach Berechnung kann man davon ausgehen, dass uns die Sonne innerhalb von einer Stunde so viel Energie liefert, wie die Menschheit in einem Jahr verbraucht. Es ist also genügend Energie vorhanden.“Nicht nur Unternehmen erzeugen Sonnenstrom, sondern auch immer mehr Privathaushalte statten ihre Dächer mit Solarmodulen aus. Kernstück dieser Anlagen sind Solarzellen, in denen Sonnenlicht in Strom umgewandelt wird. Die Grundlage dafür bildet die Wechselwirkung von Licht mit Materie über den sogenannten photoelektrischen Effekt. Mehr dazu in der 337. Folge.",Photovoltaik
"Die globalen Anstrengungen gegen den Klimawandel haben zwar an Fahrt aufgenommen, sind aber immer noch unzureichend. Denn einen wichtigen Punkt, der bereits im 6. Sachstandsbericht des Weltklimarates IPCC zur Geltung kam, zeigt nun auch eine neue Studie im Fachblatt „Nature Climate Change“ deutlich: Viele klimatische Änderungen sind unumkehrbar – selbst wenn die weiteren Emissionen auf Null zurückgingen und das heute ausgestoßene Kohlendioxid in Zukunft wieder aus der Atmosphäre entfernt würde.
Die Konzentration von Treibhausgasen wie Kohlendioxid beeinflusst zahlreiche Systeme auf der Erde. Dazu gehören Meeresströmungen in den Ozeanen und die Eisbedeckung – vor allem in der Antarktis und auf Grönland, aber auch das Meereis sowie die Gletscher der Hochgebirge. Wie sich für solche und andere Systeme steigende und sinkende Konzentrationen von Treibhausgasen auswirkten, analysierten Soong-Ki Kim von der Yonsei-Universität in Seoul und seine Kollegen nun mit detaillierten Computersimulationen.
Dabei stellte sich heraus, dass sich deutlich steigende Konzentrationen der Treibhausgase über Jahrhunderte auswirkten. Selbst dann, wenn die Konzentrationen der Treibhausgase schon lange wieder auf das Anfangsniveau gesunken waren, stellten sich nicht wieder die ursprünglichen klimatischen Bedingungen ein. Taut etwa der Eisschild auf Grönland infolge des Treibhauseffekts und verliert an Höhe, gerät seine Oberfläche in immer tiefere und wärmere Lagen und lässt das Eis noch schneller schmelzen. Auch starker Schneefall und selbst die Rückkehr zu niedrigen Treibhausgaskonzentrationen würde den Eisschild dann nicht wieder wachsen lassen. Insgesamt änderten sich für die untersuchten Zeiträume auf insgesamt 89 Prozent der Erdoberfläche die Temperaturen und auf 58 Prozent die Niederschlagsmuster unumkehrbar.
Doch nicht alle Regionen auf der Erde waren davon gleichermaßen betroffen. Vielmehr hängen diese Effekte stark mit dem Wasserhaushalt und der Eisbedeckung zusammen. So wirken sich die irreversiblen Veränderungen durch Treibhausgase am stärksten auf die Ozeane, die Arktis und Antarktis sowie auf niederschlagsreiche Regionen aus. In Regionen mit kontinentalem Klima wie Nordamerika, Sibirien und Zentralasien sowie in ausgedehnten Wüstengebieten wie in der Sahara oder in Australien sind die Effekte geringer. Das bedeutet nun keinesfalls, dass der Klimawandel in diesen Regionen keinen großen Schaden anrichten kann. Doch das Klima kann dort eher wieder zu früheren Bedingungen zurückkehren, wenn die Treibhausgaskonzentrationen wieder sinken.
Nach der Simulation der Forscher würden sich die Temperaturen im weltweiten Mittel auf ungefähr ein Grad Celsius über dem Stand von 1999 einpendeln. Der Niederschlag würde ebenfalls spürbar steigen, mit entsprechend erhöhter Gefahr für Starkregen und Überschwemmungen. Auch der artenreiche tropische Regenwald kann von veränderten Niederschlagsmustern stark betroffen sein. Und in Regionen wie der Sahelzone, in Südamerika und in Südasien dürften die irreversiblen Effekte über Jahrhunderte erhalten bleiben.
Wie diese Analyse zeigt, wird die Natur nicht in den Zustand aus früheren Zeiten zurückkehren – selbst dann nicht, wenn die Treibhausgasemissionen bis Mitte des Jahrhunderts auf Null zurückgingen oder gar negative Emissionen erreicht würden und die Konzentration von Treibhausgasen wieder auf ein früheres, niedriges Niveau sänke. Denn in vielen Bereichen der Natur – insbesondere auch beim eng mit dem Klimawandel verknüpften Artenschutz – laufen Prozesse ab, die nicht wieder rückgängig zu machen sind. Deshalb ist es von höchster Wichtigkeit, der weiter steigenden Treibhausgaskonzentration in der Atmosphäre frühzeitig entgegenzuwirken.",Irreversible Folgen durch den Klimawandel
"Mit verschiedenen Methoden untersuchen Geophysiker die Eigenschaften der Erdkruste – so lässt sich etwa ihre Leitfähigkeit mithilfe der Magnetotellurik bestimmen oder ihre Dichtestruktur durch Gravimetrie. Das erlaubt unter anderem Rückschlüsse auf die chemische Zusammensetzung der Erdkruste. Allerdings sind die Ergebnisse oft uneindeutig. Um dieses Problem zu lösen, hat Max Moorkamp von der Ludwig-Maximilians-Universität München nun eine Methode aus der Medizin verwendet, um die aus der Magnetotellurik und der Gravimetrie gewonnenen Messdaten zu einem einheitlichen Bild des Erdinneren zu kombinieren. Wie das funktioniert, erklärt der Physiker im Interview mit Welt der Physik.
Welt der Physik: Welche Rolle spielt die Erdkruste für die Prozesse, die an der Oberfläche stattfinden?












                Max Moorkamp
            



Max Moorkamp: Der Aufbau der Erdkruste, also im Schnitt der oberen 30 bis 35 Kilometer der Erde, unterscheidet sich zum Beispiel deutlich zwischen einem Kontinent und dem Meeresboden. Wenn man etwa wissen will, wo Erdbeben auftreten können, hat das unter anderem damit zu tun, ob es irgendwelche Schwächezonen in der Kruste gibt. Auch Vulkanismus ist ein Prozess, der von vielen Faktoren beeinflusst wird: Da sind vor allem Prozesse im Erdmantel, der unter der Kruste liegt, wichtig. Aber für die Frage, wieso das Magma an bestimmten Stellen austritt, ist wieder der Aufbau der Erdkruste relevant.
Mit welchen Methoden untersucht man die Erdkruste?
Da gibt es die Seismologie, bei der man entweder künstliche Erschütterungen oder Erdbeben benutzt, um sich anzuschauen, wie sich die seismischen Wellen in der Erde ausbreiten. Eine weitere Methode ist die Gravimetrie. Dabei untersucht man die Variationen der Erdanziehungskraft, die zwar sehr klein sind, sich aber dennoch messen lassen. Diese Variationen erlauben dann Rückschlüsse auf die Dichtestruktur der Erde. Ich persönlich beschäftige mich viel mit einem elektromagnetischen Verfahren, der sogenannten Magnetotellurik. Mit dieser Methode ermittelt man die elektrische Leitfähigkeit oder den elektrischen Widerstand in der Erde.
Welche Informationen über die Erdkruste liefert denn die elektrische Leitfähigkeit?
Sie verrät etwas über die Gesteinsarten und deren chemische Zusammensetzung – selbst wenn einige davon nur in ganz geringen Konzentrationen vorliegen. Denn eine kleine Menge Metall genügt, um die Leitfähigkeit von Gestein zu erhöhen. Aber das Gleiche gilt auch für Fluide, also flüssige Materialien in der Erde: Sie sind oft sehr gut leitfähig. Deshalb ist beispielsweise für Vulkane die Magnetotellurik eine sehr beliebte Analysemethode, weil geschmolzenes Gestein einen geringen elektrischen Widerstand beziehungsweise eine hohe Leitfähigkeit hat. Das kann man sehr gut detektieren. Auch in der Mineralexploration, also der Suche nach wertvollen Rohstoffen, ist die Magnetotellurik weit verbreitet.
Wie funktioniert diese Methode?
Das Erdmagnetfeld variiert auf verschiedenen Zeitskalen. Diese Schwankungen hängen beispielsweise mit der Sonnenaktivität zusammen. Abhängig von der Leitfähigkeit induzieren diese natürlichen Magnetfelder kleinste Wirbelströme in der Erdkruste, die dann ihrerseits Magnetfelder erzeugen. Und wir bauen Messstationen an der Erdoberfläche auf, um diese elektromagnetischen Felder zu messen. Dann stellen wir uns noch die Frage, aus welcher Tiefe das gemessene Signal kommt. Dazu kann man die gemessenen Felder nach verschiedenen Frequenzen trennen. Denn die tieferen Frequenzen dringen tiefer in die Erde ein als die höheren Frequenzen. Aus dem Vergleich der gemessenen Feldstärke bei den unterschiedlichen gemessenen Frequenzen erhalten wir daher tiefenaufgelöste Informationen über die Leitfähigkeit. Allerdings entsteht aus den Daten nicht sofort ein räumlich aufgelöstes Bild, sondern man macht mit den Daten eine sogenannte Inversion: Man berechnet eine Leitfähigkeitsverteilung in der Erdkruste, die zu dem gemessenen Signal passt.
Wie schließen Sie dann von der elektrischen Leitfähigkeit auf bestimmte geophysikalische Strukturen?












                Blick ins Innere der Erde
            



Was wir letztlich finden, sind Zonen erhöhter Leitfähigkeit. Und diese Zonen interpretieren wir dann mit unserem physikalischen und geologischen Fachwissen. Zum Beispiel geht man in der Regel davon aus, dass eine erhöhte Leitfähigkeit durch eine Schmelze kommt, wenn die Gegend vulkanisch aktiv ist. Fünfzig Kilometer weiter westlich oder östlich hat man womöglich den gleichen Leitfähigkeitswert, dort ist er aber Ausdruck eines erhöhten Metallgehalts. Das schließt man allein aus dem Kontext. Das heißt, was dort wirklich im Boden vorliegt, ist ein bisschen unsicher, und streng genommen sind andere Interpretationen genauso kompatibel mit den Messungen.
Wie gehen Sie mit diesem Problem um?
Ich habe verschiedene Methoden kombiniert – in meinem Fall die Magnetotellurik und die Gravimetrie. Das eine Verfahren liefert die elektrische Leitfähigkeit, das andere die Dichte. Ziel des Verfahrens ist also, die Leitfähigkeitsverteilung mithilfe der Dichteverteilung zu interpretieren, um eindeutige Informationen über die geophysikalischen Strukturen zu gewinnen. Allerdings muss man dafür wissen, wie die Leitfähigkeit und die Dichte zusammenhängen. Und ich habe jetzt ein neues Verfahren entwickelt, das davon ausgeht, dass jedem elektrischen Widerstand eine bestimmte Dichte zugeordnet ist. Dafür habe ich einen Algorithmus programmiert, der mithilfe dieses Zusammenhangs die Ergebnisse der Inversion verbessert. Das Verfahren stammt eigentlich aus der Medizin beziehungsweise dem maschinellen Lernen.
Wofür wird dieses Verfahren in der Medizin genutzt?
Kernspintomografie und Computertomografie sind zwei bildgebende Verfahren in der Medizin, mit denen man jeweils unterschiedliche Sachen im Körper sichtbar macht. Und auch dort gibt es die Frage, wie man diese unterschiedlichen Bilder bestmöglich kombinieren kann. Da werden auch Inversionsverfahren eingesetzt, mit denen zum Beispiel zwei nicht deckungsgleiche Bilder gemeinsam skaliert werden, sodass beide Bilder zusammenpassen.
Was haben Sie durch diese Vorgehensweise erreicht?
Ich konnte die unterschiedlichen Zonen erhöhter Leitfähigkeit in den Messdaten viel besser auseinanderdividieren. Dass das nötig war, habe ich für den Westen der USA gezeigt. Es gibt aber auch Kollegen von mir, die mein Programm in anderen Gegenden benutzen. Ich scheine ein Rezept gefunden zu haben, mit dem man die Erdkruste viel genauer untersuchen kann als bisher.












                Satellitenaufnahme der westlichen USA
            



Warum haben Sie den Westen der USA gewählt für Ihre Studie?
Zum einen ist es eine sehr interessante Gegend. Man hat am Pazifik eine Subduktionszone, also eine Platte, die abtaucht und Erdbeben verursacht und es gibt den Yellowstone-Nationalpark, wo sehr viel Magmatismus ist. Man hat weiter im Nordosten aber auch sehr alte Strukturen und im Süden eine tektonisch aktive Gegend – also sehr viele interessante Phänomene. Das war der eine Grund. Der andere Grund war aber auch die exzellente Datenlage. Wenn man zum ersten Mal eine neue Methode ausprobiert, dann wählt man einen Bereich, wo es viele und zuverlässige Daten gibt.
Was haben Sie festgestellt, als Sie Ihre neue Methode auf diese Gegend angewendet haben?
Die Gegend rund um den Yellowstone-Nationalpark ist vulkanisch sehr aktiv. Deshalb hatten vorherige Studien hohe Leitfähigkeitswerte mit dem Vorhandensein von Schmelzen in Verbindung gebracht. Meine Studie hat jetzt gezeigt, dass das zu einfach gedacht war. Auch sehr nahe am Yellowstone-Nationalpark gibt es Strukturen, deren Dichten gar nicht zu Schmelzen passen. Generell scheinen die Strukturen viel heterogener zu sein als bislang angenommen. Deswegen muss man auch bei anderen Gegenden in der Welt schauen, ob die Leitfähigkeit dort richtig interpretiert wurde. Denn das hat natürlich Auswirkungen auf Fragen wie: Wie hoch ist das Risiko eines Vulkanausbruchs in der Zukunft? Oder: Wie wird sich dieser Bereich auf langen – tektonischen – Zeitskalen verhalten?
Welche Gegenden werden Sie sich als nächstes anschauen?
Ich persönlich werde meine Methode als nächstes auf den Osten der USA anwenden, denn da gibt es noch eine ganze Reihe von offenen Fragen zum Aufbau der Erdkruste. Auch im südlichen Afrika gibt es viele Fragen, zum Beispiel auch zu Rohstoffen oder aber auch zum generellen Aufbau der Erdkruste. Und ich arbeite mit Leuten zusammen, die das Ganze jetzt auch in der Antarktis oder in Australien für verschiedene Zwecke benutzen. Wir versuchen die Methode sogar in etwas angepasster Form auf den Mars anzuwenden, um bessere Modelle der Marskruste zu erstellen.",„Das war zu einfach gedacht“
"Oberhalb der südspanischen Stadt Granada erhebt sich die Alhambra, eine riesige, palastartige Burganlage aus dem Mittelalter. Mit ihren einzigartigen historischen Ornamenten zieht das Weltkulturerbe zahlreiche Besucher an. Doch die Vergoldung einiger Ornamente zeigt inzwischen eine bisher unerklärte Violettfärbung. Zwei Forscherinnen gingen dieser ungewöhnlichen Färbung nun auf den Grund. Wie sie in der Fachzeitschrift „Science Advances“ berichten, sind offenbar komplexe Prozesse, bei denen sich winzige Nanoteilchen bildeten, für die Verfärbung verantwortlich.
Für ihre Analysen entnahmen Carolina Cardell und Isabel Guerra von der Universität Granada winzige Proben von den violett verfärbten Ornamenten. Diese Proben untersuchten sie sowohl mit hochauflösenden Elektronenmikroskopen als auch mit spektroskopischen Methoden. Dadurch entschlüsselten sie die genaue Zusammensetzung der Materialproben und entdeckten winzige Nanoteilchen aus Gold – mit nur etwa 70 Nanometer Durchmesser.












                Deckenornamente der Alhambra
            



Die Wissenschaftlerinnen machen diese Teilchen für die violett gefärbte Vergoldung verantwortlich. Ihrer Erklärung nach entstehen die nur wenige Nanometer kleinen Goldpartikel durch einen komplexen, natürlichen Zerfallsprozess an der vergoldeten Ornamentoberfläche. Dabei spielten auch weitere Substanzen wie Silber, Zinn und Gips eine Rolle, die beim Vergolden vor mehreren Jahrhunderten oder später zum Schutz der Ornamente aufgetragen wurden. So sollen elektrochemische Vorgänge dazu geführt haben, dass sich die ursprüngliche Goldschicht teilweise aufgelöst und sich anschließend Nanoteilchen aus Gold abgelagert haben. Diese reflektieren nun andere Wellenlängen des einfallenden Lichts als die Oberfläche zuvor – die vormals goldfarbenen Schichten erscheinen nun in einem kräftigen Violett.
Diese Analyse liefert eine plausible Erklärung für die ungewöhnliche Violettfärbung der ursprünglich schillernd goldenen Oberflächen. Solche Verfärbungen treten aber nicht nur in der Alhambra, sondern auch an anderen historischen Monumenten und Kunstwerken auf. Auch dort könnten sich – so die Forscherinnen – Nanoteilchen aus Gold finden lassen.",Vergoldete Ornamente färben sich violett
"Die beiden Exoplaneten WASP-76b und WASP-121b enthalten hoch in ihrer Atmosphäre das schwere Element Barium. Das zeigen hochauflösende Analysen mit dem Very Large Telescope in Chile. Es ist das schwerste Element, das sich bislang in der Atmosphäre von Planeten außerhalb unseres Sonnensystems nachweisen ließ. Und es stellt die Astronomen vor ein Rätsel: Bei der starken Anziehungskraft der beiden Planeten sollten sich keine derart schweren Stoffe in der oberen Atmosphäre befinden, schreiben die Entdecker im Fachblatt „Astronomy & Astrophysics“.
Bei den beiden Planeten handelt es sich um 640 und 860 Lichtjahre von der Erde entfernte Gasplaneten. Die Forscher nennen diese Planeten auch Heiße Jupiter, da sie in ihrer Größe und Masse dem Jupiter in unserem Sonnensystem ähneln. Allerdings sind sie ihrem Zentralstern deutlich näher und umrunden ihn in weniger als zwei Tagen auf engen Bahnen. Durch die Nähe zum Stern ist es in den Atmosphären der Planeten außerdem etwa 2000 Grad Celsius heiß. „Da sie hauptsächlich aus Gas bestehen und sehr heiß sind, besitzen die Planeten ausgedehnte Atmosphären“, erläutert Olivier Demangeon von der Universität Porto in Portugal. Deshalb lassen sich ihre Atmosphären viel leichter untersuchen als jene von kleineren und kühleren Planeten. Außerdem ziehen die beiden Planeten auf ihren Bahnen regelmäßig vor ihrem Zentralstern vorüber und lassen sich dann gut beobachten.
So verdeckt ein Planet jedes Mal, wenn er vor dem Stern vorüberzieht, einen Teil des Sterns und schwächt dessen Licht ab. Ein kleiner Teil des Sternenlichts strahlt jedoch durch die Atmosphäre des Planeten hindurch. In diesem Teil des Sternenlichts hinterlässt die Atmosphäre eine Art Fingerabdruck: Die Stoffe, aus denen die Atmosphäre besteht, absorbieren das Sternenlicht bestimmter Wellenlängen. Das durch die Atmosphäre gelangte Sternenlicht analysierten die Forscher nun am VLT, dem Very Large Telescope der Europäischen Südsternwarte ESO. Mithilfe des Instruments ESPRESSO ermittelten sie das Lichtspektrum, bestimmten also, aus welchen Wellenlängen sich das Licht zusammensetzt. Daraus schlossen sie, welche Stoffe in den Atmosphären der Planeten vorkommen und die im gemessenen Spektrum fehlenden Wellenlängen verursachen.
Mit ihren Messungen bestätigten Demangeon und seine Kollegen zunächst eine Vielzahl von Stoffen, auf die bereits frühere Beobachtungen hingewiesen hatten, spürten aber auch bisher dort unentdeckte Elemente wie Kobalt und Strontium auf. Schließlich stießen sie auf ein typisches Wellenlängenmuster für Barium – und zweifelten zunächst daran, dass diese tatsächlich von den Exoplaneten stammen. „Wir haben dort kein Barium erwartet“, sagt Azevedo Silva von der Universität Porto, denn nie zuvor war ein solch schweres Element in der Atmosphäre eines Exoplaneten entdeckt worden. Erst nach weiteren Überprüfungen waren die Wissenschaftler von ihrer überraschenden Entdeckung überzeugt.
Der Nachweis von Barium in den Atmosphären von gleich zwei fernen Gasplaneten deute nach Ansicht der Forscher darauf hin, dass solche schweren Elemente häufig in den Atmosphären von extrem heißen Vertretern unter den Heißen Jupitern auftreten könnten. Doch eigentlich sollte Barium sehr schnell aus der oberen Atmosphäre nach unten absinken, erläutern die Forscher. Ihrer Einschätzung nach müsse es bislang unbekannte Strömungen in den Atmosphären von WASP-76b und WASP-121b geben, die solche Stoffe in die Hochatmosphäre transportieren.",Exoplaneten mit rätselhafter Atmosphäre
"Die häufigste Art von Sternen in der Milchstraße sind Rote Zwergsterne. Sie sind kleiner und kühler als unsere Sonne und werden von drei unterschiedlichen Arten von Planeten umkreist: Solche mit einer ausgedehnten Atmosphäre, Gesteinsplaneten ähnlich unserer Erde und Planeten mit einem sehr hohen Anteil an Wasser. Das fanden nun zwei Astrophysiker heraus, indem sie Beobachtungsdaten insbesondere des Weltraumteleskops TESS genau analysierten. Wie die Wissenschaftler im Fachblatt „Science“ schreiben, sind alle diese Planeten interessante Kandidaten, um ihre Atmosphäre mit dem neuen James-Webb-Weltraumteleskop auf der Suche nach Leben im All zu untersuchen.
Mit Teleskopen wie dem Weltraumteleskop TESS der NASA spüren Astronomen Planeten auf, die um ferne Sterne kreisen. Denn wenn die Planeten von der Erde aus gesehen vor ihrem Stern vorüberziehen, verdecken sie einen Teil des Sterns und schwächen so das Sternenlicht ab. Dieser Effekt ist bei Roten Zwergsternen aufgrund ihrer geringen Größe besonders deutlich erkennbar. Um die entdeckten Planeten nun genauer zu untersuchen, haben Rafael Luque vom Institut für Astrophysik Andalusiens und Enric Pallé von der Universität La Laguna auf Teneriffa die alten Beobachtungsdaten neu analysiert.
Dabei konzentrierten sich die Forscher auf Planeten, die nicht mehr als vier Mal so groß wie unsere Erde sind. Bislang gingen Wissenschaftler davon aus, dass sich diese Planeten in zwei Arten einteilen lassen: Kleinere Gesteinsplaneten ähnlich zu unserer Erde und größere Planeten mit einer ausgedehnten Atmosphäre aus Helium und Wasserstoff. Diese Annahme wollten Luque und Pallé nun überprüfen und haben dazu nicht nur die Größe, sondern auch die Masse der Planeten bestimmt.
Während sich die Größe eines Planeten unmittelbar daraus ablesen lässt, wie stark er das Sternenlicht im Vorüberziehen abschwächt, ist seine Masse schwieriger zu bestimmen. Denn dafür muss man die Bewegung des Roten Zwergsterns sehr präzise vermessen: Genau genommen kreist ein Planet nämlich nicht um seinen Stern, sondern beide bewegen sich um ihren gemeinsamen Schwerpunkt. Je schwerer der Planet ist, desto stärker beeinflusst er die Bewegung des Sterns. Doch aufgrund der großen Sternenmasse ist der Effekt nur sehr gering und damit schwierig zu beobachten.
Für insgesamt 34 Planeten von Roten Zwergen gelang es den Forschern anhand sehr genauer Beobachtungsdaten weiterer Teleskope jedoch, die Masse und damit auch die Dichte der Himmelskörper zu bestimmen. Neben den bereits bekannten Gesteinsplaneten und solchen mit einer ausgedehnten Gashülle fanden die Wissenschaftler eine große Zahl von Planeten mit einer Dichte, die einem Gemisch aus etwa der Hälfte Gestein und der Hälfte Wasser entspricht.
Diese wasserreichen Planeten könnten in größerer Entfernung von dem Stern entstanden sein – denn dort gibt es in der Entstehungsphase der Planeten mehr Wasser – und erst später auf ihre engeren Umlaufbahnen gelangt sein, vermuten Luque und Pallé. Dafür spreche auch, dass sich die reinen Gesteinsplaneten stets näher am Stern befinden als die Wasserplaneten. Generell könnte es auf allen drei Planetenarten lebensfreundliche Bedingungen geben. Somit sind sie geeignete Kandidaten, um ihre Atmosphäre mit dem neuen James-Webb-Weltraumteleskop auf mögliche Spuren von Leben zu untersuchen.",Rote Zwergsterne und ihre Planeten
"Bis vor 3,7 Milliarden Jahren herrschten auf dem Mars noch völlig andere Bedingungen als heute: Das Klima war deutlich wärmer und damit geeigneter für mikrobielles Leben. Ein Forscherteam hat jetzt genau untersucht, ob und wie sich Mikroben unter den damaligen Bedingungen entwickeln konnten. Mit einem neuartigen Modell fanden sie heraus, dass auf dem Mars – ähnlich wie auf der Erde – vor allem Bakterien gelebt haben könnten, die sich von Wasserstoff ernährten und Methan produzierten. Doch während sich das Klima auf der Erde durch den Stoffwechsel der Bakterien stabilisierte, kühlte das produzierte Methan den Roten Planeten um etwa 40 Grad Celsius ab. Damit verschlechterten sich die Lebensbedingungen für die Bakterien drastisch, wie die Wissenschaftler im Fachblatt „Nature Astronomy“ berichten.
Bakterien, die Wasserstoff verbrauchen und dabei Methan produzieren, gehörten zu den ersten Lebensformen auf der Erde. Während die irdischen Bakterien vor allem in den Ozeanen aktiv waren, fanden sie auf dem Mars auch in der porösen Planetenkruste einen idealen Lebensraum. „Dort waren sie vor ultravioletter und kosmischer Strahlung geschützt“, erläutern Boris Sauterey von der University of Arizona und seine Kollegen, „und das dort vorhandene Wasser lieferte ihnen den Wasserstoff für ihren Stoffwechsel.“
Die Wissenschaftler entwickelten ein klimatisches Modell, das zudem den biologischen Einfluss durch den Stoffwechsel der Bakterien miteinbezieht. So konnten sie erstmals die Auswirkungen der Bakterien auf deren Lebensraum untersuchen. Dabei fanden sie heraus, dass die Bakterien auf dem Mars eine ähnlich große Biomasse produziert haben könnten wie ihre Artgenossen in den irdischen Ozeanen. Allerdings mit ganz unterschiedlichen Konsequenzen: Während das erzeugte Methan auf der Erde die Temperatur der Atmosphäre stabilisierte, kühlte es die Atmosphäre auf dem Mars um etwa 33 bis 45 Grad Celsius ab. Dadurch konnten sich die Mikroorganismen auf dem Roten Planeten nicht an der Oberfläche ausbreiten, sondern mussten in immer tiefere und wärmere Gesteinsschichten hinunter wandern.
Die unterschiedliche Reaktion des planetarischen Klimas auf die Bakterien erkläre sich, so die Forscher, aus der unterschiedlichen Zusammensetzung der Atmosphären der beiden jungen Planeten. Denn die Atmosphäre auf dem Mars besteht vor allem aus Kohlendioxid. In einer solchen Umgebung erzeugt Wasserstoff einen stärkeren Treibhauseffekt und erwärmt den Planeten damit stärker als Methan. Durch den Stoffwechsel der Bakterien verringerte sich jedoch der Anteil von Wasserstoff und der Anteil von Methan stieg – woraufhin sich der Mars abkühlte.
In der Atmosphäre der Erde dominierte hingegen schon damals Stickstoff. Dadurch sei der Effekt genau entgegengesetzt. Dieser Unterschied zeige, so betonen die Forscher, wie wichtig klimatische Rückkopplungen für die Entwicklung von Leben auf einem Planeten sind – und dass sie in beide Richtungen wirken können. „Rückkopplungen zwischen Leben und Umwelt können die Bewohnbarkeit von ganzen Planeten gefährden“, so die Wissenschaftler.
Der Abkühlungseffekt könnte auf dem Mars sogar noch stärker gewesen sein, da das Modell bislang eine mögliche Vereisung der Marsoberfläche nicht berücksichtige. Hierzu seien verbesserte klimatische Modelle für den Roten Planeten nötig, so Sauterey und seine Kollegen. Die Forscher liefern aber auch Hinweise, wo sich auf dem Mars möglicherweise noch Spuren der frühen Bakterien im Boden finden lassen: In den Tiefebenen Hellas und Isidis, sowie im Krater Jezero. Denn diese Regionen sind wahrscheinlich weniger stark abgekühlt und frei von Eis auf der Oberfläche geblieben.",Leben auf dem jungen Mars
"Weltweit schmelzen die Eisschilde der Erde jedes Jahr um rund 750 Milliarden Tonnen ab. Diese enorme Wassermenge ist der wesentliche Grund für den Anstieg des Meeresspiegels um gut 20 Zentimeter seit dem Jahr 1900. Zusätzlich schrumpfen Gletscher durch Abbrüche an ihrer Vorderseite – dieser Eisverlust entspricht etwa einem Zehntel des abgeschmolzenen Eises. Die Entwicklung solcher Abbrüche untersuchten nun Forscher an knapp 1500 Gletschern auf der Nordhalbkugel genauer. Wie sie in der Fachzeitschrift „Nature Communications“ berichten, gelangten in den vergangenen Jahren durch Gletscherabbrüche immer größere Mengen an Eis ins Meer – im vergangenen Jahrzehnt sogar gut 52 Milliarden Tonnen pro Jahr.
Im Fokus der Forscher um William Kochtitzky von der University of Ottawa in Kanada standen Gletscher mit direktem Meereszugang. Bei jedem Abbruch kalbt ein solcher Gletscher einen mehr oder weniger großen Eisberg, der dann frei im Meer schwimmt. Lag dieses Eis zuvor an Land, lässt es, wenn es als Eisberg ins Meer abbricht, alleine durch dessen Verdrängung im Wasser den Meeresspiegel ansteigen – selbst dann, wenn das Eis nicht direkt abschmilzt. Anhand von Positionsdaten der Gletscherzungen und zahlreichen Satellitenaufnahmen schätzten die Wissenschaftler nun ab, welche gefrorenen Anteile von insgesamt 1496 Gletschern auf der Nordhalbkugel über die vergangenen beiden Jahrzehnte ins Meer gelangt war.
So ermittelten Kochtitzky und seine Kollegen, dass zwischen den Jahren 2000 und 2010 knapp 45 Milliarden Tonnen Eis von Gletschern ins Meer abgebrochen war. Im zweiten Jahrzehnt stieg dieser Wert noch weiter an – auf etwa 52 Milliarden Tonnen. Allerdings erkannten sie regional große Unterschiede. So schrumpften die Gletscher in Alaska, entlang der russischen Arktisküste und auf Spitzbergen besonders stark durch Abbrüche. Auf Island und entlang der Küsten von Grönland und Kanada traten dagegen weniger Abbrüche auf. Die genaue Ursache dafür könnten kommende Analysen liefern.
Nach Berechnungen der Forscher ließen allein die Gletscherabbrüche auf der Nordhalbkugel den Meeresspiegel in den ersten 20 Jahren dieses Jahrhunderts um 2,1 Millimeter ansteigen. Im Vergleich zum Anstieg, den Schmelzwasser verursacht, ist dieser Wert zwar gering. Um künftig genauer zu prognostizieren, wie sich der Meeresspiegel entwickeln wird, ist der Beitrag abbrechender Gletscher dennoch zu beachten. Genau dazu liefert die aktuelle Studie einen grundlegenden Beitrag und soll darüber hinaus helfen, den weltweiten Rückgang der Gletscher besser zu verstehen – insbesondere wenn noch ähnliche Detailuntersuchungen für die Gletscher auf der Südhalbkugel erfolgen.",Abbrechende Gletscher und der Meeresspiegel
"Ungewöhnlich starke Regenfälle führen seit August dieses Jahres zu katastrophalen Überflutungen in Pakistan. Die Folgen dieses Ereignisses sind besonders verheerend. Global betrachtet treten Hochwasserkatastrophen recht häufig auf und am stärksten betroffen ist Asien. Das zeigte eine Analyse von Hochwasserereignissen auf der ganzen Welt zwischen den Jahren 1985 und 2019. Warum es überhaupt zu solchen Ereignissen kommt, wie man sie vorhersagt und wie Gesellschaften sich schützen können, erklärt Bruno Merz vom Deutschen GeoForschungsZentrum im Interview mit Welt der Physik.
Welt der Physik: Wie konnte es zu den verheerenden Überschwemmungen in Pakistan im August 2022 kommen?












                Bruno Merz
            



Bruno Merz: Der Sommermonsun in Pakistan ist relativ unregelmäßig, und es ist schwierig vorherzusagen, wann er einsetzt und mit welcher Intensität der Regen fällt. Im Sommer 2022 gab es in Pakistan Monsunniederschläge, die deutlich über dem normalen Niederschlag dieser Jahreszeit lagen. Die Folge war ein Hochwasserextrem mit immensen Schäden – eine Hochwasserkatastrophe.
Worin liegt der Unterschied zwischen einem Hochwasser, einem Hochwasserextrem und einer Hochwasserkatastrophe?
In der Hydrologie sprechen wir allgemein von Hochwasser, wenn temporär irgendwo auf dem Land Wasser steht, das dort nicht hingehört – also beispielsweise, wenn Flüsse über die Ufer treten. Ein Hochwasserextrem ist nicht ganz genau definiert, aber es handelt sich dabei um einen Wasserstand, der sehr selten eintritt – etwa einmal in hundert Jahren. Diese beiden Definitionen berücksichtigen noch nicht die Auswirkungen des Hochwasserereignisses. Von einer Hochwasserkatastrophe spricht man, wenn ein Hochwasserereignis große gesellschaftliche und ökonomische Schäden anrichtet.
Was verursacht Hochwasserereignisse?
Das kommt auf den Typ an. Es gibt die großräumigen Flusshochwasser, die nach langanhaltendem Regen auftreten können, wenn der Boden im Einzugsgebiet so nass ist, dass er kein Wasser mehr aufnehmen kann. Häufig wird das auch durch Schneeschmelze beeinflusst. Außerdem gibt es Sturzfluten: Dabei löst starker Regen sehr schnell ein Hochwasserereignis in einem Bach oder kleinen Fluss aus. Bei diesen Hochwassertypen spielen die Topographie und Vegetation eine Rolle. Waldboden kann zum Beispiel viel besser Wasser aufnehmen als ein bebautes Gebiet. Aber auch die Prozesse im Fluss sind wichtig: Wie schnell kann die Hochwasserwelle abfließen? Überlagert sie sich möglicherweise mit anderen Hochwasserspitzen aus den Nebenflüssen? Das ist ein sehr komplexes System. Und dann gibt es pluviale Überschwemmungen, die insbesondere in Städten eine Rolle spielen – da führt ein Starkregen schnell zu Überflutungen, wenn die Kanalisation das Wasser nicht mehr abführen kann.
Sie haben die Häufigkeit und Ursachen von Hochwasserkatastrophen untersucht. Wie sind Sie dabei vorgegangen?
Wir haben dokumentierte Hochwasserereignisse auf der ganzen Welt zwischen 1985 und 2019 untersucht. Es gibt dafür spezielle Datenbanken, in denen solche Informationen systematisch gesammelt werden. Die haben wir analysiert: Wie groß war jeweils die Gefährdung – also wie hoch und wie ungewöhnlich war der Wasserpegel – und wie schlimm waren die Auswirkungen? Denn auch nicht extreme Hochwasser können bei fehlendem Schutz katastrophale Folgen haben! Unsere Analyse ergab, dass in diesem Zeitraum von 35 Jahren weltweit rund zweieinhalbtausend Hochwasserkatastrophen registriert wurden. Das sind im Schnitt rund 70 Hochwasserkatastrophen im Jahr.












                Überflutungen in der Pakistanischen Provinz Sindh
            



Welche Regionen sind am stärksten davon betroffen und warum?
Ganz eindeutig ist Asien am stärksten betroffen. Das hat unterschiedliche Gründe: Erstens gibt es dort große Überschwemmungsflächen entlang der Flüsse, und zweitens leben dort viele Menschen. Die Leute sind teilweise gezwungen, in gefährdete Regionen zu ziehen, weil sie keine andere Option haben. Und dann kommt dazu, dass es dort wenige Schutzmaßnahmen gibt und Hochwasser deshalb große Schäden anrichten können.
Welche atmosphärischen Prozesse begünstigen die Entwicklung von Hochwasserereignissen?
Wenn viel Feuchtigkeit in der Atmosphäre ist und die Luft, beispielsweise durch Konvektion, angehoben wird, kommt es zu intensiven Regenfällen. Als Beispiel kann man die Wetterlage vor der Flut in Deutschland und einigen Nachbarländern im Juli 2021 betrachten. Damals waren die Temperaturen sehr hoch – auch die Wassertemperaturen der Ostsee und des Mittelmeeres. Deshalb ist viel Wasser verdunstet und die Atmosphäre konnte wegen der hohen Temperaturen auch viel Feuchtigkeit aufnehmen. Ein Tiefdruckgebiet ist dann sehr langsam über Europa gezogen und deshalb kam es zu diesen extremen Niederschlägen von teilweise über 150 Litern pro Quadratmeter in zwei Tagen. Zum Vergleich: Im Schnitt regnet es in Deutschland rund 800 Liter pro Quadratmeter im ganzen Jahr!
Wird es durch den Klimawandel häufiger zu solchen Wetterlagen und damit häufiger zu Hochwasserextremen kommen?
Durch den Klimawandel erhöht sich die Temperatur der Atmosphäre. Sie kann deshalb mehr Wasser aufnehmen – pro Grad Erwärmung etwa sieben Prozent mehr Wasser. Dadurch können Regenfälle intensiver werden. Die zweite, deutlich schwierigere Frage ist, wie sich globale Strömungsmuster verändern werden. Der sogenannte polare Jetstream ist ein Starkwindfeld, welches das Wetter bei uns maßgeblich beeinflusst – beispielsweise treibt der Höhenwind Hoch- und Tiefdruckgebiete über den Globus. Angetrieben wird der Jetstream durch das Temperaturgefälle zwischen der Arktis und den Tropen. Die Arktis erwärmt sich allerdings deutlich schneller als andere Regionen der Erde. Es gibt deshalb die Hypothese, dass das Temperaturgefälle kleiner und damit der Jetstream schwächer wird. Das würde dazu führen, dass Wetterlagen in den mittleren Breiten länger anhalten – längere Dürren, aber auch längere Regenfälle. Ich halte diese Hypothese für plausibel. Trotzdem ist es schwierig, allgemeine Aussagen zu treffen. Ob das Überflutungsrisiko steigt oder fällt, hängt stark vom Hochwassertyp und von der Region ab. So sagen Modelle eine steigende Hochwassergefährdung für Ost- und Südasien, Nordwesteuropa, Subsahara-Afrika, Nordrussland und bestimmte Regionen in Amerika voraus. In anderen Regionen hingegen nimmt das Hochwasserrisiko teilweise ab.
Wie gut lassen sich Hochwasserereignisse vorhersagen?
Hochwasser in größeren Flüssen lassen sich mittlerweile einige Tage im Voraus vorhersagen. Am Rhein reicht es beispielsweise schon, wenn man den Pegel flussaufwärts und die Pegel der Zuflüsse kennt. Dann lässt sich der Wasserstand flussabwärts sehr gut vorhersagen. Wenn das Einzugsgebiet kleiner wird, muss man den Flusspegel mithilfe der Niederschlagsmenge und der Beschaffenheit des Einzugsgebietes vorhersagen. Zu berechnen, wie eine Landschaft auf starken Niederschlag reagiert, ist deutlich schwieriger. Tatsächlich entwickeln wir hier am GFZ gerade ein Modell, mit dem man nicht nur Pegelstände vorhersagen kann, sondern Überflutungen in der Fläche – also zum Beispiel den Wasserstand und die Strömungsgeschwindigkeit an einem konkreten Ort, etwa einem Krankenhaus. Das könnte in Zukunft bei der Planung von Schutzmaßnahmen helfen.
Wie können sich Gesellschaften vor Hochwasserereignissen schützen?
Technischer Hochwasserschutz ist sehr wichtig – beispielsweise durch Deiche oder Rückhaltebecken. Die Niederlande sind hier ein absoluter Vorreiter. Dann gibt es den Punkt Raumplanung. Das wäre der effektivste Hebel, aber es ist leider auch der schwierigste: Wir müssten das Schadenspotential reduzieren, indem wir uns von den Flussufern zurückziehen, und zwar insbesondere dort, wo anfällige Nutzungen wie Wohngebäude auf eine hohe Hochwassergefährdung treffen. Αußerdem wäre es sinnvoll, die natürlichen Überschwemmungsflächen, die in Deutschland weitgehend verschwunden sind, zumindest teilweise wieder herzustellen. Ein weiterer Punkt ist das Katastrophenmanagement. Wann wird der Katastrophenfall ausgerufen, wann werden Gebäude evakuiert? Auch die Bürgerinnen und Bürger müssen das Hochwasserrisiko an ihrem Wohnort kennen und sie müssen wissen, was sie im Falle eines Falles zu tun haben.",„Wir müssten uns von den Flussufern zurückziehen“
"Woraus besteht das Universum – und wie schnell dehnt es sich aus? Auf diese Fragen hat ein Forscherteam die bislang genauesten Antworten erhalten. Dazu werteten die Astronomen Daten von über 1500 Sternexplosionen in bis zu 10,7 Milliarden Lichtjahren Entfernung aus. Doch ihr Ergebnis, wie schnell sich das Weltall ausdehnt, steht in deutlichem Widerspruch zu einem mit einer anderen Methode ermittelten Wert. Das deute auf ein bislang unbekanntes Phänomen im jungen Kosmos hin, so die Wissenschaftler im Fachblatt „Astrophysical Journal“.
Im Rahmen des Projekts Pantheon+ haben Dillon Brout vom Harvard-Smithsonian Center for Astrophysics in Cambridge in den USA und sein Team bestimmte Arten von Sternexplosionen – Supernovae des Typs Ia – analysiert. Diese Sternexplosionen leuchten alle gleich hell auf und sind daher für Astronomen äußerst wertvoll. Beobachtet man, wie hell wir sie von der Erde aus sehen, lässt sich daraus die Entfernung der Explosion berechnen. Mithilfe dieser kosmischen Messlatte lässt sich dann bestimmen, woraus das Universum besteht und wie schnell es sich ausdehnt.
Dies berechneten die Forscher anhand des gemessenen Lichts der Sternexplosionen. So bestätigen die Daten genauer als alle vorigen Messungen das bisherige kosmologische Modell. Nach diesem Modell macht die Materie, aus der das Universum – sprich Sterne, Planeten und auch wir Menschen – bestehen, nur einen verschwindend kleinen Anteil von etwa fünf Prozent des Kosmos aus. Dominiert wird das Universum vielmehr von Dunkler Materie und Dunkler Energie – zwei bislang rätselhafte Bestandteile.
Die Dunkle Materie macht etwa 29 Prozent des Kosmos aus und sorgt dafür, dass Galaxien und Galaxienhaufen von der Schwerkraft zusammengehalten werden. Die sichtbare Materie allein würde dazu nicht ausreichen und so wären ohne Dunkle Materie niemals Sterne, Planeten und auch kein Leben entstanden. Rätselhafter noch ist die Dunkle Energie: Astronomen sehen sie als Ursache dafür an, dass sich das Universum immer schneller ausdehnt, nachdem es vor 13,8 Milliarden Jahren im Urknall entstanden ist. Denn ohne eine zusätzliche Art von Energie müsste diese Ausdehnung langsamer werden. Nun zeigen die Daten von Pantheon+, dass die Dunkle Energie im Verlauf der kosmischen Geschichte vielmehr konstant blieb, und bestätigen bisherige Vermutungen dieser Art.
Darüber hinaus berechneten die Autoren der Studie, wie schnell sich das Universum heute ausdehnt. Astronomen beschreiben die Ausdehnungsgeschwindigkeit mit der Hubble-Konstanten, benannt nach Edwin Hubble, dem Entdecker der kosmischen Expansion. Die Forscher errechneten für die Hubble-Konstante einen Wert von 73,4 mit einer Unsicherheit von nur noch 1,3 Prozent. Ähnliche Werte waren bereits auf diese Weise bestimmt worden, allerdings deutlich weniger präzise. Es gibt jedoch auch eine zweite, unabhängige Methode, die Hubble-Konstante zu bestimmen. Sie basiert auf einer genauen Untersuchung der kosmischen Hintergrundstrahlung – einer Art Strahlungsecho des Urknalls – und liefert einen Wert von 67,4 mit einer Unsicherheit von 0,7 Prozent.
Bislang hatte es unter Forschern immer noch die Hoffnung gegeben, dass sich dieser markante Unterschied zwischen den errechneten Hubble-Konstanten schlicht als statistischer Fehler erweisen würde. Doch mit den neuen Daten ist die Wahrscheinlichkeit dafür auf weit unter ein Zehntausendstel Prozent gesunken. Diese Abweichung, auch Hubble-Spannung genannt, lässt sich dank der neuen, präzisen Bestimmung, kaum mehr als Zufallsergebnis werten.
„Wir hatten gehofft, mit unseren Daten eine mögliche Lösung für das Problem zu finden – stattdessen müssen wir viele Erklärungen verwerfen und die Unterschiede sind ernster als zuvor“, sagt Brout. Die unterschiedlichen Werte für die Hubble-Konstante deuten also auf bislang unbekannte physikalische Phänomene im jungen Kosmos hin. Doch welcher Natur diese Phänomene sind, muss sich erst noch zeigen.",Vermessung des Weltalls lässt Rätsel offen
"Der 8150 Lichtjahre entfernte Neutronenstern HESS J1731-347 ist ein Leichtgewicht: Mit lediglich 77 Prozent der Masse unserer Sonne ist er der erste bekannte Neutronenstern mit weniger Masse als unser Heimatstern. Seine Masse liegt zudem deutlich unterhalb von Vorhersagen theoretischer Modelle. Es könne sich daher möglicherweise um einen Sternenüberrest aus exotischer Materie handeln, so Wissenschaftler im Fachblatt „Nature Astronomy“.
Ein Neutronenstern ist ein extrem dicht gepackter Überrest eines explodierten, massereichen Sterns und besteht hauptsächlich aus Neutronen – daher der Name. In ihm ist Materie so dicht gepackt wie sonst nur in den Kernen von Atomen: So wiegt ein stecknadelkopfgroßes Stück eines Neutronensterns etwa eine Million Tonnen. Der Neutronenstern HESS J1731-347 wurde bei Beobachtungen mit der Teleskopanalage H.E.S.S. in Namibia entdeckt. Wie andere Neutronensterne auch ließ er sich anhand der typischen hochenergetischen Röntgen- und Gammastrahlung aufspüren, die solche Sterne aussenden.
Um nun zu bestimmen, wie weit der Neutronenstern von uns entfernt ist, nutzten Victor Doroshenko und seine Kollegen vom Institut für Astronomie und Astrophysik der Universität Tübingen einen normalen Stern in unmittelbarer Nachbarschaft zu dem Neutronenstern. Beobachtungen mit dem Weltraumteleskop Gaia der Europäischen Raumfahrtagentur ESA lieferten den Forschern eine akkurate Entfernungsmessung zu jenem benachbarten Stern und damit auch zum Neutronenstern – und nur mit dieser Information ließ sich schließlich auch die Masse von HESS J1731-347 berechnen.
„Mithilfe dieser Messungen konnten wir vorherige Ungenauigkeiten beheben und unsere Modelle verbessern. Masse und Radius des Neutronensterns ließen sich viel genauer bestimmen, als es bisher möglich war“, erklärt Valery Suleimanov aus dem Forscherteam. Das Ergebnis: HESS J1731-347 hat einen Durchmesser von lediglich zwanzig Kilometern und bringt gerade einmal das 0,77-Fache der Sonne auf die Waage – er ist damit der erste bekannte Neutronenstern, der leichter als die Sonne ist. Der bislang leichteste Neutronenstern PSR J0453+1559 wiegt 1,174 Sonnenmassen – in guter Übereinstimmung mit dem theoretischen Limit von 1,17 Sonnenmassen.
„Eine bestätigte Verletzung dieses Limits hätte enorme Folgen für unser Verständnis der Entstehung und der Physik von Neutronensternen“, betonen Doroshenko und seine Kollegen. Sie äußern daher den Verdacht, dass es sich nicht um einen echten Neutronenstern, sondern ein noch exotischeres Objekt handeln könnte: einen Quarkstern. In einem solchen Objekt wären aufgrund der extremen Bedingungen die Quarks, aus denen die Kernbausteine der Materie, Neutronen und Protonen, bestehen, frei beweglich – ein als Quark-Gluon-Plasma bezeichneter Zustand. HESS J1731-347 sei damit der bislang beste Kandidat für einen solchen exotischen Sternenüberrest, so die Forscher.",Überraschend leichter Neutronenstern
"Zahlreiche Muscheln und Schnecken schützen sich mit einer verblüffend stabilen Schale. Verantwortlich dafür sind die vielen Schichten, aus denen das Gehäuse besteht, sowie deren mikroskopisch feine Struktur. Doch wie genau die Tiere ihre oft elegant geschwungenen Hüllen bilden, ist im Detail noch nicht geklärt. Eine Forschungsgruppe untersuchte nun Flügelschnecken, die im Meer leben und fand in den Schalen Hinweise auf spiralförmige Flüssigkristalle. Diese könnten den Wachstumsprozess der Schalen unterstützen, berichten die Forscher in der Fachzeitschrift „Physical Review Materials“.
Flüssigkristalle haben spannende Eigenschaften: Sie verhalten sich einerseits wie eine Flüssigkeit, andererseits sind ihre Moleküle ähnlich wie in Kristallen streng geordnet. Synthetische Flüssigkristalle werden beispielsweise in elektronischen Bildschirmen eingesetzt. Hinweise auf ähnliche Flüssigkristalle fanden Julyan Cartwright und seine Kollegen von der Universität Granada auch in der Natur. Dazu analysierten sie Schalen von Flügelschnecken, die wegen ihrer eleganten geformten Hülle auch Seeschmetterlinge genannt werden.
Mit einem Rasterelektronenmikroskop betrachteten die Forscher die mikroskopisch feine Struktur der Schalen und richteten ihren Fokus auf Bruchstellen, in denen der mehrschichtige Aufbau des Materials sichtbar wurde. Bei zwei Arten von Flügelschnecken namens Creseis acicula und Cuvierina columnella stießen die Forscher auf eine besonders ausgeprägte Spiralstruktur. Gerade dieser Aufbau wird schon seit Längerem für die hohe Stabilität der Schalen verantwortlich gemacht. Vor allem verhindern diese Mikrostrukturen, dass sich Risse durch die gesamte Schale ausbreiten.
Die Schalen der Flügelschnecken wachsen, indem sich stäbchenförmige Bauteile aus Proteinen und anderen natürlichen Molekülen Schicht für Schicht ablagern. Doch warum sie spiralförmige Strukturen bilden, war bislang unklar. Wegen der strukturellen Ähnlichkeit vermuten Cartwright und seine Kollegen, dass der Grund spiralförmige Flüssigkristalle sind, die quasi ein vorgeformtes Gerüst bilden. Einmal an die wachsende Schale angedockt, ordnet sich das flüssige und kristalline Material in spiralförmigen Strukturen an. Danach könnte ein Verkalkungsprozess einsetzen, der durch den Einbau von Kalziumkarbonat die Spiralstruktur stabilisiert. Um die Vermutung der Forscher zu überprüfen, ist allerdings noch mehr Arbeit mit weiteren Lebewesen notwendig, die solche faszinierenden Materialien herstellen können.",Die Schale von Flügelschnecken
"Von der Sahara über die Wüste Gobi in Asien bis zu den Trockengebieten im Westen Australiens – rund um den Globus finden sich Sanddünen. Wie sie sich mit zunehmender Erderwärmung fortbewegen und ausbreiten, konnten Forscher nun mit einem neuen Modell vorhersagen: So ist je nach Wind und Wüste teils eine zunehmende, teils eine abnehmende Dynamik der Dünen zu erwarten, berichten die Forscher in der Fachzeitschrift „Nature Climate Change“. Mit diesen Prognosen lassen sich künftig große Infrastrukturprojekte wie etwa Eisenbahnlinien besser an die zu erwartende Entwicklung von Sanddünen anpassen.
Je nachdem, wie stark und woher die Winde in einer Wüste wehen, und wie viel Sand vorhanden ist, bilden Dünen gerade, stern- oder sichelförmige Strukturen. Außerdem können sie – vom Wind getrieben – durch die Wüste wandern und sich an andere Orte ausbreiten. Doch durch die weiterhin starke Nutzung fossiler Energieträger wie Kohle, Gas und Erdöl schreitet der Klimawandel weiter voran, weshalb in den verschiedenen Regionen mit einer Verlagerung der Niederschlagszonen – etwa in Monsungebieten – und veränderten Windverhältnissen zu rechnen ist. Diese Klimaveränderungen wirken sich auf das regionale Wetter – und damit auch auf Ausbreitung und Dynamik von Dünen in Sandwüsten – aus.
Um diese Auswirkungen näher zu untersuchen, machten Andreas Baas und Lucie Delobel vom King´s College London zuerst eine Bestandsaufnahme größerer Sandwüsten auf der gesamten Erde. Mit Satellitenaufnahmen und Daten von Wetterstationen ermittelten sie, wie sich Sanddünen vom Jahr 2000 bis zum Jahr 2015 ausbreiteten und wanderten. Danach blickten die Forscher mit Klimamodellen in die Zukunft, genauer auf die Jahre von 2085 bis 2100.
Die Prognosen der beiden Forscher zeigen sehr unterschiedliche Entwicklungen: So rechnen sie im Westen Australiens, in der Wüste Thar in Indien und vor allem auf der arabischen Halbinsel mit deutlich stärkerer Dünenwanderung. In der Wüste Gobi und in der nördlichen Sahara dagegen könnten schwächere Winde die Dünendynamik zurückgehen lassen.
„Diese Veränderungen werden die Verkehrsinfrastruktur, Industrie und die urbane Entwicklung in Wüstenregionen beeinflussen“, folgern Baas und Delobel. Allerdings gibt ihre Studie nur einen ersten groben Überblick, wie sich die Dünendynamik entwickelt. Sie könnte sich weniger stark verändern, sollten die Emissionen von Treibhausgasen schneller reduziert werden als in dem genutzten Klimamodell. Für die regionale Planung von langlebigen Infrastruktur- und Siedlungsprojekten wären jedoch noch genauere Analysen mit höherer räumlicher Auflösung der Wüstengebiete sinnvoll.",Sanddünen in Zeiten des Klimawandels
"Wie neue Planetensysteme entstehen, gehört zu den spannendsten Fragen der Astrophysik. Um diesem Rätsel auf die Spur zu kommen, haben Forscher nun mehr als 800 protoplanetare Scheiben in der etwa 1350 Lichtjahre entfernten Molekülwolke Orion A untersucht. Mithilfe von Daten der Teleskopanlage ALMA in Chile bestimmten die Astronomen etwa die Massenverteilung der planetenbildenden Regionen und gewannen so neue Erkenntnisse über die Entwicklung der Scheiben. Was diese neuen Ergebnisse über den Zusammenhang von Alter und Masse der Scheiben verraten, erzählt Sierk van Terwisga vom Max-Planck-Institut für Astronomie in Heidelberg im Interview mit Welt der Physik.
Welt der Physik: Warum haben Sie protoplanetare Scheiben in der Molekülwolke Orion A untersucht?












                Sierk van Terwisga
            



Sierk van Terwisga: Der Orionnebel ist ein großer Komplex aus Molekülwolken, in dem an vielen Orten eine rege Sternentstehung stattfindet. Das liefert reichlich Material für die Untersuchung protoplanetarer Scheiben um junge Sterne. Allerdings befinden sich dort auch viele sehr massereiche junge Sterne. Diese senden enorm starke Sternwinde und UV-Strahlung aus, was die Entwicklung protoplanetarer Scheiben weiträumig über Distanzen von mehreren Lichtjahren stören kann. Wir haben uns deshalb auf einen ziemlich ruhigen Teil von Orion A beschränkt, in dem sich protoplanetare Scheiben ungestört entwickeln können. Davon haben wir uns Aufschluss über typische Entstehungsprozesse von neuen Planetensystemen erhofft.
Wie viele protoplanetare Scheiben haben Sie untersucht?
Wir haben insgesamt 873 protoplanetare Scheiben in ihren Eigenschaften beschrieben. Das ist ein sehr großer Datensatz, größer als der von bisherigen Studien dieser Art. Möglich war das, weil sich mit den – mittlerweile abgeschalteten – Weltraumteleskopen Spitzer und Herschel viele dieser neuen Planetensysteme identifizieren ließen. Spitzer und Herschel sind Infrarotteleskope und waren deswegen gut geeignet, um den heißen inneren Bereich von protoplanetaren Scheiben zu entdecken. Denn dieser Bereich wird von der Strahlung des jungen Sterns im Zentrum der Scheibe angestrahlt und leuchtet dann im infraroten Licht. Aber mit den Daten dieser Teleskope erhält man noch keine Informationen über die Ausdehnung und weiteren Charakteristika der Scheibe, und insbesondere nicht über ihre Masse.
Wie haben Sie diese Daten dann gewonnen?
Hierzu haben wir auf die Teleskopanlage ALMA zurückgegriffen, einem Verbund aus 66 Teleskopen auf dem Hochplateau der chilenischen Atacamawüste. Diese Teleskope können wesentlich langwelligere Strahlung mit sehr hoher Auflösung aufnehmen. Damit lässt sich auch der kalte Staub in großer Entfernung vom Stern aufzeichnen, sodass wir die Gesamtmasse der protoplanetaren Scheiben bestimmen konnten. Denn das Verhältnis von Gas- und Staubmasse ist bekannt. Gleichzeitig lassen sich mit ALMA keine Planeten oder große Felskörper beobachten. Wir sehen in unseren Daten also nur das Material, aus dem sich noch Planeten bilden können.













                Künstlerische Darstellung einer planetenbildenden Scheibe 
            



Wie schwer ist so eine protoplanetare Scheibe durchschnittlich?
Das kann sehr stark variieren. Die Masse hängt unter anderem davon ab, wie groß und dicht die Gas- und Staubwolke anfangs ist, die schließlich zu einem neuen Planetensystem kollabiert. Die Masse der von uns beobachteten Systeme reicht von ungefähr der Masse des Erdmondes bis hin zu Hunderten Erdmassen. Sehr schwere Scheiben sind allerdings selten. Wir haben nur zwanzig Scheiben nachgewiesen, die mehr als hundert Erdmassen auf die Waage bringen. Im Durchschnitt entspricht die Masse etwas mehr als zwei Erdmassen.
Wie entwickeln sich die Scheiben im Lauf der Zeit?
Es gibt eine Reihe unterschiedlicher Eigenschaften, die bei der Entstehung und Entwicklung von Planetensystemen eine Rolle spielen, wie etwa die chemische Zusammensetzung oder die Dynamik der Ursprungswolke. Wie wir in unserer Analyse jetzt festgestellt haben, hat all das interessanterweise kaum einen Effekt auf die Masse einer typischen Scheibe. Der einzige wesentliche Faktor für die Entwicklung der protoplanetaren Scheibe ist die Zeit: Im Lauf der Jahrhunderttausende und Jahrmillionen schrumpft die Masse. Das ist auch zu erwarten, weil sich aus dem Material ja Planeten, Asteroiden und Kometen bilden. Außerdem bläst der Sternwind stetig einen Teil des Staubs nach außen.
Was lässt sich daraus allgemein für die Bildung von Planetensystemen schließen?
Das ist eine schwierige Frage, da sich die galaktischen Rahmenbedingungen doch sehr unterscheiden. Wir haben aber die Systeme in Orion A mit anderen Sternentstehungsgebieten in unserer galaktischen Umgebung verglichen und sehr ähnliche Daten erhalten. Die hohe Regelmäßigkeit, die wir sehen, ist schon auffällig. Man könnte fast sagen, am überraschendsten ist, dass die Daten keine Überraschungen aufweisen. Ich denke, man kann zumindest für unsere nähere galaktische Umgebung sagen, dass alle Gruppierungen von planetenbildenden Scheiben eine ähnliche Massenverteilung bei einem bestimmten Alter zeigen. Die Entwicklung von Planetensystemen könnte also erstaunlich gleichförmig verlaufen. Genaueres wird man aber erst in Zukunft wissen, denn wir müssen noch sehr viel mehr junge Planetensysteme im Detail untersuchen.",„Der wesentliche Faktor ist die Zeit“
"In manchen Regionen der Erde ist es so kalt, dass das Wasser an der Oberfläche des Ozeans gefriert: Es entsteht Meereis. Was dieses Eis von gewöhnlichem Eis unterscheidet und warum es wichtig für das Klima unserer Erde ist, erläutert Christian Haas von der Universität Bremen und dem Alfred-Wegener-Institut für Polar- und Meeresforschung in Bremerhaven in dieser Folge des Podcasts.












                Christian Haas
            



Das Eis von Gletschern und Eisschilden besteht aus gefrorenem Süßwasser, doch auch Salzwasser kann zu Eis erstarren. Dafür sind allerdings etwas tiefere Temperaturen erforderlich als beim Süßwasser.Christian Haas: „Normales Meerwasser mit Salzgehalten zwischen 32 bis 34 Gramm Salz pro Kilogramm Wasser hat einen Gefrierpunkt von zwischen minus 1,6 und minus 1,9 Grad Celsius. Das heißt, das Meerwasser muss sich auf diese Temperatur abkühlen, um anzufangen zu gefrieren.“Der Salzgehalt beeinflusst nicht nur, wann Wasser zu Eis erstarrt, sondern führt noch zu einem weiteren Phänomen: Nähert sich Meerwasser seinem Gefrierpunkt an, steigt seine Dichte und es sinkt in die Tiefe. Mehr dazu in der 339. Folge.",Meereis
"Sie sind unregelmäßig geformt und wenige Meter bis hin zu einigen Hundert Kilometern groß: Asteroiden. Mehr als eine Million davon haben Astronomen bereits im Sonnensystem aufgespürt, manche mit Raumsonden besucht und sogar Gesteinsproben dieser kleinen Himmelskörper zur Erde zurückgebracht.
Neben den Planeten und ihren Monden schwirren viele kleinere Himmelskörper durch das Sonnensystem. Traditionell haben Astronomen diese in Kometen, Asteroiden und Meteoroiden eingeteilt. Die Trennung schien klar genug: Kometen zeigen einen Schweif, Asteroiden nicht. Und Meteoroide sind die Ursprungskörper der Sternschnuppen und Feuerbälle, die am Nachthimmel aufleuchten.
Doch inzwischen lässt sich diese einfache Klassifizierung nicht mehr aufrechterhalten. So stießen Himmelsforscher beispielsweise im äußeren Sonnensystem – jenseits der Bahn von Neptun – auf zahlreiche Objekte, die sich gemäß der ursprünglichen Definition den Asteroiden zuordnen lassen: Ihr Durchmesser beträgt einige Kilometer und sie besitzen keinen Schweif. Allerdings setzen sie sich aus einem lockeren Gemisch aus Felsbrocken und Eis zusammen und ähneln damit eher Kometen. Heute verwenden Wissenschaftler den Begriff Asteroid daher nur für Himmelskörper, die überwiegend aus Gestein oder Metallen bestehen.
Genügend Schwerkraft muss man haben
Von Planeten und Zwergplaneten lassen sich Asteroiden dagegen durch ihre Form abgrenzen: Aufgrund ihrer eigenen Schwerkraft nehmen Planeten die Form einer Kugel an oder – wenn sie sich schnell um ihre eigene Achse drehen – die eines abgeplatteten Ellipsoiden. Zwergplaneten sind durch ihre Gravitation zwar nicht mehr exakt, aber immer noch nahezu kugelförmig. Im Gegensatz dazu zeigen Asteroiden ein höchst unterschiedliches, unregelmäßiges Aussehen.












                Ceres und Vesta
            



Denn ihre Schwerkraft reicht nicht aus, um größere Erhebungen auf der Oberfläche unter ihrem Eigengewicht zusammenbrechen zu lassen. Erst ab einer Größe von etwa 800 Kilometern wäre die Anziehungskraft eines Himmelskörpers dafür genügend stark. Ein gutes Beispiel ist der nahezu kugelförmige Zwergplanet Ceres mit einem Durchmesser von 970 Kilometern. Vesta, mit 525 Kilometern der größte bekannte Asteroid, weicht dagegen bereits stark von einer regelmäßigen Gestalt ab.
Asteroiden exakt von Meteoroiden abzugrenzen, ist wohl am schwierigsten. Gemäß Definition der Internationalen Astronomischen Union ist ein Meteoroid „ein festes Objekt, das sich im interplanetaren Raum bewegt und wesentlich kleiner als ein Asteroid und wesentlich größer als ein Atom ist.“ Was aber ist „wesentlich kleiner“? In der Praxis haben sich Astronomen darauf verständigt, dass ein Himmelskörper ab einer Größe von zehn Metern von einem Meteoroiden zu einem Asteroiden wird.
Asteroiden von steinig bis metallisch
Nach der Entdeckung der ersten Asteroiden Anfang des 19. Jahrhunderts dachten Astronomen zunächst, es handele sich bei ihnen um Überreste eines zerbrochenen Planeten. Doch obwohl es vermutlich Millionen von Asteroiden im Sonnensystem gibt, macht ihre Gesamtmasse höchstens ein Zehntel der Erdmasse aus. Heute geht man daher von einer anderen Entstehungsgeschichte aus: Vor etwa 4,5 Milliarden Jahren formten sich die erdähnlichen Planeten aus kleineren Gesteinsbrocken, Planetesimale genannt. In der Region zwischen Mars und Jupiter verhinderte die starke Schwerkraft von Jupiter allerdings, dass sich Planetesimale zusammenschließen und zu einem Planeten heranwachsen konnten. Diese Geröllhaufen blieben also gewissermaßen übrig und zogen einzeln ihre Bahnen um die Sonne.
Noch heute finden sich in diesem Gebiet – dem Asteroidengürtel – besonders viele Asteroiden. Allerdings dürften die meisten von ihnen Bruchstücke ursprünglich größerer Planetesimale sein, die durch Zusammenstöße untereinander regelrecht zertrümmert wurden. So erklärt sich vermutlich auch die Existenz verschiedener Arten von Asteroiden. Die „steinigen“ Asteroiden stammen aus der Kruste, die „metallischen“ aus dem Inneren der Planetesimale.
Seit den 1990er-Jahren werden Asteroiden auch von Raumsonden besucht. Die Jupitersonde Galileo flog 1991 beispielsweise am Asteroiden Gaspra und zwei Jahre später am Asteroiden Ida vorüber. Die japanische Sonde Hayabusa brachte 2010 erstmals Gesteinsproben eines Asteroiden zur Erde zurück. Im Jahr 2011 erreichte die NASA-Raumsonde Dawn den Asteroiden Vesta, von dem aus sie schließlich zum Zwergplaneten Ceres aufbrach, den sie 2015 erreichte. Gegenwärtig sind zwei weitere NASA-Sonden auf dem Weg zu Asteroiden: Lucy wird voraussichtlich ab 2025 sechs Asteroiden untersuchen, die sich auf der Umlaufbahn des Jupiter bewegen.
Auf weniger gefährliche Bahnen lenken
Prinzipiell wäre es auch denkbar, dass sich eines – hoffentlich fernen – Tages ein Asteroid auf die Erde zubewegt. Ab einer Größe von etwa fünfzig Metern könnte ein Asteroid die untere Atmosphäre oder gar den Erdboden in einem Stück erreichen. Die Folgen eines solchen Einschlags dürften verheerend sein. Das zeigt beispielsweise das Tunguska-Ereignis von 1908: In einem Gebiet von über 2000 Quadratkilometern wurden damals Bäume entwurzelt oder wie Streichhölzer umgeknickt.












                Die Mission DART
            



Noch größere Asteroiden könnten gar eine globale Katastrophe auslösen, wie das Aussterben der Dinosaurier durch einen Einschlag vor 65 Millionen Jahren eindrucksvoll belegt. In internationaler Zusammenarbeit versuchen Astronomen daher alle Asteroiden zu erfassen und zu überwachen, die der Erde nahekommen können. Die NASA schätzt, dass mittlerweile mehr als 90 Prozent der erdnahen und mehr als einen Kilometer großen Asteroiden bekannt sind. Und das ist wichtig – denn nur bei einer ausreichenden Vorwarnzeit bestünde die Chance, die Himmelskörper rechtzeitig auf eine weniger gefährliche Bahn umzulenken.
Erprobt haben Wissenschaftler ein solches Unterfangen erstmals mit der Raumsonde DART, die am frühen Morgen des 27. September 2022 auf Dimorphos – dem kleinen Mond des Asteroiden Didymos – einschlug. Der Aufprall veränderte tatsächlich die Umlaufzeit des Mondes: Statt 11 Stunden und 55 Minuten benötigt Didymos jetzt offenbar nur noch 11 Stunden und 23 Minuten für einen Orbit. Um den Einschlag von DART und dessen Folgen für das Asteroidensystem genauer zu untersuchen, soll im Herbst 2024 die Raumsonde Hera der Europäischen Weltraumorganisation ESA starten und zwei Jahre später bei Dimorphos eintreffen.",Asteroiden
